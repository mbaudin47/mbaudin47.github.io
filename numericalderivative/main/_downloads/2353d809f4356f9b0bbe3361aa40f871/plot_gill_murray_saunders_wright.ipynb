{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment with Gill, Murray, Saunders and Wright method\n\nFind a step which is near to optimal for a centered finite difference \nformula.\n\n## References\n- Gill, P. E., Murray, W., Saunders, M. A., & Wright, M. H. (1983). Computing forward-difference intervals for numerical optimization. SIAM Journal on Scientific and Statistical Computing, 4(2), 310-321.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_first_derivative_GMSW(\n    f,\n    x,\n    f_prime,\n    kmin,\n    kmax,\n    verbose=False,\n):\n    \"\"\"\n    Compute the approximate derivative from finite differences\n\n    Parameters\n    ----------\n    f : function\n        The function.\n    x : float\n        The point where the derivative is to be evaluated\n    f_prime : function\n        The exact first derivative of the function.\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n\n    Returns\n    -------\n    relative_error : float, > 0\n        The relative error between the approximate first derivative\n        and the true first derivative.\n\n    feval : int\n        The number of function evaluations.\n    \"\"\"\n    algorithm = nd.GillMurraySaundersWright(f, x, verbose=verbose)\n    step, number_of_iterations = algorithm.compute_step(kmin, kmax)\n    f_prime_approx = algorithm.compute_first_derivative(step)\n    feval = algorithm.get_number_of_function_evaluations()\n    f_prime_exact = f_prime(x)\n    if verbose:\n        print(f\"Computed step = {step:.3e}\")\n        print(f\"Number of iterations = {number_of_iterations}\")\n        print(f\"f_prime_approx = {f_prime_approx}\")\n        print(f\"f_prime_exact = {f_prime_exact}\")\n    absolute_error = abs(f_prime_approx - f_prime_exact)\n    return absolute_error, feval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Test on ExponentialDerivativeBenchmark\")\nkmin = 1.0e-15\nkmax = 1.0e1\nx = 1.0\nbenchmark = nd.ExponentialProblem()\nsecond_derivative_value = benchmark.second_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeForward.compute_step(\n    second_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\n(\n    absolute_error,\n    number_of_function_evaluations,\n) = compute_first_derivative_GMSW(\n    benchmark.function,\n    x,\n    benchmark.first_derivative,\n    kmin,\n    kmax,\n    verbose=True,\n)\nprint(\n    \"x = %.3f, error = %.3e, Func. eval. = %d\"\n    % (x, absolute_error, number_of_function_evaluations)\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Test on ScaledExponentialDerivativeBenchmark\")\nkmin = 1.0e-9\nkmax = 1.0e8\nx = 1.0\nbenchmark = nd.ScaledExponentialProblem()\nsecond_derivative_value = benchmark.second_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeForward.compute_step(\n    second_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\n(\n    absolute_error,\n    number_of_function_evaluations,\n) = compute_first_derivative_GMSW(\n    benchmark.function,\n    x,\n    benchmark.first_derivative,\n    kmin,\n    kmax,\n    verbose=True,\n)\nprint(\n    \"x = %.3f, error = %.3e, Func. eval. = %d\"\n    % (x, absolute_error, number_of_function_evaluations)\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark_method(\n    function, derivative_function, test_points, kmin, kmax, verbose=False\n):\n    \"\"\"\n    Apply Gill, Murray, Saunders & Wright method to compute the approximate first\n    derivative using finite difference formula.\n\n    Parameters\n    ----------\n    f : function\n        The function.\n    derivative_function : function\n        The exact first derivative of the function\n    test_points : list(float)\n        The list of x points where the benchmark must be performed.\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n\n    Returns\n    -------\n    absolute_error : float, > 0\n        The absolute error between the approximate first derivative\n        and the true first derivative.\n\n    feval : int\n        The number of function evaluations.\n\n    \"\"\"\n    number_of_test_points = len(test_points)\n    relative_error_array = np.zeros(number_of_test_points)\n    feval_array = np.zeros(number_of_test_points)\n    for i in range(number_of_test_points):\n        x = test_points[i]\n        (\n            absolute_error,\n            number_of_function_evaluations,\n        ) = compute_first_derivative_GMSW(\n            function, x, derivative_function, kmin, kmax, verbose\n        )\n        relative_error = absolute_error / abs(derivative_function(x))\n        if verbose:\n            print(\n                \"x = %.3f, abs. error = %.3e, rel. error = %.3e, Func. eval. = %d\"\n                % (x, absolute_error, relative_error, number_of_function_evaluations)\n            )\n        relative_error_array[i] = relative_error\n        feval_array[i] = number_of_function_evaluations\n\n    average_error = np.mean(relative_error_array)\n    average_feval = np.mean(feval_array)\n    if verbose:\n        print(\"Average error =\", average_error)\n        print(\"Average number of function evaluations =\", average_feval)\n    return average_error, average_feval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Benchmark on several points\")\nnumber_of_test_points = 100\ntest_points = np.linspace(0.01, 12.2, number_of_test_points)\nkmin = 1.0e-13\nkmax = 1.0e-1\nbenchmark = nd.ExponentialProblem()\naverage_error, average_feval = benchmark_method(\n    benchmark.function, benchmark.first_derivative, test_points, kmin, kmax, True\n)\n\n\n# For each function, at point x = 1, plot the error vs the step computed\n# by the method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_with_GMSW_steps(\n    name, function, function_derivative, x, h_array, kmin, kmax, verbose=False\n):\n    algorithm = nd.GillMurraySaundersWright(function, x)\n    number_of_points = len(h_array)\n    error_array = np.zeros((number_of_points))\n    for i in range(number_of_points):\n        h = h_array[i]\n        f_prime_approx = algorithm.compute_first_derivative(h)\n        error_array[i] = abs(f_prime_approx - function_derivative(x))\n\n    step, number_of_iterations = algorithm.compute_step(kmin, kmax)\n\n    if verbose:\n        print(name)\n        print(f\"Step h* = {step:.3e} using {number_of_iterations} iterations\")\n\n    minimum_error = np.nanmin(error_array)\n    maximum_error = np.nanmax(error_array)\n\n    pl.figure()\n    pl.plot(h_array, error_array)\n    pl.plot(\n        [step] * 2,\n        [minimum_error, maximum_error],\n        \"--\",\n        label=r\"$\\hat{h}$\",\n    )\n    pl.title(f\"(GMS & W). {name} at point x = {x}\")\n    pl.xlabel(\"h\")\n    pl.ylabel(\"Error\")\n    pl.xscale(\"log\")\n    pl.yscale(\"log\")\n    pl.legend(bbox_to_anchor=(1.0, 1.0))\n    pl.tight_layout()\n    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax, verbose=False):\n    plot_error_vs_h_with_GMSW_steps(\n        benchmark.name,\n        benchmark.function,\n        benchmark.first_derivative,\n        x,\n        h_array,\n        kmin,\n        kmax,\n        verbose,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.ExponentialProblem()\nx = 1.0\nnumber_of_points = 1000\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nkmin = 1.0e-15\nkmax = 1.0e-1\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 12.0\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.ScaledExponentialProblem()\nx = 1.0\nkmin = 1.0e-10\nkmax = 1.0e8\nh_array = np.logspace(-10.0, 8.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.LogarithmicProblem()\nx = 1.1\nkmin = 1.0e-14\nkmax = 1.0e-1\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.SinProblem()\nx = 1.0\nkmin = 1.0e-15\nkmax = 1.0e-1\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.SquareRootProblem()\nx = 1.0\nkmin = 1.0e-15\nkmax = 1.0e-1\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = nd.AtanProblem()\nx = 1.1\nkmin = 1.0e-15\nkmax = 1.0e-1\nh_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(benchmark, x, h_array, kmin, kmax)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}