
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_numericalderivative.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_example_plot_numericalderivative.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_numericalderivative.py:


A simple demonstration of the methods
=====================================

Finds a step which is near to optimal for a centered finite difference 
formula.

References
----------
- Adaptive numerical differentiation. R. S. Stepleman and N. D. Winarsky. Journal: Math. Comp. 33 (1979), 1257-1264 

.. GENERATED FROM PYTHON SOURCE LINES 16-21

.. code-block:: Python

    import numpy as np
    import pylab as pl
    import numericalderivative as nd









.. GENERATED FROM PYTHON SOURCE LINES 22-24

Define a function
Here, we do not use the ScaledExponentialDerivativeBenchmark class, for demonstration purposes

.. GENERATED FROM PYTHON SOURCE LINES 24-29

.. code-block:: Python

    def my_scaled_exp(x):
        alpha = 1.0e6
        return np.exp(-x / alpha)









.. GENERATED FROM PYTHON SOURCE LINES 30-31

Define its exact derivative (for testing purposes only)

.. GENERATED FROM PYTHON SOURCE LINES 31-36

.. code-block:: Python

    def my_scaled_exp_prime(x):
        alpha = 1.0e6
        return -np.exp(-x / alpha) / alpha









.. GENERATED FROM PYTHON SOURCE LINES 37-38

Define its exact second derivative (for testing purposes only)

.. GENERATED FROM PYTHON SOURCE LINES 38-43

.. code-block:: Python

    def my_scaled_exp_second(x):
        alpha = 1.0e6
        return np.exp(-x / alpha) / alpha**2









.. GENERATED FROM PYTHON SOURCE LINES 44-45

Function value

.. GENERATED FROM PYTHON SOURCE LINES 45-54

.. code-block:: Python

    print("+ Function")
    x = 1.0e0
    exact_f_value = my_scaled_exp(x)
    print("exact_f_value = ", exact_f_value)
    exact_f_prime_value = my_scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    exact_f_second_value = my_scaled_exp_second(x)
    print("exact_f_second_value = ", exact_f_second_value)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + Function
    exact_f_value =  0.9999990000005
    exact_f_prime_value =  -9.999990000005e-07
    exact_f_second_value =  9.999990000005e-13




.. GENERATED FROM PYTHON SOURCE LINES 55-56

Algorithm to detect h*: SteplemanWinarsky

.. GENERATED FROM PYTHON SOURCE LINES 56-74

.. code-block:: Python

    print("+ SteplemanWinarsky")
    h0 = 1.0e5
    x = 1.0e0
    algorithm = nd.SteplemanWinarsky(my_scaled_exp, x)
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = my_scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + SteplemanWinarsky
    Optimum h = 1.52587890625
    iterations = 8
    Function evaluations = 20
    f_prime_approx =  -9.999990000142134e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 1.371e-17
    Relative error = 1.371e-11




.. GENERATED FROM PYTHON SOURCE LINES 75-76

Algorithm to detect h*: DumontetVignes

.. GENERATED FROM PYTHON SOURCE LINES 76-96

.. code-block:: Python

    print("+ DumontetVignes")
    x = 1.0e0
    algorithm = nd.DumontetVignes(my_scaled_exp, x)
    h_optimal, _ = algorithm.compute_step(
        kmin=1.0e-2,
        kmax=1.0e2,
    )
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = my_scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + DumontetVignes
    Optimum h = 31.05547598808822
    iterations = 8
    Function evaluations = 21
    f_prime_approx =  -9.99999000162036e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 1.615e-16
    Relative error = 1.615e-10




.. GENERATED FROM PYTHON SOURCE LINES 97-98

Algorithm to detect h*: GillMurraySaundersWright

.. GENERATED FROM PYTHON SOURCE LINES 98-118

.. code-block:: Python

    print("+ GillMurraySaundersWright")
    x = 1.0e0
    absolute_precision = 1.0e-15
    algorithm = nd.GillMurraySaundersWright(my_scaled_exp, x, absolute_precision)
    kmin = 1.0e-2
    kmax = 1.0e7
    step, number_of_iterations = algorithm.compute_step(kmin, kmax)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h for f'=", step)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(step)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = my_scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + GillMurraySaundersWright
    Optimum h for f'= 0.06324695766445854
    Function evaluations = 12
    f_prime_approx =  -9.999989679432984e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 3.206e-14
    Relative error = 3.206e-08




.. GENERATED FROM PYTHON SOURCE LINES 119-120

Define a function with arguments

.. GENERATED FROM PYTHON SOURCE LINES 120-124

.. code-block:: Python

    def my_exp_with_args(x, scaling):
        return np.exp(-x / scaling)









.. GENERATED FROM PYTHON SOURCE LINES 125-126

Compute the derivative of a function with extra input arguments

.. GENERATED FROM PYTHON SOURCE LINES 126-137

.. code-block:: Python

    print("+ Function with extra input arguments")
    h0 = 1.0e5
    x = 1.0e0
    scaling = 1.0e6
    algorithm = nd.SteplemanWinarsky(my_exp_with_args, x, args=[scaling])
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h for f''=", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + Function with extra input arguments
    Optimum h for f''= 1.52587890625
    iterations = 8
    Function evaluations = 20





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.035 seconds)


.. _sphx_glr_download_auto_example_plot_numericalderivative.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_numericalderivative.ipynb <plot_numericalderivative.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_numericalderivative.py <plot_numericalderivative.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_numericalderivative.zip <plot_numericalderivative.zip>`
