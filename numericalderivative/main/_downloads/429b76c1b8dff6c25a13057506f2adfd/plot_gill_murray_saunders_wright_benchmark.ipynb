{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark Gill, Murray, Saunders and Wright method\n\nThe goal of this example is to benchmark the :class:`~numericalderivative.GillMurraySaundersWright`\non a collection of test problems.\nThese problems are created by the :meth:`~numericalderivative.build_benchmark()` \nstatic method, which returns a list of problems.\n\n## References\n- Gill, P. E., Murray, W., Saunders, M. A., & Wright, M. H. (1983). Computing forward-difference intervals for numerical optimization. SIAM Journal on Scientific and Statistical Computing, 4(2), 310-321.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport tabulate\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function is an oracle which returns the absolute precision\nof the value of the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def absolute_precision_oracle(function, x, relative_precision):\n    \"\"\"\n    Return the absolute precision of the function value\n\n    This oracle may fail if the function value is zero.\n\n    Parameters\n    ----------\n    function : function\n        The function\n    x : float\n        The test point\n    relative_precision : float, > 0, optional\n        The relative precision of evaluation of f.\n\n    Returns\n    -------\n    absolute_precision : float, >= 0\n        The absolute precision\n    \"\"\"\n    function_value = function(x)\n    if function_value == 0.0:\n        raise ValueError(\n            \"The function value is zero: \" \"cannot compute the absolute precision\"\n        )\n    absolute_precision = relative_precision * abs(function_value)\n    return absolute_precision\n\n\nclass GillMurraySaundersWrightMethod:\n    def __init__(self, kmin, kmax, relative_precision):\n        \"\"\"\n        Create a GillMurraySaundersWright method to compute the approximate first derivative\n\n        Notice that the algorithm is parametrized here based on\n        the relative precision of the value of the function f.\n        Then an oracle computes the absolute precision depending on\n        the function, the point x and the relative precision.\n\n        Parameters\n        ----------\n        kmin : float, kmin > 0\n            A minimum bound for the finite difference step of the third derivative.\n            If no value is provided, the default is to compute the smallest\n            possible kmin using number_of_digits and x.\n        kmax : float, kmax > kmin > 0\n            A maximum bound for the finite difference step of the third derivative.\n            If no value is provided, the default is to compute the largest\n            possible kmax using number_of_digits and x.\n        relative_precision : float, > 0, optional\n            The relative precision of evaluation of f.\n        \"\"\"\n        self.kmin = kmin\n        self.kmax = kmax\n        self.relative_precision = relative_precision\n\n    def compute_first_derivative(self, function, x):\n        \"\"\"\n        Compute the first derivative using GillMurraySaundersWright\n\n        Parameters\n        ----------\n        function : function\n            The function\n        x : float\n            The test point\n\n        Returns\n        -------\n        f_prime_approx : float\n            The approximate value of the first derivative of the function at point x\n        number_of_function_evaluations : int\n            The number of function evaluations.\n        \"\"\"\n        absolute_precision = absolute_precision_oracle(\n            function, x, self.relative_precision\n        )\n        algorithm = nd.GillMurraySaundersWright(\n            function, x, absolute_precision=absolute_precision\n        )\n        step, _ = algorithm.find_step(kmin, kmax)\n        f_prime_approx = algorithm.compute_first_derivative(step)\n        number_of_function_evaluations = algorithm.get_number_of_function_evaluations()\n        return f_prime_approx, number_of_function_evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next example computes the approximate derivative on the\n:class:`~numericalderivative.ExponentialProblem`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Benchmark on several points\")\nnumber_of_test_points = 20\nkmin = 1.0e-16\nkmax = 1.0e-1\nproblem = nd.ExponentialProblem()\nprint(problem)\ninterval = problem.get_interval()\ntest_points = np.linspace(interval[0], interval[1], number_of_test_points)\nrelative_precision = 1.0e-15\nmethod = GillMurraySaundersWrightMethod(kmin, kmax, relative_precision)\naverage_relative_error, average_feval, data = nd.benchmark_method(\n    problem.get_function(),\n    problem.get_first_derivative(),\n    test_points,\n    method.compute_first_derivative,\n    True,\n)\nprint(\"Average relative error =\", average_relative_error)\nprint(\"Average number of function evaluations =\", average_feval)\ntabulate.tabulate(data, headers=[\"x\", \"Rel. err.\", \"F. Eval.\"], tablefmt=\"html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Map from the problem name to kmax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kmax_map = {\n    \"polynomial\": 1.0,\n    \"inverse\": 1.0e0,\n    \"exp\": 1.0e-1,\n    \"log\": 1.0e-3,  # x > 0\n    \"sqrt\": 1.0e-3,  # x > 0\n    \"atan\": 1.0e0,\n    \"sin\": 1.0e0,\n    \"scaled exp\": 1.0e5,\n    \"GMSW\": 1.0e0,\n    \"SXXN1\": 1.0e0,\n    \"SXXN2\": 1.0e0,  # Fails\n    \"SXXN3\": 1.0e0,\n    \"SXXN4\": 1.0e0,\n    \"Oliver1\": 1.0e0,\n    \"Oliver2\": 1.0e0,\n    \"Oliver3\": 1.0e-3,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark the :class:`~numericalderivative.GillMurraySaundersWright` class\non a collection of problems.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_test_points = 100  # This value can significantly change the results\nrelative_precision = 1.0e-15\ndelta_x = 1.0e-9\ndata = []\nfunction_list = nd.build_benchmark()\nnumber_of_functions = len(function_list)\naverage_relative_error_list = []\naverage_feval_list = []\nfor i in range(number_of_functions):\n    problem = function_list[i]\n    function = problem.get_function()\n    first_derivative = problem.get_first_derivative()\n    name = problem.get_name()\n    kmax = kmax_map[name]\n    kmin = 1.0e-16 * kmax\n    lower_x_bound, upper_x_bound = problem.get_interval()\n    if name == \"sin\":\n        # Change the lower and upper bound so that the points +/-pi\n        # are excluded (see below for details).\n        lower_x_bound += delta_x\n        upper_x_bound -= delta_x\n    test_points = np.linspace(lower_x_bound, upper_x_bound, number_of_test_points)\n    print(f\"Function #{i}, {name}\")\n    method = GillMurraySaundersWrightMethod(kmin, kmax, relative_precision)\n    average_relative_error, average_feval, _ = nd.benchmark_method(\n        function,\n        first_derivative,\n        test_points,\n        method.compute_first_derivative,\n    )\n    average_relative_error_list.append(average_relative_error)\n    average_feval_list.append(average_feval)\n    data.append(\n        (\n            name,\n            kmin,\n            kmax,\n            average_relative_error,\n            average_feval,\n        )\n    )\ndata.append(\n    [\n        \"Average\",\n        \"-\",\n        \"-\",\n        np.nanmean(average_relative_error_list),\n        np.nanmean(average_feval_list),\n    ]\n)\ntabulate.tabulate(\n    data,\n    headers=[\"Name\", \"kmin\", \"kmax\", \"Average error\", \"Average func. eval\"],\n    tablefmt=\"html\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the method cannot perform correctly for the sin function\nat the point\nIndeed, this function is such that $f''(x) = 0$ if $x = \\pm \\pi$.\nIn this case, the condition error is infinite and the method\ncannot work.\nTherefore, we make so that the points $\\pm \\pi$ are excluded from the benchmark.\nThe same problem appears at the point $x = 0$.\nThis point is not included in the test set if the number of points is even\n(e.g. with `number_of_test_points = 100`), but it might appear if the\nnumber of test points is odd.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}