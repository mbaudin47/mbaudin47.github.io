{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment with Shi, Xie, Xuan & Nocedal general method\n\nFind a step which is near to optimal for a general finite difference \nformula.\n\n## References\n- Shi, H. J. M., Xie, Y., Xuan, M. Q., & Nocedal, J. (2022). \n  Adaptive finite-difference interval estimation for noisy derivative-free\n  optimization. SIAM Journal on Scientific Computing, 44 (4), A2302-A2321.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd\nfrom matplotlib.ticker import MaxNLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the method on a simple problem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use the algorithm on the exponential function.\nWe create the :class:`~numericalderivative.ShiXieXuanNocedalGeneral` algorithm using the function and the point x.\nThen we use the :meth:`~numericalderivative.ShiXieXuanNocedalGeneral.find_step()` method to compute the step,\nusing an upper bound of the step as an initial point of the algorithm.\nFinally, use the :meth:`~numericalderivative.ShiXieXuanNocedalGeneral.compute_first_derivative()` method to compute\nan approximate value of the first derivative using finite differences.\nThe :meth:`~numericalderivative.ShiXieXuanNocedalGeneral.get_number_of_function_evaluations()` method\ncan be used to get the number of function evaluations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\ndifferentiation_order = 1  # First derivative\nformula_accuracy = 2  # Order 2\nformula = nd.GeneralFiniteDifference(\n    np.exp,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",  # Central formula\n)\nalgorithm = nd.ShiXieXuanNocedalGeneral(formula, verbose=True)\ninitial_step = 1.0\nstep, number_of_iterations = algorithm.find_step(initial_step)\nf_prime_approx = algorithm.compute_derivative(step)\nfeval = algorithm.get_number_of_function_evaluations()\nf_prime_exact = np.exp(x)  # Since the derivative of exp is exp.\nprint(f\"Computed step = {step:.3e}\")\nprint(f\"Number of iterations = {number_of_iterations}\")\nprint(f\"f_prime_approx = {f_prime_approx}\")\nprint(f\"f_prime_exact = {f_prime_exact}\")\nabsolute_error = abs(f_prime_approx - f_prime_exact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the method on the ScaledExponentialProblem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider this problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nprint(problem)\nname = problem.get_name()\nx = problem.get_x()\nthird_derivative = problem.get_third_derivative()\nthird_derivative_value = third_derivative(x)\noptimum_step, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value\n)\nprint(f\"Name = {name}, x = {x}\")\nprint(f\"Optimal step for central finite difference formula = {optimum_step}\")\nprint(f\"Minimum absolute error= {absolute_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the error vs h\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "function = problem.get_function()\nfirst_derivative = problem.get_first_derivative()\nfinite_difference = nd.FirstDerivativeCentral(function, x)\nnumber_of_points = 1000\nstep_array = np.logspace(-8.0, 4.0, number_of_points)\nerror_array = np.zeros((number_of_points))\nfor i in range(number_of_points):\n    h = step_array[i]\n    f_prime_approx = finite_difference.compute(h)\n    error_array[i] = abs(f_prime_approx - first_derivative(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, error_array)\npl.plot([optimum_step] * 2, [min(error_array), max(error_array)], label=r\"$h^*$\")\npl.title(\"Central finite difference\")\npl.xlabel(\"h\")\npl.ylabel(\"Error\")\npl.xscale(\"log\")\npl.yscale(\"log\")\npl.legend(bbox_to_anchor=(1, 1))\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the algorithm to detect h*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "differentiation_order = 1\nformula_accuracy = 2\nformula = nd.GeneralFiniteDifference(\n    function,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",  # Central formula\n)\nalgorithm = nd.ShiXieXuanNocedalGeneral(formula, verbose=True)\nx = 1.0e0\ninitial_step = 1.0\nh_optimal, iterations = algorithm.find_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_derivative(h_optimal)\nabsolute_error = abs(f_prime_approx - problem.first_derivative(x))\nprint(\"Error = \", absolute_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the criterion depending on the step\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the test ratio depending on h\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nfunction = problem.get_function()\nname = problem.get_name()\nx = problem.get_x()\ndifferentiation_order = 1\nformula_accuracy = 2\nformula = nd.GeneralFiniteDifference(\n    function,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",  # Central formula\n)\nalgorithm = nd.ShiXieXuanNocedalGeneral(formula, verbose=True)\nminimum_test_ratio, maximum_test_ratio = algorithm.get_ratio_min_max()\nabsolute_precision = 1.0e-15\nnumber_of_points = 500\nstep_array = np.logspace(-10.0, 3.0, number_of_points)\ntest_ratio_array = np.zeros((number_of_points))\nfor i in range(number_of_points):\n    test_ratio_array[i] = algorithm.compute_test_ratio(\n        step_array[i],\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, test_ratio_array, \"-\", label=\"Test ratio\")\npl.plot(step_array, [minimum_test_ratio] * number_of_points, \"--\", label=\"Min\")\npl.plot(step_array, [maximum_test_ratio] * number_of_points, \":\", label=\"Max\")\npl.title(f\"{name} at x = {x}. Test ratio.\")\npl.xlabel(\"h\")\npl.ylabel(r\"$r$\")\npl.xscale(\"log\")\npl.yscale(\"log\")\npl.legend()\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See the history of steps during the search\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Shi, Xie, Xuan & Nocedal's method, the algorithm\nproduces a sequence of steps $(h_i)_{1 \\leq i \\leq n_{iter}}$\nwhere $n_{iter} \\in \\mathbb{N}$ is the number of iterations.\nThese steps are meant to converge to an\napproximately optimal step of for the finite difference formula of the\nderivative.\nThe optimal step $h^\\star$ for the finite difference formula of the\nderivative can be computed depending on some derivative of the\nfunction.\nIn the next example, we want to compute the absolute error between\neach intermediate step $h_i$ and the exact value $h^\\star$\nto see how close the algorithm gets to the exact step.\nThe list of intermediate steps during the algorithm can be obtained\nthanks to the :meth:`~numericalderivative.ShiXieXuanNocedalGeneral.get_step_history` method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we print the intermediate steps k during\nthe bissection algorithm that searches for a step such\nthat the L ratio is satisfactory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nfunction = problem.get_function()\nname = problem.get_name()\nx = problem.get_x()\ndifferentiation_order = 1\nformula_accuracy = 2\nformula = nd.GeneralFiniteDifference(\n    function,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",  # Central formula\n)\nalgorithm = nd.ShiXieXuanNocedalGeneral(formula, verbose=True)\ninitial_step = 1.0e5\nstep, number_of_iterations = algorithm.find_step(initial_step)\nstep_h_history = algorithm.get_step_history()\nprint(f\"Number of iterations = {number_of_iterations}\")\nprint(f\"History of steps h : {step_h_history}\")\nlast_step_h = step_h_history[-1]\nprint(f\"Last step h : {last_step_h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we compute the exact step, using :meth:`~numericalderivative.FirstDerivativeCentral.compute_step`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "third_derivative = problem.get_third_derivative()\nthird_derivative_value = third_derivative(x)\nprint(f\"f^(3)(x) = {third_derivative_value}\")\nabsolute_precision = 1.0e-16\nexact_step, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value, absolute_precision\n)\nprint(f\"Optimal step k for f'(x) using forward F.D. = {exact_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the absolute error between the exact step k and the intermediate k\nof the algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "error_step_h = [abs(step_h_history[i] - exact_step) for i in range(len(step_h_history))]\nfig = pl.figure(figsize=(4.0, 3.0))\npl.title(f\"Shi, Xie, Xuan & Nocedal on {name} at x = {x}\")\npl.plot(range(len(step_h_history)), error_step_h, \"o-\")\npl.xlabel(\"Iterations\")\npl.ylabel(r\"$|h_i - h^\\star|$\")\npl.yscale(\"log\")\nax = fig.gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous figure shows that the algorithm converges relatively fast.\nThe absolute error does not evolve monotically.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}