{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment with Gill, Murray, Saunders and Wright method\n\nFind a step which is near to optimal for a central finite difference \nformula.\n\n## References\n- Gill, P. E., Murray, W., Saunders, M. A., & Wright, M. H. (1983). Computing forward-difference intervals for numerical optimization. SIAM Journal on Scientific and Statistical Computing, 4(2), 310-321.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd\nfrom matplotlib.ticker import MaxNLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the method on a simple problem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use the algorithm on the exponential function.\nWe create the :class:`~numericalderivative.GillMurraySaundersWright` algorithm using the function and the point x.\nThen we use the :meth:`~numericalderivative.GillMurraySaundersWright.find_step()` method to compute the step,\nusing an upper bound of the step as an initial point of the algorithm.\nFinally, use the :meth:`~numericalderivative.GillMurraySaundersWright.compute_first_derivative()` method to compute\nan approximate value of the first derivative using finite differences.\nThe :meth:`~numericalderivative.GillMurraySaundersWright.get_number_of_function_evaluations()` method\ncan be used to get the number of function evaluations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\nalgorithm = nd.GillMurraySaundersWright(np.exp, x, verbose=True)\nkmin = 1.0e-10\nkmax = 1.0e0\nstep, number_of_iterations = algorithm.find_step(kmin, kmax)\nf_prime_approx = algorithm.compute_first_derivative(step)\nfeval = algorithm.get_number_of_function_evaluations()\nf_prime_exact = np.exp(x)  # Since the derivative of exp is exp.\nprint(f\"Computed step = {step:.3e}\")\nprint(f\"Number of iterations = {number_of_iterations}\")\nprint(f\"f_prime_approx = {f_prime_approx}\")\nprint(f\"f_prime_exact = {f_prime_exact}\")\nabsolute_error = abs(f_prime_approx - f_prime_exact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the method on the exponential problem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function is an oracle which returns the absolute precision\nof the value of the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def absolute_precision_oracle(function, x, relative_precision):\n    \"\"\"\n    Return the absolute precision of the function value\n\n    This oracle may fail if the function value is zero.\n\n    Parameters\n    ----------\n    function : function\n        The function\n    x : float\n        The test point\n    relative_precision : float, > 0, optional\n        The relative precision of evaluation of f.\n\n    Returns\n    -------\n    absolute_precision : float, >= 0\n        The absolute precision\n    \"\"\"\n    function_value = function(x)\n    if function_value == 0.0:\n        raise ValueError(\n            \"The function value is zero: \" \"cannot compute the absolute precision\"\n        )\n    absolute_precision = relative_precision * abs(function_value)\n    return absolute_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GillMurraySaundersWrightMethod:\n    def __init__(self, kmin, kmax, relative_precision):\n        \"\"\"\n        Create a GillMurraySaundersWright method to compute the approximate first derivative\n\n        Parameters\n        ----------\n        kmin : float, kmin > 0\n            A minimum bound for the finite difference step of the third derivative.\n            If no value is provided, the default is to compute the smallest\n            possible kmin using number_of_digits and x.\n        kmax : float, kmax > kmin > 0\n            A maximum bound for the finite difference step of the third derivative.\n            If no value is provided, the default is to compute the largest\n            possible kmax using number_of_digits and x.\n        relative_precision : float, > 0, optional\n            The relative precision of evaluation of f.\n        \"\"\"\n        self.kmin = kmin\n        self.kmax = kmax\n        self.relative_precision = relative_precision\n\n    def compute_first_derivative(self, function, x):\n        \"\"\"\n        Compute the first derivative using GillMurraySaundersWright\n\n        Parameters\n        ----------\n        function : function\n            The function\n        x : float\n            The test point\n\n        Returns\n        -------\n        f_prime_approx : float\n            The approximate value of the first derivative of the function at point x\n        number_of_function_evaluations : int\n            The number of function evaluations.\n        \"\"\"\n        absolute_precision = absolute_precision_oracle(\n            function, x, self.relative_precision\n        )\n        algorithm = nd.GillMurraySaundersWright(\n            function, x, absolute_precision=absolute_precision\n        )\n        step, _ = algorithm.find_step(kmin, kmax)\n        f_prime_approx = algorithm.compute_first_derivative(step)\n        number_of_function_evaluations = algorithm.get_number_of_function_evaluations()\n        return f_prime_approx, number_of_function_evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_first_derivative_GMSW(\n    function,\n    x,\n    first_derivative,\n    kmin,\n    kmax,\n    relative_precision=1.0e-15,\n    verbose=False,\n):\n    \"\"\"\n    Compute the approximate derivative from finite differences\n\n    Parameters\n    ----------\n    function : function\n        The function.\n    x : float\n        The point where the derivative is to be evaluated\n    first_derivative : function\n        The exact first derivative of the function.\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    relative_precision : float, > 0, optional\n        The relative precision of evaluation of f.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n\n    Returns\n    -------\n    relative_error : float, > 0\n        The relative error between the approximate first derivative\n        and the true first derivative.\n\n    feval : int\n        The number of function evaluations.\n    \"\"\"\n\n    method = GillMurraySaundersWrightMethod(kmin, kmax, relative_precision)\n    f_prime_approx, number_of_function_evaluations = method.compute_first_derivative(\n        function, x\n    )\n    f_prime_exact = first_derivative(x)\n    if verbose:\n        print(f\"Computed step = {step:.3e}\")\n        print(f\"Number of iterations = {number_of_iterations}\")\n        print(f\"f_prime_approx = {f_prime_approx}\")\n        print(f\"f_prime_exact = {f_prime_exact}\")\n    absolute_error = abs(f_prime_approx - f_prime_exact)\n    return absolute_error, number_of_function_evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Test on ExponentialProblem\")\nkmin = 1.0e-15\nkmax = 1.0e1\nx = 1.0\nproblem = nd.ExponentialProblem()\nproblem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "second_derivative_value = problem.second_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeForward.compute_step(\n    second_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\n(\n    absolute_error,\n    number_of_function_evaluations,\n) = compute_first_derivative_GMSW(\n    problem.function,\n    x,\n    problem.first_derivative,\n    kmin,\n    kmax,\n    verbose=True,\n)\nprint(\n    \"x = %.3f, error = %.3e, Func. eval. = %d\"\n    % (x, absolute_error, number_of_function_evaluations)\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Test on ScaledExponentialDerivativeBenchmark\")\nkmin = 1.0e-9\nkmax = 1.0e8\nx = 1.0\nproblem = nd.ScaledExponentialProblem()\nsecond_derivative = problem.get_second_derivative()\nsecond_derivative_value = second_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeForward.compute_step(\n    second_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\n(\n    absolute_error,\n    number_of_function_evaluations,\n) = compute_first_derivative_GMSW(\n    problem.get_function(),\n    x,\n    problem.get_first_derivative(),\n    kmin,\n    kmax,\n    verbose=True,\n)\nprint(\n    \"x = %.3f, error = %.3e, Func. eval. = %d\"\n    % (x, absolute_error, number_of_function_evaluations)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark the method on a collection of test points\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Benchmark on several points on ScaledExponentialProblem\")\nnumber_of_test_points = 100\nproblem = nd.ScaledExponentialProblem()\ninterval = problem.get_interval()\ntest_points = np.linspace(interval[0], interval[1], number_of_test_points)\nkmin = 1.0e-12\nkmax = 1.0e1\nrelative_precision = 1.0e-15\nmethod = GillMurraySaundersWrightMethod(kmin, kmax, relative_precision)\naverage_error, average_feval, _ = nd.benchmark_method(\n    problem.get_function(),\n    problem.get_fifth_derivative(),\n    test_points,\n    method.compute_first_derivative,\n    False,\n)\nprint(\"Average error =\", average_error)\nprint(\"Average number of function evaluations =\", average_feval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the condition error depending on the step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_condition_error(name, function, x, kmin, kmax, number_of_points=1000):\n    # Plot the condition error depending on k.\n    k_array = np.logspace(np.log10(kmin), np.log10(kmax), number_of_points)\n    algorithm = nd.GillMurraySaundersWright(function, x)\n    c_min, c_max = algorithm.get_threshold_min_max()\n    condition_array = np.zeros((number_of_points))\n    for i in range(number_of_points):\n        condition_array[i] = algorithm.compute_condition(k_array[i])\n\n    #\n    pl.figure()\n    pl.title(f\"Condition error of {name} at x = {x}\")\n    pl.plot(k_array, condition_array)\n    pl.plot([kmin, kmax], [c_min] * 2, label=r\"$c_{min}$\")\n    pl.plot([kmin, kmax], [c_max] * 2, label=r\"$c_{max}$\")\n    pl.xlabel(r\"$h_\\Phi$\")\n    pl.ylabel(r\"$c(h_\\Phi$)\")\n    pl.xscale(\"log\")\n    pl.yscale(\"log\")\n    pl.legend(bbox_to_anchor=(1.0, 1.0))\n    pl.tight_layout()\n    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next plot presents the condition error $c(h_\\Phi)$ depending\non $h_\\Phi$.\nThe two horizontal lines represent the minimum and maximum threshold\nvalues.\nWe search for the value of $h_\\Phi$ such that the condition\nerror is between these two limits.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 200\nproblem = nd.ScaledExponentialProblem()\nx = problem.get_x()\nname = problem.get_name()\nfunction = problem.get_function()\nkmin = 1.0e-10\nkmax = 1.0e5\nplot_condition_error(name, function, x, kmin, kmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous plot shows that the condition error is a decreasing function\nof $h_\\Phi$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the end points $x = \\pm \\pi$, because sin has a zero\nsecond derivative at these points.\nThis makes the algorithm fail.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Benchmark on several points on SinProblem\")\nnumber_of_test_points = 100\nproblem = nd.SinProblem()\ninterval = problem.get_interval()\nepsilon = 1.0e-3\ntest_points = np.linspace(\n    interval[0] + epsilon, interval[1] - epsilon, number_of_test_points\n)\nkmin = 1.0e-12\nkmax = 1.0e1\nrelative_precision = 1.0e-15\nmethod = GillMurraySaundersWrightMethod(kmin, kmax, relative_precision)\naverage_error, average_feval, _ = nd.benchmark_method(\n    problem.get_function(),\n    problem.get_fifth_derivative(),\n    test_points,\n    method.compute_first_derivative,\n    False,\n)\nprint(\"Average error =\", average_error)\nprint(\"Average number of function evaluations =\", average_feval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the condition error depending on k.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 200\nproblem = nd.SinProblem()\nx = -np.pi\nname = problem.get_name()\nfunction = problem.get_function()\nkmin = 1.0e-17\nkmax = 1.0e-10\nplot_condition_error(name, function, x, kmin, kmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous plot, we see that there is no satisfactory\nvalue of $h_\\Phi$ for the sin function\nat point $x = -\\pi$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the error depending on the step\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each function, at point x = 1, plot the error vs the step computed\nby the method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_with_GMSW_steps(\n    name,\n    function,\n    first_derivative,\n    x,\n    step_array,\n    kmin,\n    kmax,\n    relative_precision=1.0e-15,\n    verbose=False,\n):\n    \"\"\"\n    Plot the computed error depending on the step for an array of F.D. steps\n\n    Parameters\n    ----------\n    name : str\n        The name of the problem\n    function : function\n        The function.\n    first_derivative : function\n        The exact first derivative of the function\n    x : float\n        The input point where the test is done\n    step_array : list(float)\n        The array of finite difference steps\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    relative_precision : float, optional\n        The relative precision of the function f at the point x.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n    \"\"\"\n\n    function_value = function(x)\n    if function_value == 0.0:\n        raise ValueError(\n            \"The function value is zero: cannot compute \"\n            \"the absolute precision from the relative precision. \"\n            \"Please set the absolute precision specifically.\"\n        )\n    absolute_precision = relative_precision * abs(function_value)\n    algorithm = nd.GillMurraySaundersWright(function, x, absolute_precision)\n    number_of_points = len(step_array)\n    error_array = np.zeros((number_of_points))\n    for i in range(number_of_points):\n        f_prime_approx = algorithm.compute_first_derivative(step_array[i])\n        error_array[i] = abs(f_prime_approx - first_derivative(x))\n\n    step, number_of_iterations = algorithm.find_step(kmin, kmax)\n\n    if verbose:\n        print(name)\n        print(f\"Step h* = {step:.3e} using {number_of_iterations} iterations\")\n\n    minimum_error = np.nanmin(error_array)\n    maximum_error = np.nanmax(error_array)\n\n    pl.figure()\n    pl.plot(step_array, error_array)\n    pl.plot(\n        [step] * 2,\n        [minimum_error, maximum_error],\n        \"--\",\n        label=r\"$\\hat{h}$\",\n    )\n    pl.title(f\"(GMS & W). {name} at point x = {x}\")\n    pl.xlabel(\"h\")\n    pl.ylabel(\"Error\")\n    pl.xscale(\"log\")\n    pl.yscale(\"log\")\n    pl.legend(bbox_to_anchor=(1.0, 1.0))\n    pl.tight_layout()\n    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_benchmark(\n    problem, x, step_array, kmin, kmax, relative_precision=1.0e-15, verbose=False\n):\n    \"\"\"\n    Plot the computed error depending on the step for an array of F.D. steps\n\n    Parameters\n    ----------\n    problem : nd.BenchmarkProblem\n        The problem\n    x : float\n        The input point where the test is done\n    step_array : list(float)\n        The array of finite difference steps\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    relative_precision : float, optional\n        The relative error of the function f at the point x.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n    \"\"\"\n    plot_error_vs_h_with_GMSW_steps(\n        problem.get_name(),\n        problem.get_function(),\n        problem.get_first_derivative(),\n        x,\n        step_array,\n        kmin,\n        kmax,\n        relative_precision,\n        verbose,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ExponentialProblem()\nx = 1.0\nnumber_of_points = 200\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nkmin = 1.0e-15\nkmax = 1.0e-1\nrelative_precision = 1.0e-15\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax, relative_precision, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 12.0\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nx = 1.0\nkmin = 1.0e-10\nkmax = 1.0e8\nstep_array = np.logspace(-10.0, 8.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.LogarithmicProblem()\nx = 1.1\nkmin = 1.0e-14\nkmax = 1.0e-1\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax, relative_precision, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nx = 1.0\nkmin = 1.0e-15\nkmax = 1.0e-1\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SquareRootProblem()\nx = 1.0\nkmin = 1.0e-15\nkmax = 1.0e-1\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax, relative_precision, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.AtanProblem()\nx = 1.1\nkmin = 1.0e-15\nkmax = 1.0e-1\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, kmin, kmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See the history of steps during the bissection search\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In G, M, S & W's method, the bisection algorithm\nproduces a sequence of steps $(k_i)_{1 \\leq i \\leq n_{iter}}$\nwhere $n_{iter} \\in \\mathbb{N}$ is the number of iterations.\nThese steps are meant to converge to an\napproximately optimal step of for the central finite difference formula of the\nsecond derivative.\nThe optimal step $k^\\star$ for the central finite difference formula of the\nsecond derivative can be computed depending on the fourth derivative of the\nfunction.\nIn the next example, we want to compute the absolute error between\neach intermediate step $k_i$ and the exact value $k^\\star$\nto see how close the algorithm gets to the exact step.\nThe list of intermediate steps during the algorithm can be obtained\nthanks to the :meth:`~numericalderivative.GillMurraySaundersWright.get_step_history` method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we print the intermediate steps k during\nthe bissection algorithm that searches for a step such\nthat the L ratio is satisfactory.\nThe algorithm has two different methods to update the step:\n\n- using the mean,\n- using the mean in the logarithm space (this is generally much faster).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_GMSW_step_history(problem, kmin, kmax, logscale):\n    function = problem.get_function()\n    name = problem.get_name()\n    x = problem.get_x()\n    algorithm = nd.GillMurraySaundersWright(function, x, verbose=True)\n    step, number_of_iterations = algorithm.find_step(\n        kmin=kmin, kmax=kmax, logscale=logscale\n    )\n    step_k_history = algorithm.get_step_history()\n    print(f\"Number of iterations = {number_of_iterations}\")\n    print(f\"History of steps k : {step_k_history}\")\n    last_step_k = step_k_history[-1]\n    print(f\"Last step k : {last_step_k}\")\n\n    # Then we compute the exact step, using :meth:`~numericalderivative.SecondDerivativeCentral.compute_step`.\n    fourth_derivative = problem.get_fourth_derivative()\n    fourth_derivative_value = fourth_derivative(x)\n    print(f\"f^(4)(x) = {fourth_derivative_value}\")\n    absolute_precision = 1.0e-16\n    exact_step_k, absolute_error = nd.SecondDerivativeCentral.compute_step(\n        fourth_derivative_value, absolute_precision\n    )\n    print(f\"Optimal step k for f^(2)(x) = {exact_step_k}\")\n\n    # Plot the absolute error between the exact step k and the intermediate k\n    # of the algorithm.\n    error_step_k = [\n        abs(step_k_history[i] - exact_step_k) for i in range(len(step_k_history))\n    ]\n    fig = pl.figure()\n    pl.title(f\"GMSW on {name} at x = {x}. Log scale = {logscale}\")\n    pl.plot(range(len(step_k_history)), error_step_k, \"o-\")\n    pl.xlabel(\"Iterations\")\n    pl.ylabel(r\"$|k_i - k^\\star|$\")\n    pl.yscale(\"log\")\n    ax = fig.gca()\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    pl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, test the logarithmic log scale.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nkmin = 1.0e-15\nkmax = 1.0e3\nlogscale = True\nplot_GMSW_step_history(problem, kmin, kmax, logscale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous figure shows that the algorithm does not necessarily\nreduce the distance to the optimal step when we use the logarithmic scale.\nThe algorithm quickly stops and gets an error approximately equal to $10^{-4}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secondly, test the ordinary scale, using the mean.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nkmin = 1.0e-15\nkmax = 1.0e3\nlogscale = False\nplot_GMSW_step_history(problem, kmin, kmax, logscale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous plot, we see that the error first decreases\ndown to an error approximately equal to $10^{-4}$.\nThen the error slightly increases before the algorithm stops.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}