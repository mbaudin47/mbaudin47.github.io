{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark Dumontet & Vignes method\n\nFind a step which is near to optimal for a centered finite difference \nformula.\n\n## References\n- Dumontet, J., & Vignes, J. (1977). D\u00e9termination du pas optimal dans le calcul des d\u00e9riv\u00e9es sur ordinateur. RAIRO. Analyse num\u00e9rique, 11 (1), 13-25.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tabulate\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark_method(\n    function,\n    derivative_function,\n    test_points,\n    kmin,\n    kmax,\n    relative_precision,\n    verbose=False,\n):\n    \"\"\"\n    Compute the first derivative using Dumontet & Vignes's method.\n\n    Parameters\n    ----------\n    f : function\n        The function.\n    derivative_function : function\n        The exact first derivative of the function.\n    test_points : list(float)\n        The list of x points where the derivative is to be evaluated\n    kmin : float, > 0\n        The minimum finite difference step\n    kmax : float, > 0\n        The maximum finite difference step\n    relative_precision : float, > 0\n        The relative precision of the function value\n    verbose : bool\n        Set to True to print intermediate messages.\n\n    Returns\n    -------\n    average_relative_error : float, > 0\n        The average relative error between the approximate first derivative\n        and the exact first derivative\n    average_feval : float\n        The average number of function evaluations\n    \"\"\"\n    number_of_test_points = len(test_points)\n    relative_error_array = np.zeros(number_of_test_points)\n    feval_array = np.zeros(number_of_test_points)\n    for i in range(number_of_test_points):\n        x = test_points[i]\n        algorithm = nd.DumontetVignes(\n            function, x, relative_precision=relative_precision, verbose=verbose\n        )\n        step, _ = algorithm.compute_step(kmin=kmin, kmax=kmax)\n        f_prime_approx = algorithm.compute_first_derivative(step)\n        number_of_function_evaluations = algorithm.get_number_of_function_evaluations()\n        exact_first_derivative = derivative_function(x)\n        absolute_error = abs(f_prime_approx - exact_first_derivative)\n        relative_error = absolute_error / abs(exact_first_derivative)\n        if verbose:\n            print(\n                \"x = %.3f, abs. error = %.3e, rel. error = %.3e, Func. eval. = %d\"\n                % (x, absolute_error, relative_error, number_of_function_evaluations)\n            )\n        relative_error_array[i] = relative_error\n        feval_array[i] = number_of_function_evaluations\n\n    average_relative_error = np.mean(relative_error_array)\n    average_feval = np.mean(feval_array)\n    if verbose:\n        print(\"Average rel. error =\", average_relative_error)\n        print(\"Average number of function evaluations =\", average_feval)\n    return average_relative_error, average_feval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.1\nbenchmark = nd.LogarithmicDerivativeBenchmark()\nf = benchmark.function\nf_prime = benchmark.first_derivative\nkmin = 1.0e-9\nkmax = 1.0e-3\nrelative_precision = 1.0e-14\nabsolute_error, feval = benchmark_method(\n    f,\n    f_prime,\n    [x],\n    kmin,\n    kmax,\n    relative_precision,\n    verbose=True,\n)\nprint(f\"absolute_error = {absolute_error}\")\nprint(f\"feval = {feval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Benchmark on several points\")\nnumber_of_test_points = 100\ntest_points = np.linspace(0.01, 12.5, number_of_test_points)\nbenchmark = nd.ExponentialDerivativeBenchmark()\nkmin = 1.0e-9\nkmax = 1.0e0\nrelative_precision = 1.0e-14\naverage_relative_error, average_feval = benchmark_method(\n    benchmark.function,\n    benchmark.first_derivative,\n    test_points,\n    kmin,\n    kmax,\n    relative_precision,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a collection of benchmark problems\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "function_list = [\n    [nd.ExponentialDerivativeBenchmark(), [1.0e-10, 1.0e-1]],\n    [nd.LogarithmicDerivativeBenchmark(), [1.0e-10, 1.0e-3]],\n    [nd.SquareRootDerivativeBenchmark(), [1.0e-10, 1.0e-3]],\n    [nd.AtanDerivativeBenchmark(), [1.0e-10, 1.0e0]],\n    [nd.SinDerivativeBenchmark(), [1.0e-10, 1.0e0]],\n    [nd.ScaledExponentialDerivativeBenchmark(), [1.0e-10, 1.0e5]],\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark_problem(benchmark, bracket, relative_precision, verbose=False):\n    function = benchmark.function\n    derivative = benchmark.first_derivative\n    kmin, kmax = bracket\n    average_relative_error, average_feval = benchmark_method(\n        function,\n        derivative,\n        test_points,\n        kmin,\n        kmax,\n        relative_precision,\n        verbose=verbose,\n    )\n    return average_relative_error, average_feval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark DumontetVignes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_test_points = 100\nrelative_precision = 1.0e-14\ntest_points = np.linspace(0.01, 12.5, number_of_test_points)\ndata = []\nnumber_of_functions = len(function_list)\naverage_relative_error_list = []\naverage_feval_list = []\nfor i in range(number_of_functions):\n    benchmark, bracket_k = function_list[i]\n    name = benchmark.name\n    function = benchmark.function\n    derivative = benchmark.first_derivative\n    kmin, kmax = bracket_k\n    average_relative_error, average_feval = benchmark_problem(\n        benchmark, bracket_k, relative_precision\n    )\n    average_relative_error_list.append(average_relative_error)\n    average_feval_list.append(average_feval)\n    data.append(\n        (\n            name,\n            kmin,\n            kmax,\n            average_relative_error,\n            average_feval,\n        )\n    )\ndata.append(\n    [\n        \"Average\",\n        \"-\",\n        \"-\",\n        np.mean(average_relative_error_list),\n        np.mean(average_feval_list),\n    ]\n)\n\ntabulate.tabulate(\n    data,\n    headers=[\"Name\", \"kmin\", \"kmax\", \"Average rel. error\", \"Average func. eval\"],\n    tablefmt=\"html\",\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}