{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A simple demonstration of the methods\n\nFinds a step which is near to optimal for a central finite difference \nformula.\n\n## References\n- Adaptive numerical differentiation. R. S. Stepleman and N. D. Winarsky. Journal: Math. Comp. 33 (1979), 1257-1264 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the function\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define a function.\nHere, we do not use the :class:`~numericalderivative.ScaledExponentialDerivativeBenchmark`\nclass, for demonstration purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp(x):\n    alpha = 1.0e6\n    return np.exp(-x / alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define its exact derivative (for testing purposes only).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp_prime(x):\n    alpha = 1.0e6\n    return -np.exp(-x / alpha) / alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define its exact second derivative (for testing purposes only).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp_second(x):\n    alpha = 1.0e6\n    return np.exp(-x / alpha) / alpha**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the function, its first and second derivatives at the point x.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nexact_f_value = scaled_exp(x)\nprint(\"f(x) = \", exact_f_value)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"f'(x) = \", exact_f_prime_value)\nexact_f_second_value = scaled_exp_second(x)\nprint(\"f''(x) = \", exact_f_second_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compute the first derivative, we use the :class:`~numericalderivative.SteplemanWinarsky`.\nThis class uses the central finite difference formula.\nIn order to compute a step which is approximately optimal,\nwe use the :meth:`~numericalderivative.SteplemanWinarsky.find_step` method.\nThen we use the :meth:`~numericalderivative.SteplemanWinarsky.compute_first_derivative` method\nto compute the approximate first derivative and use the approximately optimal\nstep as input argument.\nThe input argument of :meth:`~numericalderivative.SteplemanWinarsky.find_step` is\nan upper bound of the optimal step (but this is not the case for all\nalgorithms).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e5  # An upper bound of the truly optimal step\nx = 1.0e0\nalgorithm = nd.SteplemanWinarsky(scaled_exp, x)\nh_optimal, iterations = algorithm.find_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DumontetVignes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`DumontetVignes` to compute an approximately\noptimal step.\nFor this algorithm, we must provide an interval which contains the\noptimal step for the approximation of the third derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nalgorithm = nd.DumontetVignes(scaled_exp, x)\nh_optimal, _ = algorithm.find_step(\n    kmin=1.0e-2,\n    kmax=1.0e2,\n)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GillMurraySaundersWright\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`GillMurraySaundersWright` to compute an approximately\noptimal step.\nFor this algorithm, we must provide an interval which contains the\noptimal step for the approximation of the second derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nabsolute_precision = 1.0e-15\nalgorithm = nd.GillMurraySaundersWright(scaled_exp, x, absolute_precision)\nkmin = 1.0e-2\nkmax = 1.0e7\nstep, number_of_iterations = algorithm.find_step(kmin, kmax)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f'=\", step)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function with extra arguments\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some function use extra arguments, such as parameters for examples.\nFor such a function, the `args` optionnal argument can be used\nto pass extra parameters to the function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function with arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_exp_with_args(x, scaling):\n    return np.exp(-x / scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the derivative of a function with extra input arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e5\nx = 1.0e0\nscaling = 1.0e6\nalgorithm = nd.SteplemanWinarsky(my_exp_with_args, x, args=[scaling])\nh_optimal, iterations = algorithm.find_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f''=\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}