{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A simple demonstration of the methods\n\nIn this example, we consider a function and we want to compute the value of the \nfirst derivative at a given point x using a finite difference method.\nTo do this, we need to find a step which is near to optimal for that finite difference\nformula.\nThe goal of this example is to review several algorithms to do this.\n\n+--------------------------------------------------------------------+-------------------------------+\n| **Method**                                                         | **Finite difference formula** |\n+--------------------------------------------------------------------+-------------------------------+\n| Dumontet & Vignes (1977)                                           | central, order 2              |\n+--------------------------------------------------------------------+-------------------------------+\n| Stepleman & Winarsky (1979)                                        | central, order 2              |\n+--------------------------------------------------------------------+-------------------------------+\n| Gill, Murray, Saunders, & Wright (1983)                            | forward, order 1              |\n+--------------------------------------------------------------------+-------------------------------+\n| Shi, Xie, Xuan & Nocedal (2022) for the forward finite diff.       | forward, order 1              |\n+--------------------------------------------------------------------+-------------------------------+\n| Shi, Xie, Xuan & Nocedal (2022) for any finite diff. formula       | arbitrary                     |\n+--------------------------------------------------------------------+-------------------------------+\n\n**Table 1.** Several algorithms to compute the optimal step of a finite difference formula.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the function\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define a function.\nHere, we do not use the :class:`~numericalderivative.ScaledExponentialDerivativeBenchmark`\nclass, for demonstration purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp(x):\n    alpha = 1.0e6\n    return np.exp(-x / alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define its exact derivative (for testing purposes only).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp_prime(x):\n    alpha = 1.0e6\n    return -np.exp(-x / alpha) / alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the function, its first and second derivatives at the point x.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nexact_f_value = scaled_exp(x)\nprint(\"f(x) = \", exact_f_value)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"f'(x) = \", exact_f_prime_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function prints the exact first derivative of the scaled exponential\nfunction, the approximation from the finite difference formula and the\nabsolute and relative errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_results(f_prime_approx, x):\n    \"\"\"\n    Prints the results of a finite difference formula\n\n    Parameters\n    ----------\n    f_prime_approx : float\n        The approximate value of the first derivative\n    x : float\n        The input point\n    \"\"\"\n    exact_f_prime_value = scaled_exp_prime(x)\n    print(f\"Exact f'(x)       = {exact_f_prime_value}\")\n    print(f\"Approximate f'(x) = {f_prime_approx}\")\n    absolute_error = abs(f_prime_approx - exact_f_prime_value)\n    print(f\"Absolute error = {absolute_error:.3e}\")\n    relative_error = absolute_error / abs(exact_f_prime_value)\n    print(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compute the first derivative, we use the :class:`~numericalderivative.SteplemanWinarsky`.\nThis class uses the central finite difference formula.\nIn order to compute a step which is approximately optimal,\nwe use the :meth:`~numericalderivative.SteplemanWinarsky.find_step` method.\nThen we use the :meth:`~numericalderivative.SteplemanWinarsky.compute_first_derivative` method\nto compute the approximate first derivative and use the approximately optimal\nstep as input argument.\nThe input argument of :meth:`~numericalderivative.SteplemanWinarsky.find_step` is\nan upper bound of the optimal step (but this is not the case for all\nalgorithms).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_step = 1.0e5  # An upper bound of the truly optimal step\nx = 1.0e0\nalgorithm = nd.SteplemanWinarsky(scaled_exp, x)\nstep_optimal, iterations = algorithm.find_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", step_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step_optimal)\nprint_results(f_prime_approx, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DumontetVignes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`~numericalderivative.DumontetVignes` to compute an approximately\noptimal step.\nFor this algorithm, we must provide an interval which contains the\noptimal step for the approximation of the third derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nalgorithm = nd.DumontetVignes(scaled_exp, x)\nstep_optimal, _ = algorithm.find_step(\n    kmin=1.0e-2,\n    kmax=1.0e2,\n)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", step_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step_optimal)\nprint_results(f_prime_approx, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GillMurraySaundersWright\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`~numericalderivative.GillMurraySaundersWright` to compute an approximately\noptimal step.\nFor this algorithm, we must provide an interval which contains the\noptimal step for the approximation of the second derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nabsolute_precision = 1.0e-15\nalgorithm = nd.GillMurraySaundersWright(scaled_exp, x, absolute_precision)\nkmin = 1.0e-2\nkmax = 1.0e7\nstep, number_of_iterations = algorithm.find_step(kmin, kmax)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f'=\", step)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step)\nprint_results(f_prime_approx, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ShiXieXuanNocedalForward\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`~numericalderivative.ShiXieXuanNocedalForward` to compute an approximately\noptimal step.\nThis method uses the forward finite difference formula to approximate\nthe first derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\nabsolute_precision = 1.0e-15\nalgorithm = nd.ShiXieXuanNocedalForward(scaled_exp, x, absolute_precision)\ninitial_step = 1.0e5\nstep, number_of_iterations = algorithm.find_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f'=\", step)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step)\nprint_results(f_prime_approx, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ShiXieXuanNocedalGeneral\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use :class:`~numericalderivative.ShiXieXuanNocedalGeneral` to compute an approximately\noptimal step.\nIt uses :class:`~numericalderivative.GeneralFiniteDifference` to implement\na finite difference formula with arbitrary precision order to approximate\nany derivative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0e0\ndifferentiation_order = 1  # First derivative\nformula_accuracy = 2  # Order 2\nformula = nd.GeneralFiniteDifference(\n    scaled_exp,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",  # Central formula\n)\nabsolute_precision = 1.0e-15\nalgorithm = nd.ShiXieXuanNocedalGeneral(formula, absolute_precision)\ninitial_step = 1.0e5\nstep, number_of_iterations = algorithm.find_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f'=\", step)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_derivative(step)\nprint_results(f_prime_approx, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function with extra arguments\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some function use extra arguments, such as parameters for examples.\nFor such a function, the `args` optionnal argument can be used\nto pass extra parameters to the function.\nThe goal of the :class:`~numericalderivative.FunctionWithArguments` class\nis to evaluate such a function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function with arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_exp_with_args(x, scaling):\n    return np.exp(-x * scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the derivative of a function with extra input arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_step = 1.0e5\nx = 1.0e0\nscaling = 1.0e-6\nalgorithm = nd.SteplemanWinarsky(my_exp_with_args, x, args=[scaling])\nstep_optimal, iterations = algorithm.find_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f''=\", step_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}