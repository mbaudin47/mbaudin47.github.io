{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment with Stepleman & Winarsky method\n\nFind a step which is near to optimal for a central finite difference \nformula.\n\n## References\n- Adaptive numerical differentiation\n  R. S. Stepleman and N. D. Winarsky\n  Journal: Math. Comp. 33 (1979), 1257-1264 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd\nfrom matplotlib.ticker import MaxNLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the method on a simple problem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we use the algorithm on the exponential function.\nWe create the :class:`~numericalderivative.SteplemanWinarsky` algorithm using the function and the point x.\nThen we use the :meth:`~numericalderivative.SteplemanWinarsky.compute_step()` method to compute the step,\nusing an upper bound of the step as an initial point of the algorithm.\nFinally, use the :meth:`~numericalderivative.SteplemanWinarsky.compute_first_derivative()` method to compute\nan approximate value of the first derivative using finite differences.\nThe :meth:`~numericalderivative.SteplemanWinarsky.get_number_of_function_evaluations()` method\ncan be used to get the number of function evaluations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\nalgorithm = nd.SteplemanWinarsky(np.exp, x, verbose=True)\ninitial_step = 1.0e0\nstep, number_of_iterations = algorithm.compute_step(initial_step)\nf_prime_approx = algorithm.compute_first_derivative(step)\nfeval = algorithm.get_number_of_function_evaluations()\nf_prime_exact = np.exp(x)  # Since the derivative of exp is exp.\nprint(f\"Computed step = {step:.3e}\")\nprint(f\"Number of iterations = {number_of_iterations}\")\nprint(f\"f_prime_approx = {f_prime_approx}\")\nprint(f\"f_prime_exact = {f_prime_exact}\")\nabsolute_error = abs(f_prime_approx - f_prime_exact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the method on the ScaledExponentialProblem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider this problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nproblem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = problem.get_name()\nx = problem.get_x()\nthird_derivative = problem.get_third_derivative()\nthird_derivative_value = third_derivative(x)\noptimum_step, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value\n)\nprint(f\"Name = {name}, x = {x}\")\nprint(f\"Optimal step for central finite difference formula = {optimum_step}\")\nprint(f\"Minimum absolute error= {absolute_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the error vs h\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = problem.get_x()\nfunction = problem.get_function()\nfirst_derivative = problem.get_first_derivative()\nfinite_difference = nd.FirstDerivativeCentral(function, x)\nnumber_of_points = 200\nstep_array = np.logspace(-7.0, 5.0, number_of_points)\nerror_array = np.zeros((number_of_points))\nfor i in range(number_of_points):\n    f_prime_approx = finite_difference.compute(step_array[i])\n    error_array[i] = abs(f_prime_approx - first_derivative(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, error_array)\npl.plot([optimum_step] * 2, [min(error_array), max(error_array)], label=r\"$h^*$\")\npl.title(f\"Central finite difference on {problem.get_name()} at x = {problem.get_x()}\")\npl.xlabel(\"h\")\npl.ylabel(\"Error\")\npl.xscale(\"log\")\npl.yscale(\"log\")\npl.legend(bbox_to_anchor=(1, 1))\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the algorithm to detect h*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "function = problem.get_function()\nfirst_derivative = problem.get_first_derivative()\nalgorithm = nd.SteplemanWinarsky(function, x, verbose=True)\ninitial_step = 1.0e8\nx = problem.get_x()\nh_optimal, iterations = algorithm.compute_step(initial_step)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nabsolute_error = abs(f_prime_approx - first_derivative(x))\nprint(\"Absolute error = \", absolute_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the absolute difference depending on the step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fd_difference(h1, h2, function, x):\n    \"\"\"\n    Compute the difference of central difference approx. for different step sizes\n\n    This function computes the absolute value of the difference of approximations\n    evaluated at two different steps h1 and h2:\n\n        d = abs(FD(h1) - FD(h2))\n\n    where FD(h) is the approximation from the finite difference formula\n    evaluated from the step h.\n\n    Parameters\n    ----------\n    h1 : float, > 0\n        The first step\n    h2 : float, > 0\n        The second step\n    function : function\n        The function\n    x : float\n        The input point where the derivative is approximated.\n    \"\"\"\n    finite_difference = nd.FirstDerivativeCentral(function, x)\n    f_prime_approx_1 = finite_difference.compute(h1)\n    f_prime_approx_2 = finite_difference.compute(h2)\n    diff_current = abs(f_prime_approx_1 - f_prime_approx_2)\n    return diff_current"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the evolution of | FD(h) - FD(h / 2) | for different values of h\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 200\nstep_array = np.logspace(-7.0, 5.0, number_of_points)\ndiff_array = np.zeros((number_of_points))\nfunction = problem.get_function()\nfor i in range(number_of_points):\n    diff_array[i] = fd_difference(step_array[i], step_array[i] / 2, function, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, diff_array)\npl.title(\"F.D. difference\")\npl.xlabel(\"h\")\npl.ylabel(r\"$|\\operatorname{FD}(h) - \\operatorname{FD}(h / 2) |$\")\npl.xscale(\"log\")\npl.yscale(\"log\")\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the criterion depending on the step\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the evolution of | FD(h) - FD(h / 2) | for different values of h\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 20\nh_initial = 1.0e5\nbeta = 4.0\nstep_array = np.zeros((number_of_points))\ndiff_array = np.zeros((number_of_points))\nfunction = problem.get_function()\nfor i in range(number_of_points):\n    if i == 0:\n        step_array[i] = h_initial / beta\n        diff_array[i] = fd_difference(step_array[i], h_initial, function, x)\n    else:\n        step_array[i] = step_array[i - 1] / beta\n        diff_array[i] = fd_difference(step_array[i], step_array[i - 1], function, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, diff_array, \"o\")\npl.title(\"F.D. difference\")\npl.xlabel(\"h\")\npl.ylabel(r\"$|\\operatorname{FD}(h) - \\operatorname{FD}(h / 2) |$\")\npl.xscale(\"log\")\npl.yscale(\"log\")\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute reference step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = 1.0e-16\nbeta = 4.0\nh_reference = beta * p ** (1 / 3) * x\nprint(\"Suggested h0 = \", h_reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot number of lost digits vs h\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :meth:`~numericalderivative.SteplemanWinarsky.number_of_lost_digits` method\ncomputes the number of lost digits in the approximated derivative\ndepending on the step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h = 1.0e4\nprint(\"Starting h = \", h)\nn_digits = algorithm.number_of_lost_digits(h)\nprint(\"Number of lost digits = \", n_digits)\nthreshold = np.log10(p ** (-1.0 / 3.0) / beta)\nprint(\"Threshold = \", threshold)\n\nstep_zero, iterations = algorithm.search_step_with_bisection(\n    1.0e-5,\n    1.0e7,\n)\nprint(\"step_zero = \", step_zero)\nprint(\"iterations = \", iterations)\n\nestim_step, iterations = algorithm.compute_step(step_zero, beta=1.5)\nprint(\"estim_step = \", estim_step)\nprint(\"iterations = \", iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 200\nstep_array = np.logspace(-7.0, 7.0, number_of_points)\nn_digits_array = np.zeros((number_of_points))\nfor i in range(number_of_points):\n    h = step_array[i]\n    n_digits_array[i] = algorithm.number_of_lost_digits(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_max = algorithm.number_of_lost_digits(h_reference)\npl.figure()\npl.plot(step_array, n_digits_array, label=\"$N(h)$\")\npl.plot([h_reference] * 2, [0.0, y_max], \"--\", label=r\"$h_{ref}$\")\npl.plot([step_zero] * 2, [0.0, y_max], \"--\", label=r\"$h^{(0)}$\")\npl.plot([estim_step] * 2, [0.0, y_max], \"--\", label=r\"$h^\\star$\")\npl.plot(\n    step_array,\n    np.array([threshold] * number_of_points),\n    \":\",\n    label=r\"$T$\",\n)\npl.title(\"Number of digits lost by F.D.\")\npl.xlabel(\"h\")\npl.ylabel(\"$N(h)$\")\npl.xscale(\"log\")\n_ = pl.legend(bbox_to_anchor=(1.0, 1.0))\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, error_array)\npl.plot([step_zero] * 2, [0.0, 1.0e-9], \"--\", label=r\"$h^{(0)}$\")\npl.plot([estim_step] * 2, [0.0, 1.0e-9], \"--\", label=r\"$h^\\star$\")\npl.title(\"Finite difference\")\npl.xlabel(\"h\")\npl.ylabel(\"Error\")\npl.xscale(\"log\")\npl.legend(bbox_to_anchor=(1.0, 1.0))\npl.yscale(\"log\")\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the bisection search\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In some cases, it is difficult to find the initial step.\nIn this case, we can use the bisection algorithm, which can produce\nan initial guess for the step.c\nThis algorithm is based on a search for a suitable step within\nan interval.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test with single point and default parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\nf_prime_approx, number_of_iterations = algorithm.search_step_with_bisection(\n    1.0e-7,\n    1.0e7,\n)\nfeval = algorithm.get_number_of_function_evaluations()\nprint(\"FD(x) = \", f_prime_approx)\nprint(\"number_of_iterations = \", number_of_iterations)\nprint(\"Func. eval = \", feval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See how the algorithm behaves if we use or do not use the log scale\nwhen searching for the optimal step (this can be slower).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\nmaximum_bisection = 53\nprint(\"+ No log scale.\")\nh0, iterations = algorithm.search_step_with_bisection(\n    1.0e-7,\n    1.0e1,\n    maximum_bisection=53,\n    log_scale=False,\n)\nprint(\"Pas initial = \", h0, \", iterations = \", iterations)\nprint(\"+ Log scale.\")\nh0, iterations = algorithm.search_step_with_bisection(\n    1.0e-7,\n    1.0e1,\n    maximum_bisection=53,\n    log_scale=True,\n)\nprint(\"Pas initial = \", h0, \", iterations = \", iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we search for an initial step using bisection,\nthen use this step as an initial guess for the algorithm.\nFinally, we compute an approximation of the first derivative using\nthe finite difference formula.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ExponentialProblem()\nfunction = problem.get_function()\nfirst_derivative = problem.get_first_derivative()\nx = 1.0\nalgorithm = nd.SteplemanWinarsky(function, x, verbose=True)\ninitial_step, estim_relative_error = algorithm.search_step_with_bisection(\n    1.0e-6,\n    100.0 * x,\n    beta=4.0,\n)\nstep, number_of_iterations = algorithm.compute_step(initial_step)\nf_prime_approx = algorithm.compute_first_derivative(step)\nabsolute_error = abs(f_prime_approx - first_derivative(x))\nfeval = algorithm.get_number_of_function_evaluations()\nprint(\n    \"x = %.3f, abs. error = %.3e, estim. rel. error = %.3e, Func. eval. = %d\"\n    % (x, absolute_error, estim_relative_error, number_of_function_evaluations)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See the history of steps during the search\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Stepleman & Winarsky's method, the algorithm\nproduces a sequence of steps $(h_i)_{1 \\leq i \\leq n_{iter}}$\nwhere $n_{iter} \\in \\mathbb{N}$ is the number of iterations.\nThese steps are meant to converge to an\napproximately optimal step of for the central finite difference formula of the\nfirst derivative.\nThe optimal step $h^\\star$ for the central finite difference formula of the\nfirst derivative can be computed depending on the third derivative of the\nfunction.\nIn the next example, we want to compute the absolute error between\neach intermediate step $h_i$ and the exact value $h^\\star$\nto see how close the algorithm gets to the exact step.\nThe list of intermediate steps during the algorithm can be obtained\nthanks to the :meth:`~numericalderivative.SteplemanWinarsky.get_step_history` method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next example, we print the intermediate steps k during\nthe bissection algorithm that searches for a step such\nthat the L ratio is satisfactory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nfunction = problem.get_function()\nname = problem.get_name()\nx = problem.get_x()\nalgorithm = nd.SteplemanWinarsky(function, x, verbose=True)\ninitial_step = 1.0e0\nstep, number_of_iterations = algorithm.compute_step(initial_step)\nstep_h_history = algorithm.get_step_history()\nprint(f\"Number of iterations = {number_of_iterations}\")\nprint(f\"History of steps h : {step_h_history}\")\n# The last step is not the best one, sinces it breaks the monotony\nlast_step_h = step_h_history[-2]\nprint(f\"Last step h : {last_step_h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we compute the exact step, using :meth:`~numericalderivative.ThirdDerivativeCentral.compute_step`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "third_derivative = problem.get_third_derivative()\nthird_derivative_value = third_derivative(x)\nprint(f\"f^(3)(x) = {third_derivative_value}\")\nabsolute_precision = 1.0e-16\nexact_step_k, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value, absolute_precision\n)\nprint(f\"Optimal step k for f^(3)(x) = {exact_step_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the absolute error between the exact step k and the intermediate k\nof the algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "error_step_h = [\n    abs(step_h_history[i] - exact_step_k) for i in range(len(step_h_history))\n]\nfig = pl.figure()\npl.title(f\"Stepleman & Winarsky on {name} at x = {x}\")\npl.plot(range(len(step_h_history)), error_step_h, \"o-\")\npl.xlabel(\"Iterations\")\npl.ylabel(r\"$|h_i - h^\\star|$\")\npl.yscale(\"log\")\nax = fig.gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous figure shows that the algorithm gets closer to the optimal\nvalue of the step k in the early iterations.\nIn the last iterations of the algorithm, the absolute error does not\ncontinue to decrease monotically and produces a final absolute\nerror close to $10^{-3}$.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}