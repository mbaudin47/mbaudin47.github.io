
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_numericalderivative.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_example_plot_numericalderivative.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_numericalderivative.py:


A simple demonstration of the methods
=====================================

Finds a step which is near to optimal for a central finite difference 
formula.

References
----------
- Adaptive numerical differentiation. R. S. Stepleman and N. D. Winarsky. Journal: Math. Comp. 33 (1979), 1257-1264 

.. GENERATED FROM PYTHON SOURCE LINES 16-20

.. code-block:: Python

    import numpy as np
    import pylab as pl
    import numericalderivative as nd








.. GENERATED FROM PYTHON SOURCE LINES 21-23

Define the function
-------------------

.. GENERATED FROM PYTHON SOURCE LINES 25-28

We first define a function.
Here, we do not use the :class:`~numericalderivative.ScaledExponentialDerivativeBenchmark`
class, for demonstration purposes.

.. GENERATED FROM PYTHON SOURCE LINES 31-36

.. code-block:: Python

    def scaled_exp(x):
        alpha = 1.0e6
        return np.exp(-x / alpha)









.. GENERATED FROM PYTHON SOURCE LINES 37-38

Define its exact derivative (for testing purposes only).

.. GENERATED FROM PYTHON SOURCE LINES 38-44

.. code-block:: Python

    def scaled_exp_prime(x):
        alpha = 1.0e6
        return -np.exp(-x / alpha) / alpha










.. GENERATED FROM PYTHON SOURCE LINES 45-46

We evaluate the function, its first and second derivatives at the point x.

.. GENERATED FROM PYTHON SOURCE LINES 48-54

.. code-block:: Python

    x = 1.0e0
    exact_f_value = scaled_exp(x)
    print("f(x) = ", exact_f_value)
    exact_f_prime_value = scaled_exp_prime(x)
    print("f'(x) = ", exact_f_prime_value)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    f(x) =  0.9999990000005
    f'(x) =  -9.999990000005e-07




.. GENERATED FROM PYTHON SOURCE LINES 55-57

SteplemanWinarsky
-----------------

.. GENERATED FROM PYTHON SOURCE LINES 59-69

In order to compute the first derivative, we use the :class:`~numericalderivative.SteplemanWinarsky`.
This class uses the central finite difference formula.
In order to compute a step which is approximately optimal,
we use the :meth:`~numericalderivative.SteplemanWinarsky.find_step` method.
Then we use the :meth:`~numericalderivative.SteplemanWinarsky.compute_first_derivative` method
to compute the approximate first derivative and use the approximately optimal
step as input argument.
The input argument of :meth:`~numericalderivative.SteplemanWinarsky.find_step` is
an upper bound of the optimal step (but this is not the case for all
algorithms).

.. GENERATED FROM PYTHON SOURCE LINES 71-88

.. code-block:: Python

    step_initial = 1.0e5  # An upper bound of the truly optimal step
    x = 1.0e0
    algorithm = nd.SteplemanWinarsky(scaled_exp, x)
    step_optimal, iterations = algorithm.find_step(step_initial)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", step_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(step_optimal)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Optimum h = 1.52587890625
    iterations = 8
    Function evaluations = 20
    f_prime_approx =  -9.999990000142134e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 1.371e-17
    Relative error = 1.371e-11




.. GENERATED FROM PYTHON SOURCE LINES 89-91

DumontetVignes
--------------

.. GENERATED FROM PYTHON SOURCE LINES 93-97

In the next example, we use :class:`~numericalderivative.DumontetVignes` to compute an approximately
optimal step.
For this algorithm, we must provide an interval which contains the
optimal step for the approximation of the third derivative.

.. GENERATED FROM PYTHON SOURCE LINES 99-118

.. code-block:: Python

    x = 1.0e0
    algorithm = nd.DumontetVignes(scaled_exp, x)
    step_optimal, _ = algorithm.find_step(
        kmin=1.0e-2,
        kmax=1.0e2,
    )
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", step_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(step_optimal)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Optimum h = 14.378359910724852
    iterations = 8
    Function evaluations = 24
    f_prime_approx =  -9.999990000345462e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 3.405e-17
    Relative error = 3.405e-11




.. GENERATED FROM PYTHON SOURCE LINES 119-121

GillMurraySaundersWright
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 123-127

In the next example, we use :class:`~numericalderivative.GillMurraySaundersWright` to compute an approximately
optimal step.
For this algorithm, we must provide an interval which contains the
optimal step for the approximation of the second derivative.

.. GENERATED FROM PYTHON SOURCE LINES 129-148

.. code-block:: Python

    x = 1.0e0
    absolute_precision = 1.0e-15
    algorithm = nd.GillMurraySaundersWright(scaled_exp, x, absolute_precision)
    kmin = 1.0e-2
    kmax = 1.0e7
    step, number_of_iterations = algorithm.find_step(kmin, kmax)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h for f'=", step)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(step)
    print("f_prime_approx = ", f_prime_approx)
    exact_f_prime_value = scaled_exp_prime(x)
    print("exact_f_prime_value = ", exact_f_prime_value)
    absolute_error = abs(f_prime_approx - exact_f_prime_value)
    print(f"Absolute error = {absolute_error:.3e}")
    relative_error = absolute_error / abs(exact_f_prime_value)
    print(f"Relative error = {relative_error:.3e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Optimum h for f'= 0.06324695766445854
    Function evaluations = 12
    f_prime_approx =  -9.999989679432984e-07
    exact_f_prime_value =  -9.999990000005e-07
    Absolute error = 3.206e-14
    Relative error = 3.206e-08




.. GENERATED FROM PYTHON SOURCE LINES 149-151

Function with extra arguments
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 153-158

Some function use extra arguments, such as parameters for examples.
For such a function, the `args` optionnal argument can be used
to pass extra parameters to the function.
The goal of the :class:`~numericalderivative.FunctionWithArguments` class
is to evaluate such a function.

.. GENERATED FROM PYTHON SOURCE LINES 161-162

Define a function with arguments.

.. GENERATED FROM PYTHON SOURCE LINES 162-166

.. code-block:: Python

    def my_exp_with_args(x, scaling):
        return np.exp(-x * scaling)









.. GENERATED FROM PYTHON SOURCE LINES 167-168

Compute the derivative of a function with extra input arguments.

.. GENERATED FROM PYTHON SOURCE LINES 170-180

.. code-block:: Python

    step_initial = 1.0e5
    x = 1.0e0
    scaling = 1.0e-6
    algorithm = nd.SteplemanWinarsky(my_exp_with_args, x, args=[scaling])
    step_optimal, iterations = algorithm.find_step(step_initial)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h for f''=", step_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Optimum h for f''= 1.52587890625
    iterations = 8
    Function evaluations = 20





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.003 seconds)


.. _sphx_glr_download_auto_example_plot_numericalderivative.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_numericalderivative.ipynb <plot_numericalderivative.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_numericalderivative.py <plot_numericalderivative.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_numericalderivative.zip <plot_numericalderivative.zip>`
