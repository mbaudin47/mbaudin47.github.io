{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Plot Stepleman & Winarsky's method\n\nFind a step which is near to optimal for a central finite difference \nformula.\n\n## References\n- Adaptive numerical differentiation\n  R. S. Stepleman and N. D. Winarsky\n  Journal: Math. Comp. 33 (1979), 1257-1264 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport tabulate\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the number of lost digits for exp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_points = 100\nx = 1.0\nstep_array = np.logspace(-15.0, 1.0, number_of_points)\nn_digits_array = np.zeros((number_of_points))\nalgorithm = nd.SteplemanWinarsky(np.exp, x)\nfor i in range(number_of_points):\n    h = step_array[i]\n    n_digits_array[i] = algorithm.number_of_lost_digits(h)\n\npl.figure()\npl.plot(step_array, n_digits_array)\npl.title(r\"Number of digits lost by F.D.. $f(x) = \\exp(x)$\")\npl.xlabel(\"h\")\npl.ylabel(\"$N(h)$\")\npl.xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the number of lost digits for sin\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 1.0\nstep_array = np.logspace(-7.0, 2.0, number_of_points)\nn_digits_array = np.zeros((number_of_points))\nalgorithm = nd.SteplemanWinarsky(np.sin, x)\nfor i in range(number_of_points):\n    h = step_array[i]\n    n_digits_array[i] = algorithm.number_of_lost_digits(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.plot(step_array, n_digits_array)\npl.title(r\"Number of digits lost by F.D.. $f(x) = \\sin(x)$\")\npl.xlabel(\"h\")\npl.ylabel(\"$N(h)$\")\npl.xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each function, at point x = 1, plot the error vs the step computed\nby the method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_with_SW_steps(\n    name, function, function_derivative, x, step_array, h_min, h_max, verbose=False\n):\n    \"\"\"\n    Plot the computed error depending on the step for an array of F.D. steps\n\n    Parameters\n    ----------\n    name : str\n        The name of the problem\n    function : function\n        The function.\n    first_derivative : function\n        The exact first derivative of the function\n    x : float\n        The input point where the test is done\n    step_array : list(float)\n        The array of finite difference steps\n    h_min : float, > 0\n        The lower bound to bracket the initial differentiation step.\n    h_max : float, > kmin\n        The upper bound to bracket the initial differentiation step.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n    \"\"\"\n    algorithm = nd.SteplemanWinarsky(function, x)\n    number_of_points = len(step_array)\n    error_array = np.zeros((number_of_points))\n    for i in range(number_of_points):\n        h = step_array[i]\n        f_prime_approx = algorithm.compute_first_derivative(h)\n        error_array[i] = abs(f_prime_approx - function_derivative(x))\n\n    bisection_h0_step, bisection_h0_iteration = algorithm.search_step_with_bisection(\n        h_min, h_max\n    )\n    step, bisection_iterations = algorithm.compute_step(bisection_h0_step)\n\n    if verbose:\n        print(name)\n        print(f\"h_min = {h_min:.3e}, h_max = {h_max:.3e}\")\n        print(\n            \"Bisection h0 = %.3e using %d iterations\"\n            % (bisection_h0_step, bisection_h0_iteration)\n        )\n        print(\"Bisection h* = %.3e using %d iterations\" % (step, bisection_iterations))\n\n    minimum_error = np.nanmin(error_array)\n    maximum_error = np.nanmax(error_array)\n\n    pl.figure()\n    pl.plot(step_array, error_array)\n    pl.plot(\n        [h_min] * 2,\n        [minimum_error, maximum_error],\n        \"--\",\n        label=r\"$h_{\\min}$\",\n    )\n    pl.plot(\n        [h_max] * 2,\n        [minimum_error, maximum_error],\n        \"--\",\n        label=r\"$h_{\\max}$\",\n    )\n    pl.plot(\n        [bisection_h0_step] * 2,\n        [minimum_error, maximum_error],\n        \"--\",\n        label=\"$h_{0}^{(B)}$\",\n    )\n    pl.plot([step] * 2, [minimum_error, maximum_error], \"--\", label=\"$h^{*}$\")\n    pl.title(\"Finite difference : %s at point x = %.0f\" % (name, x))\n    pl.xlabel(\"h\")\n    pl.ylabel(\"Error\")\n    pl.xscale(\"log\")\n    pl.yscale(\"log\")\n    pl.legend(bbox_to_anchor=(1.0, 1.0))\n    pl.subplots_adjust(right=0.8)\n    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_error_vs_h_benchmark(problem, x, step_array, h_min, h_max, verbose=False):\n    \"\"\"\n    Plot the computed error depending on the step for an array of F.D. steps\n\n    Parameters\n    ----------\n    problem : nd.BenchmarkProblem\n        The problem\n    x : float\n        The input point where the test is done\n    step_array : list(float)\n        The array of finite difference steps\n    kmin : float, > 0\n        The minimum step k for the second derivative.\n    kmax : float, > kmin\n        The maximum step k for the second derivative.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n    \"\"\"\n    plot_error_vs_h_with_SW_steps(\n        problem.get_name(),\n        problem.get_function(),\n        problem.get_first_derivative(),\n        x,\n        step_array,\n        h_min,\n        h_max,\n        True,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ExponentialProblem()\nx = 1.0\nnumber_of_points = 1000\nstep_array = np.logspace(-15.0, 1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-10, 1.0e0, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = 12.0\nstep_array = np.logspace(-15.0, 1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-10, 1.0e0)\n\nif False:\n    problem = nd.LogarithmicProblem()\n    x = 1.0\n    plot_error_vs_h_benchmark(problem, x, step_array, 1.0e-15, 1.0e0, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.LogarithmicProblem()\nx = 1.1\nstep_array = np.logspace(-15.0, -1.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-14, 1.0e-1, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nx = 1.0\nstep_array = np.logspace(-15.0, 0.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-15, 1.0e-0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SquareRootProblem()\nx = 1.0\nstep_array = np.logspace(-15.0, 0.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-15, 1.0e-0, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.AtanProblem()\nx = 1.0\nstep_array = np.logspace(-15.0, 0.0, number_of_points)\nplot_error_vs_h_benchmark(problem, x, step_array, 1.0e-15, 1.0e-0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ExponentialProblem()\nprint(\"+ Sensitivity of SW step depending on h0\")\nprint(\"Case 1 : exp\")\nx = 1.0\nfunction = problem.get_function()\nthird_derivative = problem.get_third_derivative()\nalgorithm = nd.SteplemanWinarsky(\n    function,\n    x,\n)\nthird_derivative_value = third_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\nprint(\"absolute_error = %.3e\" % (absolute_error))\nfor h0 in np.logspace(-4, 0, 10):\n    estim_step, iterations = algorithm.compute_step(h0)\n    print(\"h0 = %.3e, Approx. h* = %.3e (%d iterations)\" % (h0, estim_step, iterations))\n\nprint(\"Case 2 : Scaled exp\")\nx = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.ScaledExponentialProblem()\nfunction = problem.get_function()\nthird_derivative = problem.get_third_derivative()\nx = problem.get_x()\nalgorithm = nd.SteplemanWinarsky(function, x)\nthird_derivative_value = third_derivative(x)\noptimal_step, absolute_error = nd.FirstDerivativeCentral.compute_step(\n    third_derivative_value\n)\nprint(\"Exact h* = %.3e\" % (optimal_step))\nprint(\"absolute_error = %.3e\" % (absolute_error))\nfor h0 in np.logspace(0, 6, 10):\n    estim_step, iterations = algorithm.compute_step(h0)\n    print(\"h0 = %.3e, Approx. h* = %.3e (%d iterations)\" % (h0, estim_step, iterations))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}