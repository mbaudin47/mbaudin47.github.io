{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Applies Stepleman & Winarsky method to an OpenTURNS function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport numericalderivative as nd\nfrom openturns.usecases import chaboche_model\nfrom openturns.usecases import cantilever_beam\nfrom openturns.usecases import fireSatellite_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chaboche model\n\nLoad the Chaboche model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cm = chaboche_model.ChabocheModel()\nmodel = ot.Function(cm.model)\ninputMean = cm.inputDistribution.getMean()\nprint(f\"inputMean = {inputMean}\")\nmeanStrain, meanR, meanC, meanGamma = inputMean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the derivative from OpenTURNS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "derivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nderivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the derivative with default step size in OpenTURNS :\n\n```\nderivative =\n[[  1.93789e+09 ]\n[  1           ]\n[  0.0297619   ]\n[ -1.33845e+06 ]]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the gradient function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gradient = model.getGradient()\ngradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with respect to strain\nDefine a function which takes only strain as input and returns a float\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def functionStrain(strain, r, c, gamma):\n    x = [strain, r, c, gamma]\n    sigma = model(x)\n    return sigma[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h* for Strain\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e0\nargs = [meanR, meanC, meanGamma]\nalgorithm = nd.SteplemanWinarsky(functionStrain, meanStrain, args=args, verbose=True)\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(f\"Derivative wrt strain= {f_prime_approx:.17e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with respect to R\nDefine a function which takes only R as input and returns a float\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def functionR(r, strain, c, gamma):\n    x = [strain, r, c, gamma]\n    sigma = model(x)\n    return sigma[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h* for R\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e9\nargs = [meanStrain, meanC, meanGamma]\nalgorithm = nd.SteplemanWinarsky(\n    functionR, meanR, args=args, relative_precision=1.0e-14, verbose=True\n)\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(f\"Optimum h = {h_optimal:e}\")\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(f\"Derivative wrt R= {f_prime_approx:.17e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with respect to C\nDefine a function which takes only C as input and returns a float\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def functionR(c, strain, r, gamma):\n    x = [strain, r, c, gamma]\n    sigma = model(x)\n    return sigma[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h* for C\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e15\nargs = [meanStrain, meanR, meanGamma]\nalgorithm = nd.SteplemanWinarsky(\n    functionR, meanC, args=args, relative_precision=1.0e-14, verbose=True\n)\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(f\"Optimum h = {h_optimal:e}\")\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(f\"Derivative wrt C= {f_prime_approx:.17e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with respect to Gamma\nDefine a function which takes only C as input and returns a float\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def functionGamma(gamma, strain, r, c):\n    x = [strain, r, c, gamma]\n    sigma = model(x)\n    return sigma[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h* for Gamma\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0 = 1.0e0\nargs = [meanStrain, meanR, meanC]\nalgorithm = nd.SteplemanWinarsky(\n    functionGamma, meanGamma, args=args, relative_precision=1.0e-14, verbose=True\n)\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(f\"Optimum h = {h_optimal:e}\")\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(f\"Derivative wrt Gamma= {f_prime_approx:.17e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with respect to [strain, r, c, gamma]\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def genericFunction(x, xIndex, referenceInput):\n    inputDimension = referenceInput.getDimension()\n    complementIndices = [i for i in range(inputDimension) if i != xIndex]\n    modelInput = ot.Point(inputDimension)\n    modelInput[xIndex] = x\n    for i in complementIndices:\n        modelInput[i] = referenceInput[i]\n    y = model(modelInput)\n    modelOutput = y[0]\n    return modelOutput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Default step size for all components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialStep = ot.Point([1.0e0, 1.0e8, 1.0e8, 1.0e0])\ninputDimension = model.getInputDimension()\nreferenceInput = cm.inputDistribution.getMean()\ninputDescription = cm.inputDistribution.getDescription()\noptimalStep = ot.Point(inputDimension)\nfor xIndex in range(inputDimension):\n    inputMarginal = referenceInput[xIndex]\n    print(\n        f\"+ Derivative with respect to {inputDescription[xIndex]} \"\n        f\"at point {inputMarginal}\"\n    )\n    args = [xIndex, referenceInput]\n    algorithm = nd.SteplemanWinarsky(\n        genericFunction,\n        inputMarginal,\n        args=args,\n        relative_precision=1.0e-12,\n        verbose=True,\n    )\n    h_optimal, iterations = algorithm.compute_step(initialStep[xIndex])\n    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()\n    print(f\"    Optimum h = {h_optimal:e}\")\n    print(\"    Iterations =\", iterations)\n    print(\"    Function evaluations =\", number_of_function_evaluations)\n    f_prime_approx = algorithm.compute_first_derivative(h_optimal)\n    print(f\"    Derivative wrt {inputDescription[xIndex]}= {f_prime_approx:.17e}\")\n    # Store optimal point\n    optimalStep[xIndex] = h_optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"The optimal step for central finite difference is\")\nprint(f\"optimalStep = {optimalStep}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the model with the optimal step computed\nfrom SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gradStep = ot.ConstantStep(optimalStep)  # Constant gradient step\nmodel.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))\n# Now the gradient uses the optimal step sizes\nderivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nprint(derivative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with step size computed from SteplemanWinarsky :\n\n```\nderivative =\n[[  1.93789e+09 ]\n [  1           ]\n [  0.0295312   ]  # <- This is a change in the 3d decimal\n [ -1.33845e+06 ]]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the step of a ot.Function using Stepleman & Winarsky\n\nThe function below computes the step of a finite difference formula\napplied to an OpenTURNS function using Stepleman & Winarsky's method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeSteplemanWinarskyStep(\n    model,\n    initial_step,\n    referenceInput,\n    relative_precision=1.0e-16,\n    beta=4.0,\n    verbose=False,\n):\n    \"\"\"\n    Uses SteplemanWinarsky to compute a step size for central finite differences\n\n    The central F.D. is:\n\n    f'(x) ~ (f(x + h) - f(x - h)) / (2 * h)\n\n    Parameters\n    ----------\n    model : ot.Function(inputDimension, 1)\n        The model, which output dimension equal to 1.\n    initial_step : ot.Point(inputDimension)\n        The initial step size.\n    referenceInput : ot.Point(inputDimension)\n        The point X where the derivative is to be computed.\n    relative_precision : float, > 0, optional\n        The absolute relative_precision of evaluation of f. The default is 1.0e-16.\n    verbose : bool, optional\n        Set to True to print intermediate messages. The default is False.\n\n    Returns\n    -------\n    optimalStep : ot.Point(inputDimension)\n        The optimal step for central finite difference.\n    \"\"\"\n\n    def genericFunction(x, xIndex, referenceInput):\n        if verbose:\n            print(\"x = \", x)\n        inputDimension = referenceInput.getDimension()\n        complementIndices = [i for i in range(inputDimension) if i != xIndex]\n        modelInput = ot.Point(inputDimension)\n        modelInput[xIndex] = x\n        for i in complementIndices:\n            modelInput[i] = referenceInput[i]\n        y = model(modelInput)\n        modelOutput = y[0]\n        if verbose:\n            print(\"y = \", y)\n        return modelOutput\n\n    inputDimension = model.getInputDimension()\n    inputDescription = model.getInputDescription()\n    optimalStep = ot.Point(inputDimension)\n    if verbose:\n        print(f\"Input dimension = {inputDimension}\")\n        print(f\"Input description = {inputDescription}\")\n    for xIndex in range(inputDimension):\n        inputMarginal = referenceInput[xIndex]\n        if verbose:\n            print(\n                f\"+ Derivative with respect to {inputDescription[xIndex]} \"\n                f\"at point {inputMarginal}\"\n            )\n        args = [xIndex, referenceInput]\n        algorithm = nd.SteplemanWinarsky(\n            genericFunction,\n            inputMarginal,\n            args=args,\n            relative_precision=relative_precision,\n            verbose=verbose,\n        )\n        h_optimal, iterations = algorithm.compute_step(\n            initial_step[xIndex],\n            beta=beta,\n        )\n        number_of_function_evaluations = algorithm.get_number_of_function_evaluations()\n        f_prime_approx = algorithm.compute_first_derivative(h_optimal)\n        if verbose:\n            print(f\"    Optimum h = {h_optimal:e}\")\n            print(\"    Iterations =\", iterations)\n            print(\"    Function evaluations =\", number_of_function_evaluations)\n            print(\n                f\"    Derivative wrt {inputDescription[xIndex]} = {f_prime_approx:.17e}\"\n            )\n        # Store optimal point\n        optimalStep[xIndex] = h_optimal\n    return optimalStep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cantilever beam model\n\nApply the same method to the cantilever beam model\nLoad the cantilever beam model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cb = cantilever_beam.CantileverBeam()\nmodel = ot.Function(cb.model)\ninputMean = cb.distribution.getMean()\nprint(f\"inputMean = {inputMean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the derivative from OpenTURNS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "derivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nprint(f\"{derivative}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with OpenTURNS's default step size :\n\n```\nderivative =\n[[ -2.53725e-12 ]\n [  0.000567037 ]\n [  0.200131    ]\n [ -1.17008e+06 ]]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the CantileverBeam model has an exact gradient in OpenTURNS,\nbecause it is symbolic.\nHence, using the optimal step should not make any difference.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute step from SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialStep = ot.Point(inputMean) / 2\noptimalStep = computeSteplemanWinarskyStep(model, initialStep, inputMean)\nprint(\"The optimal step for central finite difference is\")\nprint(f\"optimalStep = {optimalStep}\")\n\ngradStep = ot.ConstantStep(optimalStep)  # Constant gradient step\nmodel.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))\n# Now the gradient uses the optimal step sizes\nderivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nprint(f\"{derivative}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Derivative with SteplemanWinarskyStep\n\n```\nderivative =\n[[ -2.53725e-12 ]\n [  0.000567037 ]\n [  0.200131    ]\n [ -1.17008e+06 ]]\n```\nWe see that this is the correct step size.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fire satellite model\n\nLoad the Fire satellite use case with total torque as output\nPrint the derivative from OpenTURNS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m = fireSatellite_function.FireSatelliteModel()\ninputMean = m.inputDistribution.getMean()\nm.modelTotalTorque.setInputDescription(\n    [\"H\", \"Pother\", \"Fs\", \"theta\", \"Lsp\", \"q\", \"RD\", \"Lalpha\", \"Cd\"]\n)\nm.modelTotalTorque.setOutputDescription([\"Total torque\"])\nmodel = ot.Function(m.modelTotalTorque)\nderivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nprint(f\"{derivative}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From OpenTURNS with default settings:\n\n```\nderivative =\n9x1\n[[ -4.78066e-10 ]\n [ -3.46945e-13 ]\n [  2.30666e-09 ]\n [  4.90736e-09 ]\n [  1.61453e-06 ]\n [  2.15271e-06 ]\n [  5.17815e-10 ]\n [  0.00582819  ]\n [  0.0116564   ]]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a specific difficulty with FireSatellite model for the derivative\nwith respect to Pother.\nIndeed, the gradient of the TotalTorque with respect to Pother is close\nto zero.\nFurthermore, the nominal value (mean) of Pother is 1000.\nTherefore, in order to get a sufficiently large number of lost digits,\nthe algorithm is forced to use a very large step h, close to 10^4.\nBut this leads to a negative value of Pother - h, which produces\na math domain error.\nIn other words, the model has an input range which is ignored by the\nalgorithm.\nTo solve this issue the interval which defines the set of\npossible values of x should be introduced.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute step from SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialStep = ot.Point(inputMean) / 2\noptimalStep = computeSteplemanWinarskyStep(\n    model,\n    initialStep,\n    inputMean,\n    verbose=True,\n    relative_precision=1.0e-16,\n)\nprint(\"The optimal step for central finite difference is\")\nprint(f\"optimalStep = {optimalStep}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gradStep = ot.ConstantStep(optimalStep)  # Constant gradient step\nmodel.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))\n# Now the gradient uses the optimal step sizes\nderivative = model.gradient(inputMean)\nprint(f\"derivative = \")\nprint(f\"{derivative}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From SteplemanWinarsky\n\n```\nderivative =\n9x1\n[[ -4.78157e-10 ]\n [ -2.91776e-13 ]  # <- This is a minor change\n [  2.30671e-09 ]\n [  4.90745e-09 ]\n [  1.61453e-06 ]\n [  2.15271e-06 ]\n [  5.17805e-10 ]\n [  0.00582819  ]\n [  0.0116564   ]]\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}