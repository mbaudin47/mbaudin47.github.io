<!DOCTYPE html>

<html lang="python" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Applies Stepleman &amp; Winarsky method to an OpenTURNS function &#8212; numericalderivative 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=8cfa8c60"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="icon" href="../_static/Icon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Use the benchmark problems" href="plot_use_benchmark.html" />
    <link rel="prev" title="A simple demonstration of the methods" href="plot_numericalderivative.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-example-plot-openturns-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="applies-stepleman-winarsky-method-to-an-openturns-function">
<span id="sphx-glr-auto-example-plot-openturns-py"></span><h1>Applies Stepleman &amp; Winarsky method to an OpenTURNS function<a class="headerlink" href="#applies-stepleman-winarsky-method-to-an-openturns-function" title="Link to this heading">¶</a></h1>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">openturns</span> <span class="k">as</span> <span class="nn">ot</span>
<span class="kn">import</span> <span class="nn">numericalderivative</span> <span class="k">as</span> <span class="nn">nd</span>
<span class="kn">from</span> <span class="nn">openturns.usecases</span> <span class="kn">import</span> <span class="n">chaboche_model</span>
<span class="kn">from</span> <span class="nn">openturns.usecases</span> <span class="kn">import</span> <span class="n">cantilever_beam</span>
<span class="kn">from</span> <span class="nn">openturns.usecases</span> <span class="kn">import</span> <span class="n">fireSatellite_function</span>
</pre></div>
</div>
<section id="chaboche-model">
<h2>Chaboche model<a class="headerlink" href="#chaboche-model" title="Link to this heading">¶</a></h2>
<p>Load the Chaboche model</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">chaboche_model</span><span class="o">.</span><span class="n">ChabocheModel</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">inputMean</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">inputDistribution</span><span class="o">.</span><span class="n">getMean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;inputMean = </span><span class="si">{</span><span class="n">inputMean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">meanStrain</span><span class="p">,</span> <span class="n">meanR</span><span class="p">,</span> <span class="n">meanC</span><span class="p">,</span> <span class="n">meanGamma</span> <span class="o">=</span> <span class="n">inputMean</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputMean = [0.035,7.5e+08,2.75e+09,10]
</pre></div>
</div>
<p>Print the derivative from OpenTURNS</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="n">derivative</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>derivative =
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<p>[[  1.93789e+09 ]<br>
 [  1           ]<br>
 [  0.0297619   ]<br>
 [ -1.33845e+06 ]]</p>
</div>
<br />
<br /><p>Here is the derivative with default step size in OpenTURNS :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="p">[[</span>  <span class="mf">1.93789e+09</span> <span class="p">]</span>
<span class="p">[</span>  <span class="mi">1</span>           <span class="p">]</span>
<span class="p">[</span>  <span class="mf">0.0297619</span>   <span class="p">]</span>
<span class="p">[</span> <span class="o">-</span><span class="mf">1.33845e+06</span> <span class="p">]]</span>
</pre></div>
</div>
<p>Print the gradient function</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">gradient</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getGradient</span><span class="p">()</span>
<span class="n">gradient</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<p>No analytical gradient available. Try using finite difference instead.</p>
</div>
<br />
<br /><p>Derivative with respect to strain
Define a function which takes only strain as input and returns a float</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">functionStrain</span><span class="p">(</span><span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Algorithm to detect h* for Strain</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="mf">1.0e0</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">meanR</span><span class="p">,</span> <span class="n">meanC</span><span class="p">,</span> <span class="n">meanGamma</span><span class="p">]</span>
<span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span><span class="n">functionStrain</span><span class="p">,</span> <span class="n">meanStrain</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
<span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimum h =&quot;</span><span class="p">,</span> <span class="n">h_optimal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
<span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Derivative wrt strain= </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ find_step()
initial_step=1.000e+00
  number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=2.1296e+12
  number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=2.6233e+09
  number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=1.2076e+08
  number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=7.4021e+06
  number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=4.6207e+05
  number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=2.8877e+04
  number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=1.8048e+03
  number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=1.1280e+02
  number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=7.0430e+00
  number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=4.5312e-01
  number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=0.0000e+00
  Stop because zero difference.
Optimum h = 9.5367431640625e-07
iterations = 10
Function evaluations = 24
Derivative wrt strain= 1.93789224675000000e+09
</pre></div>
</div>
<p>Derivative with respect to R
Define a function which takes only R as input and returns a float</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">functionR</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">strain</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Algorithm to detect h* for R</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="mf">1.0e9</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">meanStrain</span><span class="p">,</span> <span class="n">meanC</span><span class="p">,</span> <span class="n">meanGamma</span><span class="p">]</span>
<span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span><span class="n">functionR</span><span class="p">,</span> <span class="n">meanR</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
<span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimum h = </span><span class="si">{</span><span class="n">h_optimal</span><span class="si">:</span><span class="s2">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
<span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Derivative wrt R= </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ find_step()
initial_step=1.000e+09
  number_of_iterations=0, h=2.5000e+08, |FD(h_current) - FD(h_previous)|=2.2204e-16
  number_of_iterations=1, h=6.2500e+07, |FD(h_current) - FD(h_previous)|=2.2204e-16
  number_of_iterations=2, h=1.5625e+07, |FD(h_current) - FD(h_previous)|=0.0000e+00
  Stop because zero difference.
Optimum h = 6.250000e+07
iterations = 2
Function evaluations = 8
Derivative wrt R= 1.00000000000000000e+00
</pre></div>
</div>
<p>Derivative with respect to C
Define a function which takes only C as input and returns a float</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">functionR</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Algorithm to detect h* for C</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="mf">1.0e15</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">meanStrain</span><span class="p">,</span> <span class="n">meanR</span><span class="p">,</span> <span class="n">meanGamma</span><span class="p">]</span>
<span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span><span class="n">functionR</span><span class="p">,</span> <span class="n">meanC</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
<span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimum h = </span><span class="si">{</span><span class="n">h_optimal</span><span class="si">:</span><span class="s2">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
<span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Derivative wrt C= </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ find_step()
initial_step=1.000e+15
  number_of_iterations=0, h=2.5000e+14, |FD(h_current) - FD(h_previous)|=3.4694e-18
  number_of_iterations=1, h=6.2500e+13, |FD(h_current) - FD(h_previous)|=3.4694e-18
  number_of_iterations=2, h=1.5625e+13, |FD(h_current) - FD(h_previous)|=0.0000e+00
  Stop because zero difference.
Optimum h = 6.250000e+13
iterations = 2
Function evaluations = 8
Derivative wrt C= 2.95311910281286574e-02
</pre></div>
</div>
<p>Derivative with respect to Gamma
Define a function which takes only C as input and returns a float</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">functionGamma</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">strain</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Algorithm to detect h* for Gamma</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="mf">1.0e0</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">meanStrain</span><span class="p">,</span> <span class="n">meanR</span><span class="p">,</span> <span class="n">meanC</span><span class="p">]</span>
<span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span><span class="n">functionGamma</span><span class="p">,</span> <span class="n">meanGamma</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
<span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimum h = </span><span class="si">{</span><span class="n">h_optimal</span><span class="si">:</span><span class="s2">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
<span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Derivative wrt Gamma= </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ find_step()
initial_step=1.000e+00
  number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=1.2204e+02
  number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=7.6272e+00
  number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=4.7670e-01
  number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=2.9785e-02
  number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.8768e-03
  number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=1.2207e-04
  number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=2.4414e-04
  Stop because no monotony anymore.
Optimum h = 2.441406e-04
iterations = 6
Function evaluations = 16
Derivative wrt Gamma= -1.33845466918945312e+06
</pre></div>
</div>
<p>Derivative with respect to [strain, r, c, gamma]</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">genericFunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xIndex</span><span class="p">,</span> <span class="n">referenceInput</span><span class="p">):</span>
    <span class="n">inputDimension</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()</span>
    <span class="n">complementIndices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">xIndex</span><span class="p">]</span>
    <span class="n">modelInput</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span>
    <span class="n">modelInput</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">complementIndices</span><span class="p">:</span>
        <span class="n">modelInput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">modelInput</span><span class="p">)</span>
    <span class="n">modelOutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">modelOutput</span>
</pre></div>
</div>
<p>Default step size for all components</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">initialStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">([</span><span class="mf">1.0e0</span><span class="p">,</span> <span class="mf">1.0e8</span><span class="p">,</span> <span class="mf">1.0e8</span><span class="p">,</span> <span class="mf">1.0e0</span><span class="p">])</span>
<span class="n">inputDimension</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getInputDimension</span><span class="p">()</span>
<span class="n">referenceInput</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">inputDistribution</span><span class="o">.</span><span class="n">getMean</span><span class="p">()</span>
<span class="n">inputDescription</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">inputDistribution</span><span class="o">.</span><span class="n">getDescription</span><span class="p">()</span>
<span class="n">optimalStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span>
<span class="k">for</span> <span class="n">xIndex</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">):</span>
    <span class="n">inputMarginal</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;+ Derivative with respect to </span><span class="si">{</span><span class="n">inputDescription</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;at point </span><span class="si">{</span><span class="n">inputMarginal</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">xIndex</span><span class="p">,</span> <span class="n">referenceInput</span><span class="p">]</span>
    <span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span>
        <span class="n">genericFunction</span><span class="p">,</span>
        <span class="n">inputMarginal</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span><span class="n">initialStep</span><span class="p">[</span><span class="n">xIndex</span><span class="p">])</span>
    <span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Optimum h = </span><span class="si">{</span><span class="n">h_optimal</span><span class="si">:</span><span class="s2">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
    <span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Derivative wrt </span><span class="si">{</span><span class="n">inputDescription</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span><span class="si">}</span><span class="s2">= </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Store optimal point</span>
    <span class="n">optimalStep</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_optimal</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ Derivative with respect to Strain at point 0.035
+ find_step()
initial_step=1.000e+00
  number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=2.1296e+12
  number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=2.6233e+09
  number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=1.2076e+08
  number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=7.4021e+06
  number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=4.6207e+05
  number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=2.8877e+04
  number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=1.8048e+03
  number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=1.1280e+02
  number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=7.0430e+00
  number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=4.5312e-01
  number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=0.0000e+00
  Stop because zero difference.
    Optimum h = 9.536743e-07
    Iterations = 10
    Function evaluations = 24
    Derivative wrt Strain= 1.93789224675000000e+09
+ Derivative with respect to R at point 750000000.0000002
+ find_step()
initial_step=1.000e+08
  number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=0.0000e+00
  Stop because zero difference.
    Optimum h = 1.000000e+08
    Iterations = 0
    Function evaluations = 4
    Derivative wrt R= 1.00000000000000000e+00
+ Derivative with respect to C at point 2750000000.0
+ find_step()
initial_step=1.000e+08
  number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=1.1935e-15
  number_of_iterations=1, h=6.2500e+06, |FD(h_current) - FD(h_previous)|=2.3870e-15
  Stop because no monotony anymore.
    Optimum h = 2.500000e+07
    Iterations = 1
    Function evaluations = 6
    Derivative wrt C= 2.95311910281300556e-02
+ Derivative with respect to Gamma at point 10.0
+ find_step()
initial_step=1.000e+00
  number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=1.2204e+02
  number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=7.6272e+00
  number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=4.7670e-01
  number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=2.9785e-02
  number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.8768e-03
  number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=1.2207e-04
  number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=2.4414e-04
  Stop because no monotony anymore.
    Optimum h = 2.441406e-04
    Iterations = 6
    Function evaluations = 16
    Derivative wrt Gamma= -1.33845466918945312e+06
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal step for central finite difference is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimalStep = </span><span class="si">{</span><span class="n">optimalStep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The optimal step for central finite difference is
optimalStep = [9.53674e-07,1e+08,2.5e+07,0.000244141]
</pre></div>
</div>
<p>Configure the model with the optimal step computed
from SteplemanWinarsky</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">gradStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ConstantStep</span><span class="p">(</span><span class="n">optimalStep</span><span class="p">)</span>  <span class="c1"># Constant gradient step</span>
<span class="n">model</span><span class="o">.</span><span class="n">setGradient</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">CenteredFiniteDifferenceGradient</span><span class="p">(</span><span class="n">gradStep</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">getEvaluation</span><span class="p">()))</span>
<span class="c1"># Now the gradient uses the optimal step sizes</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">derivative</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>derivative =
[[  1.93789e+09 ]
 [  1           ]
 [  0.0295312   ]
 [ -1.33845e+06 ]]
</pre></div>
</div>
<p>Derivative with step size computed from SteplemanWinarsky :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="p">[[</span>  <span class="mf">1.93789e+09</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mi">1</span>           <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.0295312</span>   <span class="p">]</span>  <span class="c1"># &lt;- This is a change in the 3d decimal</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">1.33845e+06</span> <span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="compute-the-step-of-a-ot-function-using-stepleman-winarsky">
<h2>Compute the step of a ot.Function using Stepleman &amp; Winarsky<a class="headerlink" href="#compute-the-step-of-a-ot-function-using-stepleman-winarsky" title="Link to this heading">¶</a></h2>
<p>The function below computes the step of a finite difference formula
applied to an OpenTURNS function using Stepleman &amp; Winarsky's method.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">computeSteplemanWinarskyStep</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">initial_step</span><span class="p">,</span>
    <span class="n">referenceInput</span><span class="p">,</span>
    <span class="n">beta</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses SteplemanWinarsky to compute a step size for central finite differences</span>

<span class="sd">    The central F.D. is:</span>

<span class="sd">    f&#39;(x) ~ (f(x + h) - f(x - h)) / (2 * h)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : ot.Function(inputDimension, 1)</span>
<span class="sd">        The model, which output dimension equal to 1.</span>
<span class="sd">    initial_step : ot.Point(inputDimension)</span>
<span class="sd">        The initial step size.</span>
<span class="sd">    referenceInput : ot.Point(inputDimension)</span>
<span class="sd">        The point X where the derivative is to be computed.</span>
<span class="sd">    verbose : bool, optional</span>
<span class="sd">        Set to True to print intermediate messages. The default is False.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    optimalStep : ot.Point(inputDimension)</span>
<span class="sd">        The optimal step for central finite difference.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">genericFunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xIndex</span><span class="p">,</span> <span class="n">referenceInput</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x = &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">inputDimension</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()</span>
        <span class="n">complementIndices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">xIndex</span><span class="p">]</span>
        <span class="n">modelInput</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span>
        <span class="n">modelInput</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">complementIndices</span><span class="p">:</span>
            <span class="n">modelInput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">modelInput</span><span class="p">)</span>
        <span class="n">modelOutput</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y = &quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">modelOutput</span>

    <span class="n">inputDimension</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getInputDimension</span><span class="p">()</span>
    <span class="n">inputDescription</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getInputDescription</span><span class="p">()</span>
    <span class="n">optimalStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input dimension = </span><span class="si">{</span><span class="n">inputDimension</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input description = </span><span class="si">{</span><span class="n">inputDescription</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">xIndex</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">):</span>
        <span class="n">inputMarginal</span> <span class="o">=</span> <span class="n">referenceInput</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;+ Derivative with respect to </span><span class="si">{</span><span class="n">inputDescription</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;at point </span><span class="si">{</span><span class="n">inputMarginal</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">xIndex</span><span class="p">,</span> <span class="n">referenceInput</span><span class="p">]</span>
        <span class="n">algorithm</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">SteplemanWinarsky</span><span class="p">(</span>
            <span class="n">genericFunction</span><span class="p">,</span>
            <span class="n">inputMarginal</span><span class="p">,</span>
            <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">h_optimal</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">find_step</span><span class="p">(</span>
            <span class="n">initial_step</span><span class="p">[</span><span class="n">xIndex</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">number_of_function_evaluations</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">get_number_of_function_evaluations</span><span class="p">()</span>
        <span class="n">f_prime_approx</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">compute_first_derivative</span><span class="p">(</span><span class="n">h_optimal</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Optimum h = </span><span class="si">{</span><span class="n">h_optimal</span><span class="si">:</span><span class="s2">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Iterations =&quot;</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Function evaluations =&quot;</span><span class="p">,</span> <span class="n">number_of_function_evaluations</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;    Derivative wrt </span><span class="si">{</span><span class="n">inputDescription</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">f_prime_approx</span><span class="si">:</span><span class="s2">.17e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Store optimal point</span>
        <span class="n">optimalStep</span><span class="p">[</span><span class="n">xIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_optimal</span>
    <span class="k">return</span> <span class="n">optimalStep</span>
</pre></div>
</div>
</section>
<section id="cantilever-beam-model">
<h2>Cantilever beam model<a class="headerlink" href="#cantilever-beam-model" title="Link to this heading">¶</a></h2>
<p>Apply the same method to the cantilever beam model
Load the cantilever beam model</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">cb</span> <span class="o">=</span> <span class="n">cantilever_beam</span><span class="o">.</span><span class="n">CantileverBeam</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">cb</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">inputMean</span> <span class="o">=</span> <span class="n">cb</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">getMean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;inputMean = </span><span class="si">{</span><span class="n">inputMean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputMean = [6.70455e+10,300,2.55,1.45385e-07]
</pre></div>
</div>
<p>Print the derivative from OpenTURNS</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">derivative</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>derivative =
[[ -2.53725e-12 ]
 [  0.000567037 ]
 [  0.200131    ]
 [ -1.17008e+06 ]]
</pre></div>
</div>
<p>Derivative with OpenTURNS's default step size :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">2.53725e-12</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.000567037</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.200131</span>    <span class="p">]</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">1.17008e+06</span> <span class="p">]]</span>
</pre></div>
</div>
<p>Notice that the CantileverBeam model has an exact gradient in OpenTURNS,
because it is symbolic.
Hence, using the optimal step should not make any difference.</p>
<p>Compute step from SteplemanWinarsky</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">initialStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">optimalStep</span> <span class="o">=</span> <span class="n">computeSteplemanWinarskyStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">initialStep</span><span class="p">,</span> <span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal step for central finite difference is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimalStep = </span><span class="si">{</span><span class="n">optimalStep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">gradStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ConstantStep</span><span class="p">(</span><span class="n">optimalStep</span><span class="p">)</span>  <span class="c1"># Constant gradient step</span>
<span class="n">model</span><span class="o">.</span><span class="n">setGradient</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">CenteredFiniteDifferenceGradient</span><span class="p">(</span><span class="n">gradStep</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">getEvaluation</span><span class="p">()))</span>
<span class="c1"># Now the gradient uses the optimal step sizes</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">derivative</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The optimal step for central finite difference is
optimalStep = [127879,37.5,4.86374e-06,2.77299e-13]
derivative =
[[ -2.53725e-12 ]
 [  0.000567037 ]
 [  0.200131    ]
 [ -1.17008e+06 ]]
</pre></div>
</div>
<p>Derivative with SteplemanWinarskyStep</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">2.53725e-12</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.000567037</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.200131</span>    <span class="p">]</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">1.17008e+06</span> <span class="p">]]</span>
</pre></div>
</div>
<p>We see that this is the correct step size.</p>
</section>
<section id="fire-satellite-model">
<h2>Fire satellite model<a class="headerlink" href="#fire-satellite-model" title="Link to this heading">¶</a></h2>
<p>Load the Fire satellite use case with total torque as output
Print the derivative from OpenTURNS</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">fireSatellite_function</span><span class="o">.</span><span class="n">FireSatelliteModel</span><span class="p">()</span>
<span class="n">inputMean</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">inputDistribution</span><span class="o">.</span><span class="n">getMean</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">modelTotalTorque</span><span class="o">.</span><span class="n">setInputDescription</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;H&quot;</span><span class="p">,</span> <span class="s2">&quot;Pother&quot;</span><span class="p">,</span> <span class="s2">&quot;Fs&quot;</span><span class="p">,</span> <span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="s2">&quot;Lsp&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="s2">&quot;RD&quot;</span><span class="p">,</span> <span class="s2">&quot;Lalpha&quot;</span><span class="p">,</span> <span class="s2">&quot;Cd&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">modelTotalTorque</span><span class="o">.</span><span class="n">setOutputDescription</span><span class="p">([</span><span class="s2">&quot;Total torque&quot;</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">modelTotalTorque</span><span class="p">)</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">derivative</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>derivative =
9x1
[[ -4.78066e-10 ]
 [ -3.46945e-13 ]
 [  2.30666e-09 ]
 [  4.90736e-09 ]
 [  1.61453e-06 ]
 [  2.15271e-06 ]
 [  5.17815e-10 ]
 [  0.00582819  ]
 [  0.0116564   ]]
</pre></div>
</div>
<p>From OpenTURNS with default settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="mi">9</span><span class="n">x1</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">4.78066e-10</span> <span class="p">]</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">3.46945e-13</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">2.30666e-09</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">4.90736e-09</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">1.61453e-06</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">2.15271e-06</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">5.17815e-10</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.00582819</span>  <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.0116564</span>   <span class="p">]]</span>
</pre></div>
</div>
<p>There is a specific difficulty with FireSatellite model for the derivative
with respect to Pother.
Indeed, the gradient of the TotalTorque with respect to Pother is close
to zero.
Furthermore, the nominal value (mean) of Pother is 1000.
Therefore, in order to get a sufficiently large number of lost digits,
the algorithm is forced to use a very large step h, close to 10^4.
But this leads to a negative value of Pother - h, which produces
a math domain error.
In other words, the model has an input range which is ignored by the
algorithm.
To solve this issue the interval which defines the set of
possible values of x should be introduced.</p>
<p>Compute step from SteplemanWinarsky</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">initialStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Point</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">optimalStep</span> <span class="o">=</span> <span class="n">computeSteplemanWinarskyStep</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">initialStep</span><span class="p">,</span>
    <span class="n">inputMean</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal step for central finite difference is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimalStep = </span><span class="si">{</span><span class="n">optimalStep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The optimal step for central finite difference is
optimalStep = [34.3323,1.95312,0.683594,0.00732422,0.000244141,0.000244141,0.15625,0.000244141,0.00012207]
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">gradStep</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ConstantStep</span><span class="p">(</span><span class="n">optimalStep</span><span class="p">)</span>  <span class="c1"># Constant gradient step</span>
<span class="n">model</span><span class="o">.</span><span class="n">setGradient</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">CenteredFiniteDifferenceGradient</span><span class="p">(</span><span class="n">gradStep</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">getEvaluation</span><span class="p">()))</span>
<span class="c1"># Now the gradient uses the optimal step sizes</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">inputMean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;derivative = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">derivative</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>derivative =
9x1
[[ -4.78157e-10 ]
 [ -2.91776e-13 ]
 [  2.30671e-09 ]
 [  4.90745e-09 ]
 [  1.61453e-06 ]
 [  2.15271e-06 ]
 [  5.17805e-10 ]
 [  0.00582819  ]
 [  0.0116564   ]]
</pre></div>
</div>
<p>From SteplemanWinarsky</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">derivative</span> <span class="o">=</span>
<span class="mi">9</span><span class="n">x1</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">4.78157e-10</span> <span class="p">]</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">2.91776e-13</span> <span class="p">]</span>  <span class="c1"># &lt;- This is a minor change</span>
 <span class="p">[</span>  <span class="mf">2.30671e-09</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">4.90745e-09</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">1.61453e-06</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">2.15271e-06</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">5.17805e-10</span> <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.00582819</span>  <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.0116564</span>   <span class="p">]]</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 0.220 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-example-plot-openturns-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/35951d828f02504b524fe84660d61565/plot_openturns.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_openturns.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/1ff9c61ad360161299a50257e56fe43a/plot_openturns.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_openturns.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c49b9924a12d12ff5a3d4ed61b1919c0/plot_openturns.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_openturns.zip</span></code></a></p>
</div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">numericalderivative</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/user_manual.html">User manual</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../examples/examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_numericalderivative.html">A simple demonstration of the methods</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Applies Stepleman &amp; Winarsky method to an OpenTURNS function</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#chaboche-model">Chaboche model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compute-the-step-of-a-ot-function-using-stepleman-winarsky">Compute the step of a ot.Function using Stepleman &amp; Winarsky</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cantilever-beam-model">Cantilever beam model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fire-satellite-model">Fire satellite model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_use_benchmark.html">Use the benchmark problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_finite_differences.html">Use the finite differences formulas</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_general_fd.html">Use the generalized finite differences formulas</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_general_fd_convergence.html">Convergence of the generalized finite differences formulas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/examples.html#dumontet-vignes">Dumontet &amp; Vignes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/examples.html#gill-murray-saunders-wright">Gill, Murray, Saunders &amp; Wright</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/examples.html#stepleman-winarsky">Stepleman &amp; Winarsky</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/examples.html#shi-xie-xuan-nocedal">Shi, Xie, Xuan &amp; Nocedal</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../examples/examples.html">Examples</a><ul>
      <li>Previous: <a href="plot_numericalderivative.html" title="previous chapter">A simple demonstration of the methods</a></li>
      <li>Next: <a href="plot_use_benchmark.html" title="next chapter">Use the benchmark problems</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;M. Baudin.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/auto_example/plot_openturns.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>