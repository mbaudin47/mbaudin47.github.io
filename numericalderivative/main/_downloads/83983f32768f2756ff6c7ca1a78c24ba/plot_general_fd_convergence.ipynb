{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Convergence of the generalized finite differences formulas\n\nThis example shows the convergence properties of the generalized finite\ndifference (F.D.) formulas.\n\nThe coefficients of the generalized finite difference formula are \ncomputed so that they approximate the derivative $f^{(d)}(x)$\nwith order $p$.\nMore precisely, in exact arithmetic, we have:\n\n\\begin{align}f^{(d)}(x) = \\frac{d!}{h^d} \\sum_{i = i_{\\min}}^{i_\\max} c_i f(x + h i)\n        + R(x; h)\\end{align}\n\nwhere $h > 0$ is the step, $x$ is the point where the derivative\nis to be computed, $d$ is the differentiation order and $p$ is the \norder of accuracy of the formula, \nThe remainder of the Taylor expansion is:\n\n\\begin{align}R(x; h) = - \\frac{d!}{(d + p)!} b_{d + p} f^{(d + p)}(\\xi) h^p.\\end{align}\n\nwhere $\\xi \\in (x, x + h)$ and\nthe coefficient $b_{d + p}$ is defined by the equation:\n\n\\begin{align}b_{d + p} = \\sum_{i = i_{\\min}}^{i_\\max} i^{d + p} c_i.\\end{align}\n\nThe goal of this example is to show the actual behaviour of the \nremainder in floating point arithmetic for particular \nvalues of $d$ and $p$.\n\n## References\n- M. Baudin (2023). M\u00e9thodes num\u00e9riques. Dunod.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport numericalderivative as nd\nimport math\nimport pylab as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider the sinus function and we want to compute its\nfirst derivative i.e. $d = 1$.\nWe consider an order $p = 2$ formula.\nSince $d + p = 3$, the properties of the central finite difference formula\ndepends on the third derivative of the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = nd.SinProblem()\nname = problem.get_name()\nfirst_derivative = problem.get_first_derivative()\nthird_derivative = problem.get_third_derivative()\nfunction = problem.get_function()\nx = problem.get_x()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the generalized central finite difference formula using\n:class:`~numericalderivative.GeneralFiniteDifference`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "formula_accuracy = 2\ndifferentiation_order = 1\nformula = nd.GeneralFiniteDifference(\n    function,\n    x,\n    differentiation_order,\n    formula_accuracy,\n    direction=\"central\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the absolute error of approximate derivative i.e.\n$\\left|f^{(d)}(x) - \\widetilde{f}^{(d)}(x)\\right|$\nwhere $f^{(d)}(x)$ is the exact order $d$ derivative\nof the function $f$ and $\\widetilde{f}^{(d)}(x)$\nis the approximation from the finite difference formula.\nMoreover, compute the absolute value of the remainder of the\nTaylor expansion, i.e. $|R(x; h)|$.\nThis requires to compute the constant $b_{d + p}$,\nwhich is performed by :meth:`~numericalderivative.GeneralFiniteDifference.compute_b_constant`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_derivative_exact = first_derivative(x)\nb_constant = formula.compute_b_constant()\nscaled_b_parameter = (\n    b_constant\n    * math.factorial(differentiation_order)\n    / math.factorial(differentiation_order + formula_accuracy)\n)\nnumber_of_steps = 50\nstep_array = np.logspace(-15, 1, number_of_steps)\nabs_error_array = np.zeros((number_of_steps))\nremainder_array = np.zeros((number_of_steps))\nfor i in range(number_of_steps):\n    step = step_array[i]\n    derivative_approx = formula.compute(step)\n    abs_error_array[i] = abs(derivative_approx - first_derivative_exact)\n    remainder_array[i] = (\n        abs(scaled_b_parameter * third_derivative(x)) * step**formula_accuracy\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure()\npl.title(\n    f\"Derivative of {name} at x={x}, \"\n    f\"d={differentiation_order}, p={formula_accuracy}\"\n)\npl.plot(step_array, abs_error_array, \"o--\", label=\"Abs. error\")\npl.plot(step_array, remainder_array, \"^:\", label=\"Abs. remainder\")\npl.xlabel(\"Step h\")\npl.ylabel(\"Absolute error\")\npl.xscale(\"log\")\npl.yscale(\"log\")\n_ = pl.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a good agreement between the model and the\nactual error when the step size is sufficiently large.\nWhen the step is close to zero however, the rounding errors increase the\nabsolute error and the model does not fit anymore.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}