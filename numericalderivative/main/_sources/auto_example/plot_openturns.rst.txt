
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_openturns.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_example_plot_openturns.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_openturns.py:


Applies Stepleman & Winarsky method to an OpenTURNS function
============================================================

.. GENERATED FROM PYTHON SOURCE LINES 10-16

.. code-block:: Python

    import openturns as ot
    import numericalderivative as nd
    from openturns.usecases import chaboche_model
    from openturns.usecases import cantilever_beam
    from openturns.usecases import fireSatellite_function








.. GENERATED FROM PYTHON SOURCE LINES 17-21

Chaboche model
--------------

Load the Chaboche model

.. GENERATED FROM PYTHON SOURCE LINES 21-27

.. code-block:: Python

    cm = chaboche_model.ChabocheModel()
    model = ot.Function(cm.model)
    inputMean = cm.inputDistribution.getMean()
    print(f"inputMean = {inputMean}")
    meanStrain, meanR, meanC, meanGamma = inputMean





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputMean = [0.035,7.5e+08,2.75e+09,10]




.. GENERATED FROM PYTHON SOURCE LINES 28-29

Print the derivative from OpenTURNS

.. GENERATED FROM PYTHON SOURCE LINES 29-33

.. code-block:: Python

    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    derivative





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivative = 


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <p>[[  1.93789e+09 ]<br>
     [  1           ]<br>
     [  0.0297619   ]<br>
     [ -1.33845e+06 ]]</p>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 34-43

Here is the derivative with default step size in OpenTURNS :

.. code-block::

   derivative =
   [[  1.93789e+09 ]
   [  1           ]
   [  0.0297619   ]
   [ -1.33845e+06 ]]

.. GENERATED FROM PYTHON SOURCE LINES 45-46

Print the gradient function

.. GENERATED FROM PYTHON SOURCE LINES 46-50

.. code-block:: Python

    gradient = model.getGradient()
    gradient







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <p>No analytical gradient available. Try using finite difference instead.</p>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 51-53

Derivative with respect to strain
Define a function which takes only strain as input and returns a float

.. GENERATED FROM PYTHON SOURCE LINES 53-59

.. code-block:: Python

    def functionStrain(strain, r, c, gamma):
        x = [strain, r, c, gamma]
        sigma = model(x)
        return sigma[0]









.. GENERATED FROM PYTHON SOURCE LINES 60-61

Algorithm to detect h* for Strain

.. GENERATED FROM PYTHON SOURCE LINES 61-73

.. code-block:: Python

    h0 = 1.0e0
    args = [meanR, meanC, meanGamma]
    algorithm = nd.SteplemanWinarsky(functionStrain, meanStrain, args=args, verbose=True)
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print(f"Derivative wrt strain= {f_prime_approx:.17e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=2.1296e+12
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=2.6233e+09
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=1.2076e+08
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=7.4021e+06
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=4.6207e+05
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=2.8877e+04
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=1.8048e+03
      number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=1.1280e+02
      number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=7.0430e+00
      number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=4.5312e-01
      number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    Optimum h = 9.5367431640625e-07
    iterations = 10
    Function evaluations = 24
    Derivative wrt strain= 1.93789224675000000e+09




.. GENERATED FROM PYTHON SOURCE LINES 74-76

Derivative with respect to R
Define a function which takes only R as input and returns a float

.. GENERATED FROM PYTHON SOURCE LINES 76-82

.. code-block:: Python

    def functionR(r, strain, c, gamma):
        x = [strain, r, c, gamma]
        sigma = model(x)
        return sigma[0]









.. GENERATED FROM PYTHON SOURCE LINES 83-84

Algorithm to detect h* for R

.. GENERATED FROM PYTHON SOURCE LINES 84-98

.. code-block:: Python

    h0 = 1.0e9
    args = [meanStrain, meanC, meanGamma]
    algorithm = nd.SteplemanWinarsky(
        functionR, meanR, args=args, relative_precision=1.0e-14, verbose=True
    )
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print(f"Optimum h = {h_optimal:e}")
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print(f"Derivative wrt R= {f_prime_approx:.17e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+09
      number_of_iterations=0, h=2.5000e+08, |FD(h_current) - FD(h_previous)|=2.2204e-16
      number_of_iterations=1, h=6.2500e+07, |FD(h_current) - FD(h_previous)|=2.2204e-16
      number_of_iterations=2, h=1.5625e+07, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    Optimum h = 6.250000e+07
    iterations = 2
    Function evaluations = 8
    Derivative wrt R= 1.00000000000000000e+00




.. GENERATED FROM PYTHON SOURCE LINES 99-101

Derivative with respect to C
Define a function which takes only C as input and returns a float

.. GENERATED FROM PYTHON SOURCE LINES 101-107

.. code-block:: Python

    def functionR(c, strain, r, gamma):
        x = [strain, r, c, gamma]
        sigma = model(x)
        return sigma[0]









.. GENERATED FROM PYTHON SOURCE LINES 108-109

Algorithm to detect h* for C

.. GENERATED FROM PYTHON SOURCE LINES 109-123

.. code-block:: Python

    h0 = 1.0e15
    args = [meanStrain, meanR, meanGamma]
    algorithm = nd.SteplemanWinarsky(
        functionR, meanC, args=args, relative_precision=1.0e-14, verbose=True
    )
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print(f"Optimum h = {h_optimal:e}")
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print(f"Derivative wrt C= {f_prime_approx:.17e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+15
      number_of_iterations=0, h=2.5000e+14, |FD(h_current) - FD(h_previous)|=3.4694e-18
      number_of_iterations=1, h=6.2500e+13, |FD(h_current) - FD(h_previous)|=3.4694e-18
      number_of_iterations=2, h=1.5625e+13, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    Optimum h = 6.250000e+13
    iterations = 2
    Function evaluations = 8
    Derivative wrt C= 2.95311910281286574e-02




.. GENERATED FROM PYTHON SOURCE LINES 124-126

Derivative with respect to Gamma
Define a function which takes only C as input and returns a float

.. GENERATED FROM PYTHON SOURCE LINES 126-132

.. code-block:: Python

    def functionGamma(gamma, strain, r, c):
        x = [strain, r, c, gamma]
        sigma = model(x)
        return sigma[0]









.. GENERATED FROM PYTHON SOURCE LINES 133-134

Algorithm to detect h* for Gamma

.. GENERATED FROM PYTHON SOURCE LINES 134-148

.. code-block:: Python

    h0 = 1.0e0
    args = [meanStrain, meanR, meanC]
    algorithm = nd.SteplemanWinarsky(
        functionGamma, meanGamma, args=args, relative_precision=1.0e-14, verbose=True
    )
    h_optimal, iterations = algorithm.compute_step(h0)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print(f"Optimum h = {h_optimal:e}")
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    print(f"Derivative wrt Gamma= {f_prime_approx:.17e}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=1.2204e+02
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=7.6272e+00
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=4.7670e-01
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=2.9785e-02
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.8768e-03
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=1.2207e-04
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=2.4414e-04
      Stop because no monotony anymore.
    Optimum h = 2.441406e-04
    iterations = 6
    Function evaluations = 16
    Derivative wrt Gamma= -1.33845466918945312e+06




.. GENERATED FROM PYTHON SOURCE LINES 149-150

Derivative with respect to [strain, r, c, gamma]

.. GENERATED FROM PYTHON SOURCE LINES 150-162

.. code-block:: Python

    def genericFunction(x, xIndex, referenceInput):
        inputDimension = referenceInput.getDimension()
        complementIndices = [i for i in range(inputDimension) if i != xIndex]
        modelInput = ot.Point(inputDimension)
        modelInput[xIndex] = x
        for i in complementIndices:
            modelInput[i] = referenceInput[i]
        y = model(modelInput)
        modelOutput = y[0]
        return modelOutput









.. GENERATED FROM PYTHON SOURCE LINES 163-164

Default step size for all components

.. GENERATED FROM PYTHON SOURCE LINES 164-193

.. code-block:: Python

    initialStep = ot.Point([1.0e0, 1.0e8, 1.0e8, 1.0e0])
    inputDimension = model.getInputDimension()
    referenceInput = cm.inputDistribution.getMean()
    inputDescription = cm.inputDistribution.getDescription()
    optimalStep = ot.Point(inputDimension)
    for xIndex in range(inputDimension):
        inputMarginal = referenceInput[xIndex]
        print(
            f"+ Derivative with respect to {inputDescription[xIndex]} "
            f"at point {inputMarginal}"
        )
        args = [xIndex, referenceInput]
        algorithm = nd.SteplemanWinarsky(
            genericFunction,
            inputMarginal,
            args=args,
            relative_precision=1.0e-12,
            verbose=True,
        )
        h_optimal, iterations = algorithm.compute_step(initialStep[xIndex])
        number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
        print(f"    Optimum h = {h_optimal:e}")
        print("    Iterations =", iterations)
        print("    Function evaluations =", number_of_function_evaluations)
        f_prime_approx = algorithm.compute_first_derivative(h_optimal)
        print(f"    Derivative wrt {inputDescription[xIndex]}= {f_prime_approx:.17e}")
        # Store optimal point
        optimalStep[xIndex] = h_optimal





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + Derivative with respect to Strain at point 0.035
    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=2.1296e+12
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=2.6233e+09
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=1.2076e+08
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=7.4021e+06
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=4.6207e+05
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=2.8877e+04
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=1.8048e+03
      number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=1.1280e+02
      number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=7.0430e+00
      number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=4.5312e-01
      number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
        Optimum h = 9.536743e-07
        Iterations = 10
        Function evaluations = 24
        Derivative wrt Strain= 1.93789224675000000e+09
    + Derivative with respect to R at point 750000000.0000002
    + compute_step()
    initial_step=1.000e+08
      number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
        Optimum h = 1.000000e+08
        Iterations = 0
        Function evaluations = 4
        Derivative wrt R= 1.00000000000000000e+00
    + Derivative with respect to C at point 2750000000.0
    + compute_step()
    initial_step=1.000e+08
      number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=1.1935e-15
      number_of_iterations=1, h=6.2500e+06, |FD(h_current) - FD(h_previous)|=2.3870e-15
      Stop because no monotony anymore.
        Optimum h = 2.500000e+07
        Iterations = 1
        Function evaluations = 6
        Derivative wrt C= 2.95311910281300556e-02
    + Derivative with respect to Gamma at point 10.0
    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=1.2204e+02
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=7.6272e+00
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=4.7670e-01
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=2.9785e-02
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.8768e-03
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=1.2207e-04
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=2.4414e-04
      Stop because no monotony anymore.
        Optimum h = 2.441406e-04
        Iterations = 6
        Function evaluations = 16
        Derivative wrt Gamma= -1.33845466918945312e+06




.. GENERATED FROM PYTHON SOURCE LINES 194-197

.. code-block:: Python

    print("The optimal step for central finite difference is")
    print(f"optimalStep = {optimalStep}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The optimal step for central finite difference is
    optimalStep = [9.53674e-07,1e+08,2.5e+07,0.000244141]




.. GENERATED FROM PYTHON SOURCE LINES 198-200

Configure the model with the optimal step computed
from SteplemanWinarsky

.. GENERATED FROM PYTHON SOURCE LINES 200-207

.. code-block:: Python

    gradStep = ot.ConstantStep(optimalStep)  # Constant gradient step
    model.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))
    # Now the gradient uses the optimal step sizes
    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    print(derivative)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivative = 
    [[  1.93789e+09 ]
     [  1           ]
     [  0.0295312   ]
     [ -1.33845e+06 ]]




.. GENERATED FROM PYTHON SOURCE LINES 208-217

Derivative with step size computed from SteplemanWinarsky :

.. code-block::

    derivative =
    [[  1.93789e+09 ]
     [  1           ]
     [  0.0295312   ]  # <- This is a change in the 3d decimal
     [ -1.33845e+06 ]]

.. GENERATED FROM PYTHON SOURCE LINES 219-224

Compute the step of a ot.Function using Stepleman & Winarsky
------------------------------------------------------------

The function below computes the step of a finite difference formula
applied to an OpenTURNS function using Stepleman & Winarsky's method.

.. GENERATED FROM PYTHON SOURCE LINES 227-315

.. code-block:: Python

    def computeSteplemanWinarskyStep(
        model,
        initial_step,
        referenceInput,
        relative_precision=1.0e-16,
        beta=4.0,
        verbose=False,
    ):
        """
        Uses SteplemanWinarsky to compute a step size for central finite differences

        The central F.D. is:

        f'(x) ~ (f(x + h) - f(x - h)) / (2 * h)

        Parameters
        ----------
        model : ot.Function(inputDimension, 1)
            The model, which output dimension equal to 1.
        initial_step : ot.Point(inputDimension)
            The initial step size.
        referenceInput : ot.Point(inputDimension)
            The point X where the derivative is to be computed.
        relative_precision : float, > 0, optional
            The absolute relative_precision of evaluation of f. The default is 1.0e-16.
        verbose : bool, optional
            Set to True to print intermediate messages. The default is False.

        Returns
        -------
        optimalStep : ot.Point(inputDimension)
            The optimal step for central finite difference.
        """

        def genericFunction(x, xIndex, referenceInput):
            if verbose:
                print("x = ", x)
            inputDimension = referenceInput.getDimension()
            complementIndices = [i for i in range(inputDimension) if i != xIndex]
            modelInput = ot.Point(inputDimension)
            modelInput[xIndex] = x
            for i in complementIndices:
                modelInput[i] = referenceInput[i]
            y = model(modelInput)
            modelOutput = y[0]
            if verbose:
                print("y = ", y)
            return modelOutput

        inputDimension = model.getInputDimension()
        inputDescription = model.getInputDescription()
        optimalStep = ot.Point(inputDimension)
        if verbose:
            print(f"Input dimension = {inputDimension}")
            print(f"Input description = {inputDescription}")
        for xIndex in range(inputDimension):
            inputMarginal = referenceInput[xIndex]
            if verbose:
                print(
                    f"+ Derivative with respect to {inputDescription[xIndex]} "
                    f"at point {inputMarginal}"
                )
            args = [xIndex, referenceInput]
            algorithm = nd.SteplemanWinarsky(
                genericFunction,
                inputMarginal,
                args=args,
                relative_precision=relative_precision,
                verbose=verbose,
            )
            h_optimal, iterations = algorithm.compute_step(
                initial_step[xIndex],
                beta=beta,
            )
            number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
            f_prime_approx = algorithm.compute_first_derivative(h_optimal)
            if verbose:
                print(f"    Optimum h = {h_optimal:e}")
                print("    Iterations =", iterations)
                print("    Function evaluations =", number_of_function_evaluations)
                print(
                    f"    Derivative wrt {inputDescription[xIndex]} = {f_prime_approx:.17e}"
                )
            # Store optimal point
            optimalStep[xIndex] = h_optimal
        return optimalStep









.. GENERATED FROM PYTHON SOURCE LINES 316-321

Cantilever beam model
---------------------

Apply the same method to the cantilever beam model
Load the cantilever beam model

.. GENERATED FROM PYTHON SOURCE LINES 321-326

.. code-block:: Python

    cb = cantilever_beam.CantileverBeam()
    model = ot.Function(cb.model)
    inputMean = cb.distribution.getMean()
    print(f"inputMean = {inputMean}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputMean = [6.70455e+10,300,2.55,1.45385e-07]




.. GENERATED FROM PYTHON SOURCE LINES 327-328

Print the derivative from OpenTURNS

.. GENERATED FROM PYTHON SOURCE LINES 328-332

.. code-block:: Python

    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    print(f"{derivative}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivative = 
    [[ -2.53725e-12 ]
     [  0.000567037 ]
     [  0.200131    ]
     [ -1.17008e+06 ]]




.. GENERATED FROM PYTHON SOURCE LINES 333-342

Derivative with OpenTURNS's default step size :

.. code-block::

   derivative =
   [[ -2.53725e-12 ]
    [  0.000567037 ]
    [  0.200131    ]
    [ -1.17008e+06 ]]

.. GENERATED FROM PYTHON SOURCE LINES 344-347

Notice that the CantileverBeam model has an exact gradient in OpenTURNS,
because it is symbolic.
Hence, using the optimal step should not make any difference.

.. GENERATED FROM PYTHON SOURCE LINES 349-350

Compute step from SteplemanWinarsky

.. GENERATED FROM PYTHON SOURCE LINES 350-362

.. code-block:: Python

    initialStep = ot.Point(inputMean) / 2
    optimalStep = computeSteplemanWinarskyStep(model, initialStep, inputMean)
    print("The optimal step for central finite difference is")
    print(f"optimalStep = {optimalStep}")

    gradStep = ot.ConstantStep(optimalStep)  # Constant gradient step
    model.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))
    # Now the gradient uses the optimal step sizes
    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    print(f"{derivative}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The optimal step for central finite difference is
    optimalStep = [127879,37.5,4.86374e-06,2.77299e-13]
    derivative = 
    [[ -2.53725e-12 ]
     [  0.000567037 ]
     [  0.200131    ]
     [ -1.17008e+06 ]]




.. GENERATED FROM PYTHON SOURCE LINES 363-374

Derivative with SteplemanWinarskyStep

.. code-block::

    derivative =
    [[ -2.53725e-12 ]
     [  0.000567037 ]
     [  0.200131    ]
     [ -1.17008e+06 ]]

We see that this is the correct step size.

.. GENERATED FROM PYTHON SOURCE LINES 377-382

Fire satellite model
--------------------

Load the Fire satellite use case with total torque as output
Print the derivative from OpenTURNS

.. GENERATED FROM PYTHON SOURCE LINES 382-393

.. code-block:: Python

    m = fireSatellite_function.FireSatelliteModel()
    inputMean = m.inputDistribution.getMean()
    m.modelTotalTorque.setInputDescription(
        ["H", "Pother", "Fs", "theta", "Lsp", "q", "RD", "Lalpha", "Cd"]
    )
    m.modelTotalTorque.setOutputDescription(["Total torque"])
    model = ot.Function(m.modelTotalTorque)
    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    print(f"{derivative}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivative = 
    9x1
    [[ -4.78066e-10 ]
     [ -3.46945e-13 ]
     [  2.30666e-09 ]
     [  4.90736e-09 ]
     [  1.61453e-06 ]
     [  2.15271e-06 ]
     [  5.17815e-10 ]
     [  0.00582819  ]
     [  0.0116564   ]]




.. GENERATED FROM PYTHON SOURCE LINES 394-409

From OpenTURNS with default settings:

.. code-block::

    derivative =
    9x1
    [[ -4.78066e-10 ]
     [ -3.46945e-13 ]
     [  2.30666e-09 ]
     [  4.90736e-09 ]
     [  1.61453e-06 ]
     [  2.15271e-06 ]
     [  5.17815e-10 ]
     [  0.00582819  ]
     [  0.0116564   ]]

.. GENERATED FROM PYTHON SOURCE LINES 411-424

There is a specific difficulty with FireSatellite model for the derivative
with respect to Pother.
Indeed, the gradient of the TotalTorque with respect to Pother is close
to zero.
Furthermore, the nominal value (mean) of Pother is 1000.
Therefore, in order to get a sufficiently large number of lost digits,
the algorithm is forced to use a very large step h, close to 10^4.
But this leads to a negative value of Pother - h, which produces
a math domain error.
In other words, the model has an input range which is ignored by the
algorithm.
To solve this issue the interval which defines the set of
possible values of x should be introduced.

.. GENERATED FROM PYTHON SOURCE LINES 426-427

Compute step from SteplemanWinarsky

.. GENERATED FROM PYTHON SOURCE LINES 427-438

.. code-block:: Python

    initialStep = ot.Point(inputMean) / 2
    optimalStep = computeSteplemanWinarskyStep(
        model,
        initialStep,
        inputMean,
        verbose=True,
        relative_precision=1.0e-16,
    )
    print("The optimal step for central finite difference is")
    print(f"optimalStep = {optimalStep}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Input dimension = 9
    Input description = [H,Pother,Fs,theta,Lsp,q,RD,Lalpha,Cd]
    + Derivative with respect to H at point 18000000.0
    + compute_step()
    initial_step=9.000e+06
    x =  27000000.0
    y =  [0.00851681]
    x =  9000000.0
    y =  [0.0184821]
    x =  20250000.0
    y =  [0.0106748]
    x =  15750000.0
    y =  [0.0128449]
      number_of_iterations=0, h=2.2500e+06, |FD(h_current) - FD(h_previous)|=7.1361e-11
    x =  18562500.0
    y =  [0.0113968]
    x =  17437500.0
    y =  [0.011935]
      number_of_iterations=1, h=5.6250e+05, |FD(h_current) - FD(h_previous)|=3.8542e-12
    x =  18140625.0
    y =  [0.0115928]
    x =  17859375.0
    y =  [0.0117273]
      number_of_iterations=2, h=1.4062e+05, |FD(h_current) - FD(h_previous)|=2.3884e-13
    x =  18035156.25
    y =  [0.0116429]
    x =  17964843.75
    y =  [0.0116765]
      number_of_iterations=3, h=3.5156e+04, |FD(h_current) - FD(h_previous)|=1.4920e-14
    x =  18008789.0625
    y =  [0.0116555]
    x =  17991210.9375
    y =  [0.0116639]
      number_of_iterations=4, h=8.7891e+03, |FD(h_current) - FD(h_previous)|=9.3245e-16
    x =  18002197.265625
    y =  [0.0116586]
    x =  17997802.734375
    y =  [0.0116607]
      number_of_iterations=5, h=2.1973e+03, |FD(h_current) - FD(h_previous)|=5.8278e-17
    x =  18000549.31640625
    y =  [0.0116594]
    x =  17999450.68359375
    y =  [0.0116599]
      number_of_iterations=6, h=5.4932e+02, |FD(h_current) - FD(h_previous)|=3.6411e-18
    x =  18000137.329101562
    y =  [0.0116596]
    x =  17999862.670898438
    y =  [0.0116597]
      number_of_iterations=7, h=1.3733e+02, |FD(h_current) - FD(h_previous)|=2.2737e-19
    x =  18000034.33227539
    y =  [0.0116597]
    x =  17999965.66772461
    y =  [0.0116597]
      number_of_iterations=8, h=3.4332e+01, |FD(h_current) - FD(h_previous)|=1.2632e-20
    x =  18000008.583068848
    y =  [0.0116597]
    x =  17999991.416931152
    y =  [0.0116597]
      number_of_iterations=9, h=8.5831e+00, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    x =  18000034.33227539
    y =  [0.0116597]
    x =  17999965.66772461
    y =  [0.0116597]
        Optimum h = 3.433228e+01
        Iterations = 9
        Function evaluations = 22
        Derivative wrt H = -4.78156698894357113e-10
    + Derivative with respect to Pother at point 1000.0
    + compute_step()
    initial_step=5.000e+02
    x =  1500.0
    y =  [0.0116597]
    x =  500.0
    y =  [0.0116597]
    x =  1125.0
    y =  [0.0116597]
    x =  875.0
    y =  [0.0116597]
      number_of_iterations=0, h=1.2500e+02, |FD(h_current) - FD(h_previous)|=1.0877e-16
    x =  1031.25
    y =  [0.0116597]
    x =  968.75
    y =  [0.0116597]
      number_of_iterations=1, h=3.1250e+01, |FD(h_current) - FD(h_previous)|=6.8140e-18
    x =  1007.8125
    y =  [0.0116597]
    x =  992.1875
    y =  [0.0116597]
      number_of_iterations=2, h=7.8125e+00, |FD(h_current) - FD(h_previous)|=3.3307e-19
    x =  1001.953125
    y =  [0.0116597]
    x =  998.046875
    y =  [0.0116597]
      number_of_iterations=3, h=1.9531e+00, |FD(h_current) - FD(h_previous)|=1.1102e-19
    x =  1000.48828125
    y =  [0.0116597]
    x =  999.51171875
    y =  [0.0116597]
      number_of_iterations=4, h=4.8828e-01, |FD(h_current) - FD(h_previous)|=8.8818e-19
      Stop because no monotony anymore.
    x =  1001.953125
    y =  [0.0116597]
    x =  998.046875
    y =  [0.0116597]
        Optimum h = 1.953125e+00
        Iterations = 4
        Function evaluations = 12
        Derivative wrt Pother = -2.91776380834107862e-13
    + Derivative with respect to Fs at point 1400.0
    + compute_step()
    initial_step=7.000e+02
    x =  2100.0
    y =  [0.0116617]
    x =  700.0
    y =  [0.0116585]
    x =  1575.0
    y =  [0.0116601]
    x =  1225.0
    y =  [0.0116593]
      number_of_iterations=0, h=1.7500e+02, |FD(h_current) - FD(h_previous)|=9.9805e-14
    x =  1443.75
    y =  [0.0116598]
    x =  1356.25
    y =  [0.0116596]
      number_of_iterations=1, h=4.3750e+01, |FD(h_current) - FD(h_previous)|=2.3762e-15
    x =  1410.9375
    y =  [0.0116597]
    x =  1389.0625
    y =  [0.0116597]
      number_of_iterations=2, h=1.0938e+01, |FD(h_current) - FD(h_previous)|=1.3822e-16
    x =  1402.734375
    y =  [0.0116597]
    x =  1397.265625
    y =  [0.0116597]
      number_of_iterations=3, h=2.7344e+00, |FD(h_current) - FD(h_previous)|=8.5646e-18
    x =  1400.68359375
    y =  [0.0116597]
    x =  1399.31640625
    y =  [0.0116597]
      number_of_iterations=4, h=6.8359e-01, |FD(h_current) - FD(h_previous)|=6.3441e-19
    x =  1400.1708984375
    y =  [0.0116597]
    x =  1399.8291015625
    y =  [0.0116597]
      number_of_iterations=5, h=1.7090e-01, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    x =  1400.68359375
    y =  [0.0116597]
    x =  1399.31640625
    y =  [0.0116597]
        Optimum h = 6.835938e-01
        Iterations = 5
        Function evaluations = 14
        Derivative wrt Fs = 2.30671214446991674e-09
    + Derivative with respect to theta at point 15.0
    + compute_step()
    initial_step=7.500e+00
    x =  22.5
    y =  [0.0116597]
    x =  7.5
    y =  [0.0116597]
    x =  16.875
    y =  [0.0116597]
    x =  13.125
    y =  [0.0116597]
      number_of_iterations=0, h=1.8750e+00, |FD(h_current) - FD(h_previous)|=2.0718e-10
    x =  15.46875
    y =  [0.0116597]
    x =  14.53125
    y =  [0.0116597]
      number_of_iterations=1, h=4.6875e-01, |FD(h_current) - FD(h_previous)|=1.3127e-11
    x =  15.1171875
    y =  [0.0116597]
    x =  14.8828125
    y =  [0.0116597]
      number_of_iterations=2, h=1.1719e-01, |FD(h_current) - FD(h_previous)|=8.2113e-13
    x =  15.029296875
    y =  [0.0116597]
    x =  14.970703125
    y =  [0.0116597]
      number_of_iterations=3, h=2.9297e-02, |FD(h_current) - FD(h_previous)|=5.1300e-14
    x =  15.00732421875
    y =  [0.0116597]
    x =  14.99267578125
    y =  [0.0116597]
      number_of_iterations=4, h=7.3242e-03, |FD(h_current) - FD(h_previous)|=3.1086e-15
    x =  15.0018310546875
    y =  [0.0116597]
    x =  14.9981689453125
    y =  [0.0116597]
      number_of_iterations=5, h=1.8311e-03, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    x =  15.00732421875
    y =  [0.0116597]
    x =  14.99267578125
    y =  [0.0116597]
        Optimum h = 7.324219e-03
        Iterations = 5
        Function evaluations = 14
        Derivative wrt theta = 4.90744866965542315e-09
    + Derivative with respect to Lsp at point 2.0
    + compute_step()
    initial_step=1.000e+00
    x =  3.0
    y =  [0.0116617]
    x =  1.0
    y =  [0.0116585]
    x =  2.25
    y =  [0.0116601]
    x =  1.75
    y =  [0.0116593]
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=5.2381e-11
    x =  2.0625
    y =  [0.0116598]
    x =  1.9375
    y =  [0.0116596]
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=3.2740e-12
    x =  2.015625
    y =  [0.0116597]
    x =  1.984375
    y =  [0.0116597]
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=2.0461e-13
    x =  2.00390625
    y =  [0.0116597]
    x =  1.99609375
    y =  [0.0116597]
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=1.2546e-14
    x =  2.0009765625
    y =  [0.0116597]
    x =  1.9990234375
    y =  [0.0116597]
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.1102e-15
    x =  2.000244140625
    y =  [0.0116597]
    x =  1.999755859375
    y =  [0.0116597]
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=8.8818e-16
    x =  2.00006103515625
    y =  [0.0116597]
    x =  1.99993896484375
    y =  [0.0116597]
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=7.1054e-15
      Stop because no monotony anymore.
    x =  2.000244140625
    y =  [0.0116597]
    x =  1.999755859375
    y =  [0.0116597]
        Optimum h = 2.441406e-04
        Iterations = 6
        Function evaluations = 16
        Derivative wrt Lsp = 1.61453365166153162e-06
    + Derivative with respect to q at point 0.5
    + compute_step()
    initial_step=2.500e-01
    x =  0.75
    y =  [0.0116603]
    x =  0.25
    y =  [0.0116592]
    x =  0.5625
    y =  [0.0116598]
    x =  0.4375
    y =  [0.0116596]
      number_of_iterations=0, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=7.7605e-12
    x =  0.515625
    y =  [0.0116597]
    x =  0.484375
    y =  [0.0116596]
      number_of_iterations=1, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=4.8504e-13
    x =  0.50390625
    y =  [0.0116597]
    x =  0.49609375
    y =  [0.0116597]
      number_of_iterations=2, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=3.0420e-14
    x =  0.5009765625
    y =  [0.0116597]
    x =  0.4990234375
    y =  [0.0116597]
      number_of_iterations=3, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=2.2204e-15
    x =  0.500244140625
    y =  [0.0116597]
    x =  0.499755859375
    y =  [0.0116597]
      number_of_iterations=4, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=8.8818e-16
    x =  0.50006103515625
    y =  [0.0116597]
    x =  0.49993896484375
    y =  [0.0116597]
      number_of_iterations=5, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=7.1054e-15
      Stop because no monotony anymore.
    x =  0.500244140625
    y =  [0.0116597]
    x =  0.499755859375
    y =  [0.0116597]
        Optimum h = 2.441406e-04
        Iterations = 5
        Function evaluations = 14
        Derivative wrt q = 2.15271153791718461e-06
    + Derivative with respect to RD at point 5.0
    + compute_step()
    initial_step=2.500e+00
    x =  7.5
    y =  [0.0116597]
    x =  2.5
    y =  [0.0116597]
    x =  5.625
    y =  [0.0116597]
    x =  4.375
    y =  [0.0116597]
      number_of_iterations=0, h=6.2500e-01, |FD(h_current) - FD(h_previous)|=1.3878e-17
    x =  5.15625
    y =  [0.0116597]
    x =  4.84375
    y =  [0.0116597]
      number_of_iterations=1, h=1.5625e-01, |FD(h_current) - FD(h_previous)|=4.1633e-18
    x =  5.0390625
    y =  [0.0116597]
    x =  4.9609375
    y =  [0.0116597]
      number_of_iterations=2, h=3.9062e-02, |FD(h_current) - FD(h_previous)|=1.1102e-17
      Stop because no monotony anymore.
    x =  5.15625
    y =  [0.0116597]
    x =  4.84375
    y =  [0.0116597]
        Optimum h = 1.562500e-01
        Iterations = 2
        Function evaluations = 8
        Derivative wrt RD = 5.17804699118329340e-10
    + Derivative with respect to Lalpha at point 2.0
    + compute_step()
    initial_step=1.000e+00
    x =  3.0
    y =  [0.0174881]
    x =  1.0
    y =  [0.00583233]
    x =  2.25
    y =  [0.0131168]
    x =  1.75
    y =  [0.0102027]
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=2.6270e-07
    x =  2.0625
    y =  [0.0120239]
    x =  1.9375
    y =  [0.0112954]
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=1.2330e-08
    x =  2.015625
    y =  [0.0117507]
    x =  1.984375
    y =  [0.0115686]
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=7.5861e-10
    x =  2.00390625
    y =  [0.0116824]
    x =  1.99609375
    y =  [0.0116369]
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=4.7367e-11
    x =  2.0009765625
    y =  [0.0116654]
    x =  1.9990234375
    y =  [0.011654]
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=2.9607e-12
    x =  2.000244140625
    y =  [0.0116611]
    x =  1.999755859375
    y =  [0.0116583]
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=1.8918e-13
    x =  2.00006103515625
    y =  [0.01166]
    x =  1.99993896484375
    y =  [0.0116593]
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    x =  2.000244140625
    y =  [0.0116611]
    x =  1.999755859375
    y =  [0.0116583]
        Optimum h = 2.441406e-04
        Iterations = 6
        Function evaluations = 16
        Derivative wrt Lalpha = 5.82818510592630901e-03
    + Derivative with respect to Cd at point 1.0
    + compute_step()
    initial_step=5.000e-01
    x =  1.5
    y =  [0.0174881]
    x =  0.5
    y =  [0.00583233]
    x =  1.125
    y =  [0.0131168]
    x =  0.875
    y =  [0.0102027]
      number_of_iterations=0, h=1.2500e-01, |FD(h_current) - FD(h_previous)|=5.2539e-07
    x =  1.03125
    y =  [0.0120239]
    x =  0.96875
    y =  [0.0112954]
      number_of_iterations=1, h=3.1250e-02, |FD(h_current) - FD(h_previous)|=2.4659e-08
    x =  1.0078125
    y =  [0.0117507]
    x =  0.9921875
    y =  [0.0115686]
      number_of_iterations=2, h=7.8125e-03, |FD(h_current) - FD(h_previous)|=1.5172e-09
    x =  1.001953125
    y =  [0.0116824]
    x =  0.998046875
    y =  [0.0116369]
      number_of_iterations=3, h=1.9531e-03, |FD(h_current) - FD(h_previous)|=9.4733e-11
    x =  1.00048828125
    y =  [0.0116654]
    x =  0.99951171875
    y =  [0.011654]
      number_of_iterations=4, h=4.8828e-04, |FD(h_current) - FD(h_previous)|=5.9215e-12
    x =  1.0001220703125
    y =  [0.0116611]
    x =  0.9998779296875
    y =  [0.0116583]
      number_of_iterations=5, h=1.2207e-04, |FD(h_current) - FD(h_previous)|=3.7836e-13
    x =  1.000030517578125
    y =  [0.01166]
    x =  0.999969482421875
    y =  [0.0116593]
      number_of_iterations=6, h=3.0518e-05, |FD(h_current) - FD(h_previous)|=0.0000e+00
      Stop because zero difference.
    x =  1.0001220703125
    y =  [0.0116611]
    x =  0.9998779296875
    y =  [0.0116583]
        Optimum h = 1.220703e-04
        Iterations = 6
        Function evaluations = 16
        Derivative wrt Cd = 1.16563702118526180e-02
    The optimal step for central finite difference is
    optimalStep = [34.3323,1.95312,0.683594,0.00732422,0.000244141,0.000244141,0.15625,0.000244141,0.00012207]




.. GENERATED FROM PYTHON SOURCE LINES 439-446

.. code-block:: Python

    gradStep = ot.ConstantStep(optimalStep)  # Constant gradient step
    model.setGradient(ot.CenteredFiniteDifferenceGradient(gradStep, model.getEvaluation()))
    # Now the gradient uses the optimal step sizes
    derivative = model.gradient(inputMean)
    print(f"derivative = ")
    print(f"{derivative}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivative = 
    9x1
    [[ -4.78157e-10 ]
     [ -2.91776e-13 ]
     [  2.30671e-09 ]
     [  4.90745e-09 ]
     [  1.61453e-06 ]
     [  2.15271e-06 ]
     [  5.17805e-10 ]
     [  0.00582819  ]
     [  0.0116564   ]]




.. GENERATED FROM PYTHON SOURCE LINES 447-462

From SteplemanWinarsky

.. code-block::

   derivative =
   9x1
   [[ -4.78157e-10 ]
    [ -2.91776e-13 ]  # <- This is a minor change
    [  2.30671e-09 ]
    [  4.90745e-09 ]
    [  1.61453e-06 ]
    [  2.15271e-06 ]
    [  5.17805e-10 ]
    [  0.00582819  ]
    [  0.0116564   ]]


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.233 seconds)


.. _sphx_glr_download_auto_example_plot_openturns.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_openturns.ipynb <plot_openturns.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_openturns.py <plot_openturns.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_openturns.zip <plot_openturns.zip>`
