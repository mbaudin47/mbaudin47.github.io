
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_stepleman_winarsky.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_example_plot_stepleman_winarsky.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_stepleman_winarsky.py:


Experiment with Stepleman & Winarsky method
===========================================

Find a step which is near to optimal for a central finite difference 
formula.

References
----------
- Adaptive numerical differentiation
  R. S. Stepleman and N. D. Winarsky
  Journal: Math. Comp. 33 (1979), 1257-1264 

.. GENERATED FROM PYTHON SOURCE LINES 18-23

.. code-block:: Python

    import numpy as np
    import pylab as pl
    import numericalderivative as nd
    from matplotlib.ticker import MaxNLocator








.. GENERATED FROM PYTHON SOURCE LINES 24-26

Use the method on a simple problem
----------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 28-36

In the next example, we use the algorithm on the exponential function.
We create the :class:`~numericalderivative.SteplemanWinarsky` algorithm using the function and the point x.
Then we use the :meth:`~numericalderivative.SteplemanWinarsky.compute_step()` method to compute the step,
using an upper bound of the step as an initial point of the algorithm.
Finally, use the :meth:`~numericalderivative.SteplemanWinarsky.compute_first_derivative()` method to compute
an approximate value of the first derivative using finite differences.
The :meth:`~numericalderivative.SteplemanWinarsky.get_number_of_function_evaluations()` method
can be used to get the number of function evaluations.

.. GENERATED FROM PYTHON SOURCE LINES 38-51

.. code-block:: Python

    x = 1.0
    algorithm = nd.SteplemanWinarsky(np.exp, x, verbose=True)
    initial_step = 1.0e0
    step, number_of_iterations = algorithm.compute_step(initial_step)
    f_prime_approx = algorithm.compute_first_derivative(step)
    feval = algorithm.get_number_of_function_evaluations()
    f_prime_exact = np.exp(x)  # Since the derivative of exp is exp.
    print(f"Computed step = {step:.3e}")
    print(f"Number of iterations = {number_of_iterations}")
    print(f"f_prime_approx = {f_prime_approx}")
    print(f"f_prime_exact = {f_prime_exact}")
    absolute_error = abs(f_prime_approx - f_prime_exact)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=4.4784e-01
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=2.6634e-02
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=1.6595e-03
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=1.0370e-04
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=6.4809e-06
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=4.0506e-07
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=2.5315e-08
      number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=1.5825e-09
      number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=1.1642e-10
      number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=1.1642e-10
      number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=4.6566e-10
      Stop because no monotony anymore.
    Computed step = 9.537e-07
    Number of iterations = 10
    f_prime_approx = 2.7182818283326924
    f_prime_exact = 2.718281828459045




.. GENERATED FROM PYTHON SOURCE LINES 52-54

Use the method on the ScaledExponentialProblem
----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 56-57

Consider this problem.

.. GENERATED FROM PYTHON SOURCE LINES 59-62

.. code-block:: Python

    problem = nd.ScaledExponentialProblem()
    problem






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <b>DerivativeBenchmarkProblem</b>
    <ul>
    <li>name = scaled exp</li>
    <li>x = 1.0</li>
    <li>f(x) = 0.9999990000005</li>
    <li>f'(x) = -9.999990000004999e-07</li>
    <li>f''(x) = 9.999990000005e-13</li>
    <li>f^(3)(x) = -9.999990000005e-19</li>
    <li>f^(4)(x) = 9.999990000004998e-25</li>
    <li>f^(5)(x) = -9.999990000004998e-31</li>
    </ul>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 63-75

.. code-block:: Python

    name = problem.get_name()
    x = problem.get_x()
    third_derivative = problem.get_third_derivative()
    third_derivative_value = third_derivative(x)
    optimum_step, absolute_error = nd.FirstDerivativeCentral.compute_step(
        third_derivative_value
    )
    print(f"Name = {name}, x = {x}")
    print(f"Optimal step for central finite difference formula = {optimum_step}")
    print(f"Minimum absolute error= {absolute_error}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Name = scaled exp, x = 1.0
    Optimal step for central finite difference formula = 6.694331732265233
    Minimum absolute error= 2.2407016263779204e-17




.. GENERATED FROM PYTHON SOURCE LINES 76-78

Plot the error vs h
-------------------

.. GENERATED FROM PYTHON SOURCE LINES 80-91

.. code-block:: Python

    x = problem.get_x()
    function = problem.get_function()
    first_derivative = problem.get_first_derivative()
    finite_difference = nd.FirstDerivativeCentral(function, x)
    number_of_points = 200
    step_array = np.logspace(-7.0, 5.0, number_of_points)
    error_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        f_prime_approx = finite_difference.compute(step_array[i])
        error_array[i] = abs(f_prime_approx - first_derivative(x))








.. GENERATED FROM PYTHON SOURCE LINES 92-104

.. code-block:: Python

    pl.figure()
    pl.plot(step_array, error_array)
    pl.plot([optimum_step] * 2, [min(error_array), max(error_array)], label=r"$h^*$")
    pl.title(f"Central finite difference on {problem.get_name()} at x = {problem.get_x()}")
    pl.xlabel("h")
    pl.ylabel("Error")
    pl.xscale("log")
    pl.yscale("log")
    pl.legend(bbox_to_anchor=(1, 1))
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_001.png
   :alt: Central finite difference on scaled exp at x = 1.0
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 105-106

Use the algorithm to detect h*

.. GENERATED FROM PYTHON SOURCE LINES 108-123

.. code-block:: Python

    function = problem.get_function()
    first_derivative = problem.get_first_derivative()
    algorithm = nd.SteplemanWinarsky(function, x, verbose=True)
    initial_step = 1.0e8
    x = problem.get_x()
    h_optimal, iterations = algorithm.compute_step(initial_step)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    absolute_error = abs(f_prime_approx - first_derivative(x))
    print("Absolute error = ", absolute_error)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+08
      number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=1.3441e+35
      number_of_iterations=1, h=6.2500e+06, |FD(h_current) - FD(h_previous)|=1.4401e+03
      number_of_iterations=2, h=1.5625e+06, |FD(h_current) - FD(h_previous)|=3.9981e-05
      number_of_iterations=3, h=3.9062e+05, |FD(h_current) - FD(h_previous)|=4.3393e-07
      number_of_iterations=4, h=9.7656e+04, |FD(h_current) - FD(h_previous)|=2.4036e-08
      number_of_iterations=5, h=2.4414e+04, |FD(h_current) - FD(h_previous)|=1.4909e-09
      number_of_iterations=6, h=6.1035e+03, |FD(h_current) - FD(h_previous)|=9.3135e-11
      number_of_iterations=7, h=1.5259e+03, |FD(h_current) - FD(h_previous)|=5.8208e-12
      number_of_iterations=8, h=3.8147e+02, |FD(h_current) - FD(h_previous)|=3.6380e-13
      number_of_iterations=9, h=9.5367e+01, |FD(h_current) - FD(h_previous)|=2.2738e-14
      number_of_iterations=10, h=2.3842e+01, |FD(h_current) - FD(h_previous)|=1.4208e-15
      number_of_iterations=11, h=5.9605e+00, |FD(h_current) - FD(h_previous)|=8.3819e-17
      number_of_iterations=12, h=1.4901e+00, |FD(h_current) - FD(h_previous)|=9.3131e-18
      number_of_iterations=13, h=3.7253e-01, |FD(h_current) - FD(h_previous)|=3.7253e-17
      Stop because no monotony anymore.
    Optimum h = 1.4901161193847656
    iterations = 13
    Function evaluations = 30
    Absolute error =  1.9825229646960527e-17




.. GENERATED FROM PYTHON SOURCE LINES 124-126

Plot the absolute difference depending on the step
--------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 129-159

.. code-block:: Python

    def fd_difference(h1, h2, function, x):
        """
        Compute the difference of central difference approx. for different step sizes

        This function computes the absolute value of the difference of approximations
        evaluated at two different steps h1 and h2:

            d = abs(FD(h1) - FD(h2))

        where FD(h) is the approximation from the finite difference formula
        evaluated from the step h.

        Parameters
        ----------
        h1 : float, > 0
            The first step
        h2 : float, > 0
            The second step
        function : function
            The function
        x : float
            The input point where the derivative is approximated.
        """
        finite_difference = nd.FirstDerivativeCentral(function, x)
        f_prime_approx_1 = finite_difference.compute(h1)
        f_prime_approx_2 = finite_difference.compute(h2)
        diff_current = abs(f_prime_approx_1 - f_prime_approx_2)
        return diff_current









.. GENERATED FROM PYTHON SOURCE LINES 160-161

Plot the evolution of | FD(h) - FD(h / 2) | for different values of h

.. GENERATED FROM PYTHON SOURCE LINES 163-170

.. code-block:: Python

    number_of_points = 200
    step_array = np.logspace(-7.0, 5.0, number_of_points)
    diff_array = np.zeros((number_of_points))
    function = problem.get_function()
    for i in range(number_of_points):
        diff_array[i] = fd_difference(step_array[i], step_array[i] / 2, function, x)








.. GENERATED FROM PYTHON SOURCE LINES 171-181

.. code-block:: Python

    pl.figure()
    pl.plot(step_array, diff_array)
    pl.title("F.D. difference")
    pl.xlabel("h")
    pl.ylabel(r"$|\operatorname{FD}(h) - \operatorname{FD}(h / 2) |$")
    pl.xscale("log")
    pl.yscale("log")
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_002.png
   :alt: F.D. difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 182-184

Plot the criterion depending on the step
----------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 186-187

Plot the evolution of | FD(h) - FD(h / 2) | for different values of h

.. GENERATED FROM PYTHON SOURCE LINES 189-203

.. code-block:: Python

    number_of_points = 20
    h_initial = 1.0e5
    beta = 4.0
    step_array = np.zeros((number_of_points))
    diff_array = np.zeros((number_of_points))
    function = problem.get_function()
    for i in range(number_of_points):
        if i == 0:
            step_array[i] = h_initial / beta
            diff_array[i] = fd_difference(step_array[i], h_initial, function, x)
        else:
            step_array[i] = step_array[i - 1] / beta
            diff_array[i] = fd_difference(step_array[i], step_array[i - 1], function, x)








.. GENERATED FROM PYTHON SOURCE LINES 204-213

.. code-block:: Python

    pl.figure()
    pl.plot(step_array, diff_array, "o")
    pl.title("F.D. difference")
    pl.xlabel("h")
    pl.ylabel(r"$|\operatorname{FD}(h) - \operatorname{FD}(h / 2) |$")
    pl.xscale("log")
    pl.yscale("log")
    pl.tight_layout()




.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_003.png
   :alt: F.D. difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 214-216

Compute reference step
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 218-223

.. code-block:: Python

    p = 1.0e-16
    beta = 4.0
    h_reference = beta * p ** (1 / 3) * x
    print("Suggested h0 = ", h_reference)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Suggested h0 =  1.8566355334451128e-05




.. GENERATED FROM PYTHON SOURCE LINES 224-226

Plot number of lost digits vs h
-------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 228-231

The :meth:`~numericalderivative.SteplemanWinarsky.number_of_lost_digits` method
computes the number of lost digits in the approximated derivative
depending on the step.

.. GENERATED FROM PYTHON SOURCE LINES 233-251

.. code-block:: Python

    h = 1.0e4
    print("Starting h = ", h)
    n_digits = algorithm.number_of_lost_digits(h)
    print("Number of lost digits = ", n_digits)
    threshold = np.log10(p ** (-1.0 / 3.0) / beta)
    print("Threshold = ", threshold)

    step_zero, iterations = algorithm.search_step_with_bisection(
        1.0e-5,
        1.0e7,
    )
    print("step_zero = ", step_zero)
    print("iterations = ", iterations)

    estim_step, iterations = algorithm.compute_step(step_zero, beta=1.5)
    print("estim_step = ", estim_step)
    print("iterations = ", iterations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Starting h =  10000.0
    Number of lost digits =  1.6989627661187812
    Threshold =  4.7312733420053705
    + search_step_with_bisection()
    + h_min = 1.000e-05, h_max = 1.000e+07
    + relative_precision = 1.000e-16
    n_min = 10.699, n_treshold = 4.731, n_max = -4.343
    + Iter 0 / 53, h_min = 1.000e-05, h_max = 1.000e+07
      h = 1.000e+01,   Number of lost digits = 4.699
      h is just right : stop !
    step_zero =  10.0
    iterations =  0
    + compute_step()
    initial_step=1.000e+01
      number_of_iterations=0, h=6.6667e+00, |FD(h_current) - FD(h_previous)|=2.4980e-17
      number_of_iterations=1, h=4.4444e+00, |FD(h_current) - FD(h_previous)|=4.1634e-18
      number_of_iterations=2, h=2.9630e+00, |FD(h_current) - FD(h_previous)|=6.2450e-18
      Stop because no monotony anymore.
    estim_step =  4.444444444444445
    iterations =  2




.. GENERATED FROM PYTHON SOURCE LINES 252-259

.. code-block:: Python

    number_of_points = 200
    step_array = np.logspace(-7.0, 7.0, number_of_points)
    n_digits_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        h = step_array[i]
        n_digits_array[i] = algorithm.number_of_lost_digits(h)








.. GENERATED FROM PYTHON SOURCE LINES 260-280

.. code-block:: Python

    y_max = algorithm.number_of_lost_digits(h_reference)
    pl.figure()
    pl.plot(step_array, n_digits_array, label="$N(h)$")
    pl.plot([h_reference] * 2, [0.0, y_max], "--", label=r"$h_{ref}$")
    pl.plot([step_zero] * 2, [0.0, y_max], "--", label=r"$h^{(0)}$")
    pl.plot([estim_step] * 2, [0.0, y_max], "--", label=r"$h^\star$")
    pl.plot(
        step_array,
        np.array([threshold] * number_of_points),
        ":",
        label=r"$T$",
    )
    pl.title("Number of digits lost by F.D.")
    pl.xlabel("h")
    pl.ylabel("$N(h)$")
    pl.xscale("log")
    _ = pl.legend(bbox_to_anchor=(1.0, 1.0))
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_004.png
   :alt: Number of digits lost by F.D.
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 281-293

.. code-block:: Python

    pl.figure()
    pl.plot(step_array, error_array)
    pl.plot([step_zero] * 2, [0.0, 1.0e-9], "--", label=r"$h^{(0)}$")
    pl.plot([estim_step] * 2, [0.0, 1.0e-9], "--", label=r"$h^\star$")
    pl.title("Finite difference")
    pl.xlabel("h")
    pl.ylabel("Error")
    pl.xscale("log")
    pl.legend(bbox_to_anchor=(1.0, 1.0))
    pl.yscale("log")
    pl.tight_layout()




.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_005.png
   :alt: Finite difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 294-296

Use the bisection search
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 299-304

In some cases, it is difficult to find the initial step.
In this case, we can use the bisection algorithm, which can produce
an initial guess for the step.c
This algorithm is based on a search for a suitable step within
an interval.

.. GENERATED FROM PYTHON SOURCE LINES 306-307

Test with single point and default parameters.

.. GENERATED FROM PYTHON SOURCE LINES 309-319

.. code-block:: Python

    x = 1.0
    f_prime_approx, number_of_iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e7,
    )
    feval = algorithm.get_number_of_function_evaluations()
    print("FD(x) = ", f_prime_approx)
    print("number_of_iterations = ", number_of_iterations)
    print("Func. eval = ", feval)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+07
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = -4.343
    + Iter 0 / 53, h_min = 1.000e-07, h_max = 1.000e+07
      h = 1.000e+00,   Number of lost digits = 5.699
      h is small: increase it
    + Iter 1 / 53, h_min = 1.000e+00, h_max = 1.000e+07
      h = 3.162e+03,   Number of lost digits = 2.199
      h is just right : stop !
    FD(x) =  3162.2776601683795
    number_of_iterations =  1
    Func. eval =  667




.. GENERATED FROM PYTHON SOURCE LINES 320-322

See how the algorithm behaves if we use or do not use the log scale
when searching for the optimal step (this can be slower).

.. GENERATED FROM PYTHON SOURCE LINES 324-343

.. code-block:: Python

    x = 1.0
    maximum_bisection = 53
    print("+ No log scale.")
    h0, iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e1,
        maximum_bisection=53,
        log_scale=False,
    )
    print("Pas initial = ", h0, ", iterations = ", iterations)
    print("+ Log scale.")
    h0, iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e1,
        maximum_bisection=53,
        log_scale=True,
    )
    print("Pas initial = ", h0, ", iterations = ", iterations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + No log scale.
    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+01
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = 4.699
    Pas initial =  10.0 , iterations =  0
    + Log scale.
    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+01
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = 4.699
    Pas initial =  10.0 , iterations =  0




.. GENERATED FROM PYTHON SOURCE LINES 344-348

In the next example, we search for an initial step using bisection,
then use this step as an initial guess for the algorithm.
Finally, we compute an approximation of the first derivative using
the finite difference formula.

.. GENERATED FROM PYTHON SOURCE LINES 350-370

.. code-block:: Python

    problem = nd.ExponentialProblem()
    function = problem.get_function()
    first_derivative = problem.get_first_derivative()
    x = 1.0
    algorithm = nd.SteplemanWinarsky(function, x, verbose=True)
    initial_step, estim_relative_error = algorithm.search_step_with_bisection(
        1.0e-6,
        100.0 * x,
        beta=4.0,
    )
    step, number_of_iterations = algorithm.compute_step(initial_step)
    f_prime_approx = algorithm.compute_first_derivative(step)
    absolute_error = abs(f_prime_approx - first_derivative(x))
    feval = algorithm.get_number_of_function_evaluations()
    print(
        "x = %.3f, abs. error = %.3e, estim. rel. error = %.3e, Func. eval. = %d"
        % (x, absolute_error, estim_relative_error, number_of_function_evaluations)
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + search_step_with_bisection()
    + h_min = 1.000e-06, h_max = 1.000e+02
    + relative_precision = 1.000e-16
    n_min = 5.699, n_treshold = 4.731, n_max = -43.429
    + Iter 0 / 53, h_min = 1.000e-06, h_max = 1.000e+02
      h = 1.000e-02,   Number of lost digits = 1.699
      h is just right : stop !
    + compute_step()
    initial_step=1.000e-02
      number_of_iterations=0, h=2.5000e-03, |FD(h_current) - FD(h_previous)|=4.2473e-05
      number_of_iterations=1, h=6.2500e-04, |FD(h_current) - FD(h_previous)|=2.6546e-06
      number_of_iterations=2, h=1.5625e-04, |FD(h_current) - FD(h_previous)|=1.6591e-07
      number_of_iterations=3, h=3.9063e-05, |FD(h_current) - FD(h_previous)|=1.0366e-08
      number_of_iterations=4, h=9.7656e-06, |FD(h_current) - FD(h_previous)|=6.3665e-10
      number_of_iterations=5, h=2.4414e-06, |FD(h_current) - FD(h_previous)|=6.4055e-12
      number_of_iterations=6, h=6.1035e-07, |FD(h_current) - FD(h_previous)|=3.2409e-10
      Stop because no monotony anymore.
    x = 1.000, abs. error = 6.533e-11, estim. rel. error = 0.000e+00, Func. eval. = 30




.. GENERATED FROM PYTHON SOURCE LINES 371-373

See the history of steps during the search
------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 375-389

In Stepleman & Winarsky's method, the algorithm
produces a sequence of steps :math:`(h_i)_{1 \leq i \leq n_{iter}}`
where :math:`n_{iter} \in \mathbb{N}` is the number of iterations.
These steps are meant to converge to an
approximately optimal step of for the central finite difference formula of the
first derivative.
The optimal step :math:`h^\star` for the central finite difference formula of the
first derivative can be computed depending on the third derivative of the
function.
In the next example, we want to compute the absolute error between
each intermediate step :math:`h_i` and the exact value :math:`h^\star`
to see how close the algorithm gets to the exact step.
The list of intermediate steps during the algorithm can be obtained
thanks to the :meth:`~numericalderivative.SteplemanWinarsky.get_step_history` method.

.. GENERATED FROM PYTHON SOURCE LINES 392-395

In the next example, we print the intermediate steps k during
the bissection algorithm that searches for a step such
that the L ratio is satisfactory.

.. GENERATED FROM PYTHON SOURCE LINES 397-411

.. code-block:: Python

    problem = nd.SinProblem()
    function = problem.get_function()
    name = problem.get_name()
    x = problem.get_x()
    algorithm = nd.SteplemanWinarsky(function, x, verbose=True)
    initial_step = 1.0e0
    step, number_of_iterations = algorithm.compute_step(initial_step)
    step_h_history = algorithm.get_step_history()
    print(f"Number of iterations = {number_of_iterations}")
    print(f"History of steps h : {step_h_history}")
    # The last step is not the best one, sinces it breaks the monotony
    last_step_h = step_h_history[-2]
    print(f"Last step h : {last_step_h}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+00
      number_of_iterations=0, h=2.5000e-01, |FD(h_current) - FD(h_previous)|=8.0043e-02
      number_of_iterations=1, h=6.2500e-02, |FD(h_current) - FD(h_previous)|=5.2589e-03
      number_of_iterations=2, h=1.5625e-02, |FD(h_current) - FD(h_previous)|=3.2971e-04
      number_of_iterations=3, h=3.9062e-03, |FD(h_current) - FD(h_previous)|=2.0611e-05
      number_of_iterations=4, h=9.7656e-04, |FD(h_current) - FD(h_previous)|=1.2882e-06
      number_of_iterations=5, h=2.4414e-04, |FD(h_current) - FD(h_previous)|=8.0511e-08
      number_of_iterations=6, h=6.1035e-05, |FD(h_current) - FD(h_previous)|=5.0316e-09
      number_of_iterations=7, h=1.5259e-05, |FD(h_current) - FD(h_previous)|=3.1378e-10
      number_of_iterations=8, h=3.8147e-06, |FD(h_current) - FD(h_previous)|=2.9104e-11
      number_of_iterations=9, h=9.5367e-07, |FD(h_current) - FD(h_previous)|=1.4552e-11
      number_of_iterations=10, h=2.3842e-07, |FD(h_current) - FD(h_previous)|=1.7462e-10
      Stop because no monotony anymore.
    Number of iterations = 10
    History of steps h : [1.0, 0.25, 0.0625, 0.015625, 0.00390625, 0.0009765625, 0.000244140625, 6.103515625e-05, 1.52587890625e-05, 3.814697265625e-06, 9.5367431640625e-07, 2.384185791015625e-07]
    Last step h : 9.5367431640625e-07




.. GENERATED FROM PYTHON SOURCE LINES 412-413

Then we compute the exact step, using :meth:`~numericalderivative.ThirdDerivativeCentral.compute_step`.

.. GENERATED FROM PYTHON SOURCE LINES 413-422

.. code-block:: Python

    third_derivative = problem.get_third_derivative()
    third_derivative_value = third_derivative(x)
    print(f"f^(3)(x) = {third_derivative_value}")
    absolute_precision = 1.0e-16
    exact_step_k, absolute_error = nd.FirstDerivativeCentral.compute_step(
        third_derivative_value, absolute_precision
    )
    print(f"Optimal step k for f^(3)(x) = {exact_step_k}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    f^(3)(x) = -0.5403023058681398
    Optimal step k for f^(3)(x) = 8.219173432436798e-06




.. GENERATED FROM PYTHON SOURCE LINES 423-425

Plot the absolute error between the exact step k and the intermediate k
of the algorithm.

.. GENERATED FROM PYTHON SOURCE LINES 425-438

.. code-block:: Python

    error_step_h = [
        abs(step_h_history[i] - exact_step_k) for i in range(len(step_h_history))
    ]
    fig = pl.figure()
    pl.title(f"Stepleman & Winarsky on {name} at x = {x}")
    pl.plot(range(len(step_h_history)), error_step_h, "o-")
    pl.xlabel("Iterations")
    pl.ylabel(r"$|h_i - h^\star|$")
    pl.yscale("log")
    ax = fig.gca()
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    pl.tight_layout()




.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_006.png
   :alt: Stepleman & Winarsky on sin at x = 1.0
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 439-444

The previous figure shows that the algorithm gets closer to the optimal
value of the step k in the early iterations.
In the last iterations of the algorithm, the absolute error does not
continue to decrease monotically and produces a final absolute
error close to :math:`10^{-3}`.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.086 seconds)


.. _sphx_glr_download_auto_example_plot_stepleman_winarsky.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_stepleman_winarsky.ipynb <plot_stepleman_winarsky.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_stepleman_winarsky.py <plot_stepleman_winarsky.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_stepleman_winarsky.zip <plot_stepleman_winarsky.zip>`
