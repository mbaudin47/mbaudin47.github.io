{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A simple demonstration of the methods\n\nFinds a step which is near to optimal for a centered finite difference \nformula.\n\n## References\n- Adaptive numerical differentiation. R. S. Stepleman and N. D. Winarsky. Journal: Math. Comp. 33 (1979), 1257-1264 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pylab as pl\nimport numericalderivative as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function\nHere, we do not use the ScaledExponentialDerivativeBenchmark class, for demonstration purposes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp(x):\n    alpha = 1.0e6\n    return np.exp(-x / alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define its exact derivative (for testing purposes only)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp_prime(x):\n    alpha = 1.0e6\n    return -np.exp(-x / alpha) / alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define its exact second derivative (for testing purposes only)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_exp_second(x):\n    alpha = 1.0e6\n    return np.exp(-x / alpha) / alpha**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function value\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Function\")\nx = 1.0e0\nexact_f_value = scaled_exp(x)\nprint(\"exact_f_value = \", exact_f_value)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nexact_f_second_value = scaled_exp_second(x)\nprint(\"exact_f_second_value = \", exact_f_second_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h*: SteplemanWinarsky\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ SteplemanWinarsky\")\nh0 = 1.0e5\nx = 1.0e0\nalgorithm = nd.SteplemanWinarsky(scaled_exp, x)\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h*: DumontetVignes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ DumontetVignes\")\nx = 1.0e0\nalgorithm = nd.DumontetVignes(scaled_exp, x)\nh_optimal, _ = algorithm.compute_step(\n    kmin=1.0e-2,\n    kmax=1.0e2,\n)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h =\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(h_optimal)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algorithm to detect h*: GillMurraySaundersWright\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ GillMurraySaundersWright\")\nx = 1.0e0\nabsolute_precision = 1.0e-15\nalgorithm = nd.GillMurraySaundersWright(scaled_exp, x, absolute_precision)\nkmin = 1.0e-2\nkmax = 1.0e7\nstep, number_of_iterations = algorithm.compute_step(kmin, kmax)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f'=\", step)\nprint(\"Function evaluations =\", number_of_function_evaluations)\nf_prime_approx = algorithm.compute_first_derivative(step)\nprint(\"f_prime_approx = \", f_prime_approx)\nexact_f_prime_value = scaled_exp_prime(x)\nprint(\"exact_f_prime_value = \", exact_f_prime_value)\nabsolute_error = abs(f_prime_approx - exact_f_prime_value)\nprint(f\"Absolute error = {absolute_error:.3e}\")\nrelative_error = absolute_error / abs(exact_f_prime_value)\nprint(f\"Relative error = {relative_error:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function with arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_exp_with_args(x, scaling):\n    return np.exp(-x / scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the derivative of a function with extra input arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"+ Function with extra input arguments\")\nh0 = 1.0e5\nx = 1.0e0\nscaling = 1.0e6\nalgorithm = nd.SteplemanWinarsky(my_exp_with_args, x, args=[scaling])\nh_optimal, iterations = algorithm.compute_step(h0)\nnumber_of_function_evaluations = algorithm.get_number_of_function_evaluations()\nprint(\"Optimum h for f''=\", h_optimal)\nprint(\"iterations =\", iterations)\nprint(\"Function evaluations =\", number_of_function_evaluations)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}