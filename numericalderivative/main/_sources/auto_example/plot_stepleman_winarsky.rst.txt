
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_stepleman_winarsky.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_example_plot_stepleman_winarsky.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_stepleman_winarsky.py:


Experiment with Stepleman & Winarsky method
===========================================

Find a step which is near to optimal for a centered finite difference 
formula.

References
----------
- Adaptive numerical differentiation
  R. S. Stepleman and N. D. Winarsky
  Journal: Math. Comp. 33 (1979), 1257-1264 

.. GENERATED FROM PYTHON SOURCE LINES 18-22

.. code-block:: Python

    import numpy as np
    import pylab as pl
    import numericalderivative as nd








.. GENERATED FROM PYTHON SOURCE LINES 23-24

Consider this problem.

.. GENERATED FROM PYTHON SOURCE LINES 26-39

.. code-block:: Python

    benchmark = nd.ScaledExponentialProblem()
    name = benchmark.get_name()
    x = benchmark.get_x()
    third_derivative = benchmark.get_third_derivative()
    third_derivative_value = third_derivative(x)
    optimum_step, absolute_error = nd.FirstDerivativeCentral.compute_step(
        third_derivative_value
    )
    print(f"Name = {name}, x = {x}")
    print(f"Optimal step for central finite difference formula = {optimum_step}")
    print(f"Minimum absolute error= {absolute_error}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Name = scaled exp, x = 1.0
    Optimal step for central finite difference formula = 6.694331732265233
    Minimum absolute error= 2.2407016263779204e-17




.. GENERATED FROM PYTHON SOURCE LINES 40-41

1. Plot the error vs h

.. GENERATED FROM PYTHON SOURCE LINES 41-51

.. code-block:: Python

    x = 1.0
    finite_difference = nd.FirstDerivativeCentral(benchmark.function, x)
    number_of_points = 1000
    h_array = np.logspace(-7.0, 5.0, number_of_points)
    error_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        h = h_array[i]
        f_prime_approx = finite_difference.compute(h)
        error_array[i] = abs(f_prime_approx - benchmark.first_derivative(x))








.. GENERATED FROM PYTHON SOURCE LINES 52-64

.. code-block:: Python

    pl.figure()
    pl.plot(h_array, error_array)
    pl.plot([optimum_step] * 2, [min(error_array), max(error_array)], label=r"$h^*$")
    pl.title("Central finite difference")
    pl.xlabel("h")
    pl.ylabel("Error")
    pl.xscale("log")
    pl.yscale("log")
    pl.legend(bbox_to_anchor=(1, 1))
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_001.png
   :alt: Central finite difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 65-66

2. Algorithm to detect h*

.. GENERATED FROM PYTHON SOURCE LINES 66-79

.. code-block:: Python

    algorithm = nd.SteplemanWinarsky(benchmark.function, x, verbose=True)
    initial_step = 1.0e8
    x = 1.0e0
    h_optimal, iterations = algorithm.compute_step(initial_step)
    number_of_function_evaluations = algorithm.get_number_of_function_evaluations()
    print("Optimum h =", h_optimal)
    print("iterations =", iterations)
    print("Function evaluations =", number_of_function_evaluations)
    f_prime_approx = algorithm.compute_first_derivative(h_optimal)
    absolute_error = abs(f_prime_approx - benchmark.first_derivative(x))
    print("Error = ", absolute_error)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + compute_step()
    initial_step=1.000e+08
    number_of_iterations=0, h=2.5000e+07, |FD(h_current) - FD(h_previous)|=1.3441e+35
    number_of_iterations=1, h=6.2500e+06, |FD(h_current) - FD(h_previous)|=1.4401e+03
    number_of_iterations=2, h=1.5625e+06, |FD(h_current) - FD(h_previous)|=3.9981e-05
    number_of_iterations=3, h=3.9062e+05, |FD(h_current) - FD(h_previous)|=4.3393e-07
    number_of_iterations=4, h=9.7656e+04, |FD(h_current) - FD(h_previous)|=2.4036e-08
    number_of_iterations=5, h=2.4414e+04, |FD(h_current) - FD(h_previous)|=1.4909e-09
    number_of_iterations=6, h=6.1035e+03, |FD(h_current) - FD(h_previous)|=9.3135e-11
    number_of_iterations=7, h=1.5259e+03, |FD(h_current) - FD(h_previous)|=5.8208e-12
    number_of_iterations=8, h=3.8147e+02, |FD(h_current) - FD(h_previous)|=3.6380e-13
    number_of_iterations=9, h=9.5367e+01, |FD(h_current) - FD(h_previous)|=2.2738e-14
    number_of_iterations=10, h=2.3842e+01, |FD(h_current) - FD(h_previous)|=1.4208e-15
    number_of_iterations=11, h=5.9605e+00, |FD(h_current) - FD(h_previous)|=8.3819e-17
    number_of_iterations=12, h=1.4901e+00, |FD(h_current) - FD(h_previous)|=9.3131e-18
    number_of_iterations=13, h=3.7253e-01, |FD(h_current) - FD(h_previous)|=3.7253e-17
    Stop because no monotony anymore.
    Optimum h = 1.4901161193847656
    iterations = 13
    Function evaluations = 30
    Error =  1.9825229646960527e-17




.. GENERATED FROM PYTHON SOURCE LINES 80-88

.. code-block:: Python

    def fd_difference(h1, h2, f, x):
        finite_difference = nd.FirstDerivativeCentral(f, x)
        f_prime_approx_1 = finite_difference.compute(h1)
        f_prime_approx_2 = finite_difference.compute(h2)
        diff_current = abs(f_prime_approx_1 - f_prime_approx_2)
        return diff_current









.. GENERATED FROM PYTHON SOURCE LINES 89-90

3. Plot the evolution of | FD(h) - FD(h / 2) | for different values of h

.. GENERATED FROM PYTHON SOURCE LINES 90-97

.. code-block:: Python

    number_of_points = 1000
    h_array = np.logspace(-7.0, 5.0, number_of_points)
    diff_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        h = h_array[i]
        diff_array[i] = fd_difference(h, h / 2, benchmark.function, x)








.. GENERATED FROM PYTHON SOURCE LINES 98-108

.. code-block:: Python

    pl.figure()
    pl.plot(h_array, diff_array)
    pl.title("F.D. difference")
    pl.xlabel("h")
    pl.ylabel(r"$|\operatorname{FD}(h) - \operatorname{FD}(h / 2) |$")
    pl.xscale("log")
    pl.yscale("log")
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_002.png
   :alt: F.D. difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 109-110

4. Plot the evolution of | FD(h) - FD(h / 2) | for different values of h

.. GENERATED FROM PYTHON SOURCE LINES 110-123

.. code-block:: Python

    number_of_points = 20
    h_initial = 1.0e5
    beta = 4.0
    h_array = np.zeros((number_of_points))
    diff_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        if i == 0:
            h_array[i] = h_initial / beta
            diff_array[i] = fd_difference(h_array[i], h_initial, benchmark.function, x)
        else:
            h_array[i] = h_array[i - 1] / beta
            diff_array[i] = fd_difference(h_array[i], h_array[i - 1], benchmark.function, x)








.. GENERATED FROM PYTHON SOURCE LINES 124-133

.. code-block:: Python

    pl.figure()
    pl.plot(h_array, diff_array, "o")
    pl.title("F.D. difference")
    pl.xlabel("h")
    pl.ylabel(r"$|\operatorname{FD}(h) - \operatorname{FD}(h / 2) |$")
    pl.xscale("log")
    pl.yscale("log")
    pl.tight_layout()




.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_003.png
   :alt: F.D. difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 134-135

5. Compute suggested step

.. GENERATED FROM PYTHON SOURCE LINES 135-140

.. code-block:: Python

    p = 1.0e-16
    beta = 4.0
    h_reference = beta * p ** (1 / 3) * x
    print("Suggested h0 = ", h_reference)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Suggested h0 =  1.8566355334451128e-05




.. GENERATED FROM PYTHON SOURCE LINES 141-142

6. Plot number of lost digits vs h

.. GENERATED FROM PYTHON SOURCE LINES 142-160

.. code-block:: Python

    h = 1.0e4
    print("Starting h = ", h)
    n_digits = algorithm.number_of_lost_digits(h)
    print("Number of lost digits = ", n_digits)
    threshold = np.log10(p ** (-1.0 / 3.0) / beta)
    print("Threshold = ", threshold)

    step_zero, iterations = algorithm.search_step_with_bisection(
        1.0e-5,
        1.0e7,
    )
    print("step_zero = ", step_zero)
    print("iterations = ", iterations)

    estim_step, iterations = algorithm.compute_step(step_zero, beta=1.5)
    print("estim_step = ", estim_step)
    print("iterations = ", iterations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Starting h =  10000.0
    Number of lost digits =  1.6989627661187812
    Threshold =  4.7312733420053705
    + search_step_with_bisection()
    + h_min = 1.000e-05, h_max = 1.000e+07
    + relative_precision = 1.000e-16
    n_min = 10.699, n_treshold = 4.731, n_max = -4.343
    + Iter 0 / 53, h_min = 1.000e-05, h_max = 1.000e+07
      h = 1.000e+01,   Number of lost digits = 4.699
      h is just right : stop !
    step_zero =  10.0
    iterations =  0
    + compute_step()
    initial_step=1.000e+01
    number_of_iterations=0, h=6.6667e+00, |FD(h_current) - FD(h_previous)|=2.4980e-17
    number_of_iterations=1, h=4.4444e+00, |FD(h_current) - FD(h_previous)|=4.1634e-18
    number_of_iterations=2, h=2.9630e+00, |FD(h_current) - FD(h_previous)|=6.2450e-18
    Stop because no monotony anymore.
    estim_step =  4.444444444444445
    iterations =  2




.. GENERATED FROM PYTHON SOURCE LINES 161-168

.. code-block:: Python

    number_of_points = 1000
    h_array = np.logspace(-7.0, 7.0, number_of_points)
    n_digits_array = np.zeros((number_of_points))
    for i in range(number_of_points):
        h = h_array[i]
        n_digits_array[i] = algorithm.number_of_lost_digits(h)








.. GENERATED FROM PYTHON SOURCE LINES 169-189

.. code-block:: Python

    y_max = algorithm.number_of_lost_digits(h_reference)
    pl.figure()
    pl.plot(h_array, n_digits_array, label="$N(h)$")
    pl.plot([h_reference] * 2, [0.0, y_max], "--", label=r"$h_{ref}$")
    pl.plot([step_zero] * 2, [0.0, y_max], "--", label=r"$h^{(0)}$")
    pl.plot([estim_step] * 2, [0.0, y_max], "--", label=r"$h^\star$")
    pl.plot(
        h_array,
        np.array([threshold] * number_of_points),
        ":",
        label=r"$T$",
    )
    pl.title("Number of digits lost by F.D.")
    pl.xlabel("h")
    pl.ylabel("$N(h)$")
    pl.xscale("log")
    _ = pl.legend(bbox_to_anchor=(1.0, 1.0))
    pl.tight_layout()





.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_004.png
   :alt: Number of digits lost by F.D.
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 190-202

.. code-block:: Python

    pl.figure()
    pl.plot(h_array, error_array)
    pl.plot([step_zero] * 2, [0.0, 1.0e-9], "--", label=r"$h^{(0)}$")
    pl.plot([estim_step] * 2, [0.0, 1.0e-9], "--", label=r"$h^\star$")
    pl.title("Finite difference")
    pl.xlabel("h")
    pl.ylabel("Error")
    pl.xscale("log")
    pl.legend(bbox_to_anchor=(1.0, 1.0))
    pl.yscale("log")
    pl.tight_layout()




.. image-sg:: /auto_example/images/sphx_glr_plot_stepleman_winarsky_005.png
   :alt: Finite difference
   :srcset: /auto_example/images/sphx_glr_plot_stepleman_winarsky_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 203-205

7. Benchmark
Test with single point

.. GENERATED FROM PYTHON SOURCE LINES 205-215

.. code-block:: Python

    x = 1.0
    f_prime_approx, number_of_iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e7,
    )
    feval = algorithm.get_number_of_function_evaluations()
    print("FD(x) = ", f_prime_approx)
    print("number_of_iterations = ", number_of_iterations)
    print("Func. eval = ", feval)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+07
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = -4.343
    + Iter 0 / 53, h_min = 1.000e-07, h_max = 1.000e+07
      h = 1.000e+00,   Number of lost digits = 5.699
      h is small: increase it
    + Iter 1 / 53, h_min = 1.000e+00, h_max = 1.000e+07
      h = 3.162e+03,   Number of lost digits = 2.199
      h is just right : stop !
    FD(x) =  3162.2776601683795
    number_of_iterations =  1
    Func. eval =  3067




.. GENERATED FROM PYTHON SOURCE LINES 216-217

Algorithme de dichotomie pour le pas initial

.. GENERATED FROM PYTHON SOURCE LINES 217-235

.. code-block:: Python

    x = 1.0
    maximum_bisection = 53
    log_scale = False
    h0, iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e1,
        maximum_bisection=53,
        log_scale=False,
    )
    print("Pas initial = ", h0, ", iterations = ", iterations)
    h0, iterations = algorithm.search_step_with_bisection(
        1.0e-7,
        1.0e1,
        maximum_bisection=53,
        log_scale=True,
    )
    print("Pas initial = ", h0, ", iterations = ", iterations)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+01
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = 4.699
    Pas initial =  10.0 , iterations =  0
    + search_step_with_bisection()
    + h_min = 1.000e-07, h_max = 1.000e+01
    + relative_precision = 1.000e-16
    n_min = 12.699, n_treshold = 4.731, n_max = 4.699
    Pas initial =  10.0 , iterations =  0




.. GENERATED FROM PYTHON SOURCE LINES 236-237

Test

.. GENERATED FROM PYTHON SOURCE LINES 237-254

.. code-block:: Python

    benchmark = nd.ExponentialProblem()
    x = 1.0
    algorithm = nd.SteplemanWinarsky(benchmark.function, x, verbose=True)
    f_prime_approx, estim_relative_error = algorithm.search_step_with_bisection(
        1.0e-6,
        100.0 * x,
        beta=4.0,
    )
    absolute_error = abs(f_prime_approx - benchmark.first_derivative(x))
    feval = algorithm.get_number_of_function_evaluations()
    print(
        "x = %.3f, abs. error = %.3e, estim. rel. error = %.3e, Func. eval. = %d"
        % (x, absolute_error, estim_relative_error, number_of_function_evaluations)
    )

    #





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    + search_step_with_bisection()
    + h_min = 1.000e-06, h_max = 1.000e+02
    + relative_precision = 1.000e-16
    n_min = 5.699, n_treshold = 4.731, n_max = -43.429
    + Iter 0 / 53, h_min = 1.000e-06, h_max = 1.000e+02
      h = 1.000e-02,   Number of lost digits = 1.699
      h is just right : stop !
    x = 1.000, abs. error = 2.708e+00, estim. rel. error = 0.000e+00, Func. eval. = 30





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.823 seconds)


.. _sphx_glr_download_auto_example_plot_stepleman_winarsky.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_stepleman_winarsky.ipynb <plot_stepleman_winarsky.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_stepleman_winarsky.py <plot_stepleman_winarsky.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_stepleman_winarsky.zip <plot_stepleman_winarsky.zip>`
